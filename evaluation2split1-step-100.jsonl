{"query": "question: What is a feed forward network answer: <extra_id_0>", "answers": ["A neural network architecture where the computation proceeds iteratively from one layer of units to the next"], "generation": "A feedforward network is a multilayer network network in which the units are connected with no cycles; the outputs from units in each layer are passed", "passages": [{"id": "3226", "contributed_by": "group 2", "title": "", "section": "", "text": "Basic neural units used in feedforward, simple recurrent networks (SRN), and long short-term memory (LSTM)."}, {"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}, {"id": "3198", "contributed_by": "group 2", "title": "", "section": "", "text": "Stacked recurrent networks. The output of a lower level serves as the input to higher levels with the output of the last network serving as the final output."}, {"id": "3228", "contributed_by": "group 2", "title": "", "section": "", "text": "This modularity is key to the power and widespread applicability of LSTM units. LSTM units (or other varieties, like GRUs) can be substituted into any of the network architectures described in Section 9.5. And, as with simple RNNs, multi-layered networks making use of gated units can be unrolled into deep feedforward networks and trained in the usual fashion with backpropagation."}, {"id": "3574", "contributed_by": "group 2", "title": "", "section": "", "text": "A weakness of this approach is that it doesn't distinguish the use of a word's embedding as the beginning of a span from its use as the end of one. Therefore, more elaborate schemes for representing the span boundaries involve learned representations for start and end points through the use of two distinct feedforward networks:"}, {"id": "2722", "contributed_by": "group 10", "title": "", "section": "", "text": "Let's now walk through a slightly more formal presentation of the simplest kind of neural network, the feedforward network. A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers."}, {"id": "2723", "contributed_by": "group 10", "title": "", "section": "", "text": "Let's now walk through a slightly more formal presentation of the simplest kind of neural network, the feedforward network. A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers."}, {"id": "3289", "contributed_by": "group 2", "title": "", "section": "", "text": "A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks."}, {"id": "2958", "contributed_by": "group 2", "title": "", "section": "", "text": "A feedforward neural LM is a feedforward network that takes as input at time t a representation of some number of previous words and outputs a probability distribution over possible next words. Thus-like the n-gram LM-the feedforward neural LM approximates the probability of a word given the entire prior context by approximating based on the N previous words:"}, {"id": "2936", "contributed_by": "group 2", "title": "", "section": "", "text": "Let s now walk through a slightly more formal presentation of the simplest kind of feedforward neural network, the feedforward network. A feedforward network is a multilayer network network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. (In Chapter 9 we will introduce networks with cycles, called recurrent neural networks.) For historical reasons multilayer networks, especially feedforward networks, are multilayer sometimes called multi-layer perceptrons (orMLPs); this is a technical misnomer, perceptrons MLP since the units in modern multilayer networks aren t perceptrons (perceptrons are purely linear, but modern networks are made up of units with non-linearities like sigmoids), but at some point the name stuck.  Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units. Fig. 7.8 shows a picture.  The input layer x is a vector of simple scalar values just as we saw in Fig. 7.2.  hidden layer"}], "metadata": {}, "id": "1"}
{"query": "question: What is the main advantage of neural network over logistic regression answer: <extra_id_0>", "answers": ["Neural networks are typically more powerful classifiers than logistic regression, and they can learn features automatically and are more suitable for large scale problems"], "generation": "Firstly, neural network is a non-convex optimization problem, more complex than for logistic regression. Firstly, neural network Firstly,", "passages": [{"id": "2340", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "Data scaling is especially important in large neural networks, since it helps speed up gradient descent."}, {"id": "2284", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "Logistic regression solves this task by learning, from a training set, a vector of weights and a bias term."}, {"id": "2401", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "In the next section we 'll introduce the loss function that is commonly used for logistic regression and also for neural networks, the cross-entropy loss."}, {"id": "2536", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": " Logistic regression is a supervised machine learning classier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability."}, {"id": "2981", "contributed_by": "group 2", "title": "", "section": "", "text": "Optimization in neural networks is a non-convex optimization problem, more complex than for logistic regression, and for that and other reasons there are many best practices for successful learning."}, {"id": "2736", "contributed_by": "group 10", "title": "", "section": "", "text": "Optimization in neural networks is a non-convex optimization problem, more complex than for logistic regression, and for that and other reasons there are many best practices for successful learning"}, {"id": "2539", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": " Multinomial logistic regression uses the softmax function to compute proba The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized."}, {"id": "2421", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "The negative log of y (if the true y equals 1) or 1 y (if the true y equals 0) is a convenient loss metric since it goes from 0 (negative log of 1, no loss) to innity (negative log of 0, innite loss)."}, {"id": "2356", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "Thus when there are many correlated features, logistic regression will assign a more accurate probability than naive Bayes."}, {"id": "2542", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": " Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features."}], "metadata": {}, "id": "2"}
{"query": "question: What is the property of a Sigmoid function answer: <extra_id_0>", "answers": ["The Sigmoid function maps the input into the range between 0 and 1, and it is differentiable"], "generation": "Sigmoid functions are exponential functions. They map the output into the range [0;1] and are differentiable.", "passages": [{"id": "2374", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "Like the sigmoid, it is an exponential function."}, {"id": "2299", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "The sigmoid function (named because it looks like an s) is also called the logistic function, and gives logistic regression its name."}, {"id": "2372", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "The multinomial logistic classier uses a generalization of the sigmoid, called the softmax function, to compute p(yk = 1|x)."}, {"id": "51", "contributed_by": "group 9", "title": "", "section": "", "text": "The multinomial logistic classifier uses a generalization of the sigmoid, called the softmax function, to compute p(yk = 1|x). The softmax function takes a vector z = [z1,z2,...,zK] of K arbitrary values and maps them to a probability distribution, with each value in the range [0,1], and all the values summing to 1. Like the sigmoid, it is an exponential function."}, {"id": "2305", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "We can do this as follows: P(y = 1) = (w x+b) P(y = 0) = 1(w x+b) The sigmoid function has the property 1(x) = (x) so we could also have expressed P(y = 0) as ((w x+b))."}, {"id": "2278", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "The sigmoid function The goal of binary logistic regression is to train a classier that can make a binary decision about the class of a new input observation."}, {"id": "3216", "contributed_by": "group 2", "title": "", "section": "", "text": "The gates in an LSTM share a common design pattern; each consists of a feedforward layer, followed by a sigmoid activation function, followed by a pointwise multiplication with the layer being gated. The choice of the sigmoid as the activation function arises from its tendency to push its outputs to either 0 or 1. Combining this with a pointwise multiplication has an effect similar to that of a binary mask. Values in the layer being gated that align with values near 1 in the mask are passed through nearly unchanged; values corresponding to lower values are essentially erased."}, {"id": "2924", "contributed_by": "group 2", "title": "", "section": "", "text": "we will discuss three popular non-linear functions f () below (the sigmoid, the tanh, and the rectified linear unit or ReLU) but it s pedagogically convenient to start with sigmoid the sigmoid function since we saw it in Chapter 5: The sigmoid (shown in Fig. 7.1) has a number of advantages; it maps the output into the range [0;1], which is useful in squashing outliers toward 0 or 1. And it s differentiable, which as we saw in Section 5.10 will be handy for learning.  Fig. 7.2 shows a final schematic of a basic neural unit. In this example the unit takes 3 input values x1;x2, and x3, and computes a weighted sum, multiplying each value by a weight (w1, w2, and w3, respectively), adds them to a bias term b, and then passes the resulting sum through a sigmoid function to result in a number between 0 and 1."}, {"id": "2536", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": " Logistic regression is a supervised machine learning classier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability."}, {"id": "2376", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "Thus for example given a vector: z = [0.6,1.1,1.5,1.2,3.2,1.1] the resulting (rounded) softmax(z) is Like the sigmoid, the softmax has the property of squashing values toward 0 or 1."}], "metadata": {}, "id": "3"}
{"query": "question: Describe the vanishing gradient problem answer: <extra_id_0>", "answers": ["Since we train networks by propagating an error signal backwards by multiplying gradients from each layer of the network, gradients that are almost zero cause the error signal to get smaller and smaller until it is too small to be used for training"], "generation": "The vanishing gradient problem is a problem with training RNNs. The gradients of the loss function with respect to the weights are unstable and eventually vanish. BPTT BPTT", "passages": [{"id": "3287", "contributed_by": "group 2", "title": "", "section": "", "text": "Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers."}, {"id": "3213", "contributed_by": "group 2", "title": "", "section": "", "text": "A second difficulty with training RNNs arises from the need to backpropagate the error signal back through time. Recall from Section 9.2.2 that the hidden layer at time t contributes to the loss at the next time step since it takes part in that calculation. As a result, during the backward pass of training, the hidden layers are subject to repeated multiplications, as determined by the length of the sequence. A frequent result of this process is that the gradients are eventually driven to zero, a situation called the vanishing gradients problem."}, {"id": "2765", "contributed_by": "group 10", "title": "", "section": "", "text": "Backpropagation Through Time (BPTT) is a general approach for training the weights in RNNs. It involves processing the input sequence in the forward direction to generate the output sequence, and then using the output sequence to compute the loss function. The loss function is then backpropagated through the network in the reverse direction using the chain rule of calculus, allowing the gradients to be computed for all the weights in the network. However, because the RNN is unrolled over time, the gradients can become unstable and either vanish or explode. To address this problem, various modifications have been proposed, such as gradient clipping and using different activation functions. Despite its challenges, BPTT remains a popular method for training RNNs and has been used in many applications, including speech recognition and machine translation."}, {"id": "2777", "contributed_by": "group 10", "title": "", "section": "", "text": "Simple recurrent networks can fail on long inputs because of problems like vanishing gradients. Modern systems use more complex gated architectures such as LSTMs and GRUs that explicitly decide what to remember and forget in their hidden and context layers. These models also allow for better gradient flow through time and can handle longer sequences than their simpler counterparts."}, {"id": "2436", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "The gradient descent algorithm answers this question by nding the gradient of the loss function at the current point and moving in the opposite direction."}, {"id": "3210", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN for sequence classification. The final hidden units from the forward and backward passes are combined to represent the entire sequence. This combined representation serves as input to the subsequent classifier."}, {"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}, {"id": "2764", "contributed_by": "group 10", "title": "", "section": "", "text": "The backpropagation algorithm for training weights in RNNs is a two-pass process. In the first pass, forward inference is performed to generate the output sequence. The output sequence is then compared to the target sequence, and the loss function is computed. In the second pass, the gradients of the loss function with respect to the weights are computed by backpropagating the errors through the network in reverse order. This is known as the backpropagation through time (BPTT) algorithm. However, because the RNN is unrolled over time, the gradients can become unstable and either vanish or explode. To address this problem, various modifications have been proposed, such as gradient clipping and using different activation functions. Despite its challenges, BPTT remains a popular method for training RNNs and has been used in many applications, including speech recognition and machine translation."}, {"id": "3325", "contributed_by": "group 2", "title": "", "section": "", "text": "Finally, languages differ systematically in how the conceptual properties of an event are mapped onto specific words. Talmy (1985, 1991) noted that languages can be characterized by whether direction of motion and manner of motion are marked on the verb or on the 'satellites': particles, prepositional phrases, or adverbial phrases. For example, a bottle floating out of a cave would be described in English with the direction marked on the particle out, while in Spanish the direction would be marked on the verb:"}, {"id": "2437", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "The gradient of a function of many variables is a vector pointing in the direction of the greatest increase in a function."}], "metadata": {}, "id": "4"}
{"query": "question: What is a fully connected layer in neural network answer: <extra_id_0>", "answers": ["It is a layer such that each unit in the layer takes as input the outputs from all the units in the previous layer"], "generation": "A fully connected layer is fully connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there are no cycles. Fully connected layer A fully connected layer Fully connected", "passages": [{"id": "3152", "contributed_by": "group 2", "title": "", "section": "", "text": "Simple recurrent neural network after Elman (1990). The hidden layer includes a recurrent connection as part of its input. That is, the activation value of the hidden layer depends on the current input as well as the activation value of the hidden layer from the previous time step."}, {"id": "2990", "contributed_by": "group 2", "title": "", "section": "", "text": "Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit. In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i+1, and there are no cycles. The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network."}, {"id": "2723", "contributed_by": "group 10", "title": "", "section": "", "text": "Let's now walk through a slightly more formal presentation of the simplest kind of neural network, the feedforward network. A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers."}, {"id": "2722", "contributed_by": "group 10", "title": "", "section": "", "text": "Let's now walk through a slightly more formal presentation of the simplest kind of neural network, the feedforward network. A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers."}, {"id": "3289", "contributed_by": "group 2", "title": "", "section": "", "text": "A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks."}, {"id": "3288", "contributed_by": "group 2", "title": "", "section": "", "text": "Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, using attention heads that model how the surrounding words are relevant for the processing of the current word."}, {"id": "2937", "contributed_by": "group 2", "title": "", "section": "", "text": "The core of the neural network is the hidden layer h formed of hidden units hi, each of which is a neural unit as described in Section 7.1, taking a weighted sum of its inputs and then applying a non-linearity. In the standard architecture, each layer fullyconnected is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. Thus each hidden unit sums over all the input units.  Recall that a single hidden unit has as parameters a weight vector and a bias. We represent the parameters for the entire hidden layer by combining the weight vector and bias for each unit i into a single weight matrix W and a single bias vector b for the whole layer (see Fig. 7.8). Each elementWji of the weight matrixWrepresents the weight of the connection from the ith input unit xi to the jth hidden unit hj."}, {"id": "2991", "contributed_by": "group 2", "title": "", "section": "", "text": "Neural networks are trained by optimization algorithms like gradient descent.Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling."}, {"id": "2983", "contributed_by": "group 2", "title": "", "section": "", "text": "Various forms of regularization are used to prevent overfitting. One of the most dropout important is dropout: randomly dropping some units and their connections from the network during training (Hinton et al. 2012, Srivastava et al. 2014). Tuning hyperparameter of hyperparameters is also important. The parameters of a neural network are the weightsW and biases b; those are learned by gradient descent. The hyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training set. Hyperparameters include the learning rate h, the mini-batch size, the model architecture (the number of layers, the number of hidden nodes per layer, the choice of activation functions), how to regularize, and so on. Gradient descent itself also has many architectural variants such as Adam (Kingma and Ba, 2015)."}, {"id": "3226", "contributed_by": "group 2", "title": "", "section": "", "text": "Basic neural units used in feedforward, simple recurrent networks (SRN), and long short-term memory (LSTM)."}], "metadata": {}, "id": "5"}
{"query": "question: What does softmax do answer: <extra_id_0>", "answers": ["Softmax is a function that converts (or normalizes) a vector of real values to a vector that encodes a probability distribution. Each element of the resulting vector ranges between 0 and 1, and they sum to 1."], "generation": "Softmax normalizes the scores into a probability distribution over the possible output classes. Softmax is a function that normalizes a softmax a", "passages": [{"id": "2946", "contributed_by": "group 2", "title": "", "section": "", "text": "Chapter 5. More generally for any vector z of dimensionality d, the softmax is defined as:"}, {"id": "2945", "contributed_by": "group 2", "title": "", "section": "", "text": "softmax between 0 and 1 and sum to 1): the softmax function that we saw on page 86 of"}, {"id": "2375", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "For a vector z of dimensionality K, the softmax is dened as: j=1 exp(z j) 1  i  K The softmax of an input vector z = [z1,z2,...,zK] is thus a vector itself: The denominator K i=1 exp(zi) is used to normalize all the values into probabilities."}, {"id": "2376", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "Thus for example given a vector: z = [0.6,1.1,1.5,1.2,3.2,1.1] the resulting (rounded) softmax(z) is Like the sigmoid, the softmax has the property of squashing values toward 0 or 1."}, {"id": "3158", "contributed_by": "group 2", "title": "", "section": "", "text": "In the commonly encountered case of soft classification, computing yt consists of a softmax computation that provides a probability distribution over the possible output classes."}, {"id": "3191", "contributed_by": "group 2", "title": "", "section": "", "text": "Sample a word in the output from the softmax distribution that results from using the beginning of sentence marker, <s>, as the first input."}, {"id": "3168", "contributed_by": "group 2", "title": "", "section": "", "text": "The vector resulting from Vh can be thought of as a set of scores over the vocabulary given the evidence provided in h. Passing these scores through the softmax normalizes the scores into a probability distribution. The probability that a particular word i in the vocabulary is the next word is represented by yt [i], the ith component of yt :"}, {"id": "2378", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "Applying softmax in logistic regression When we apply softmax for logistic regression, the input will (just as for the sigmoid) be the dot product between a weight vector w and an input vector x (plus a bias)."}, {"id": "3247", "contributed_by": "group 2", "title": "", "section": "", "text": "The ensuing softmax calculation resulting in alphai, j remains the same, but the output"}, {"id": "3497", "contributed_by": "group 2", "title": "", "section": "", "text": "The weights are computed via a softmax over the comparison scores between every element of an input sequence considered as a query and every other element as a key, where the comparison scores are computed using dot products."}], "metadata": {}, "id": "6"}
{"query": "question: What is an activation function answer: <extra_id_0>", "answers": ["Instead of using z, a linear function of x, as the output, neural units apply a nonlinear function f to z. The output of this function f is the activation value. Typical activation functions include sigmoid, hyperbolic tangent (tanh), and ReLU."], "generation": "An activation function is a function of the input at time t and the hidden state at time t - 1, An activation function A function", "passages": [{"id": "3341", "contributed_by": "group 2", "title": "", "section": "", "text": "More formally, if g is an activation function like tanh or ReLU, a function of the input at time t and the hidden state at time t - 1, and f is a softmax over the set of possible vocabulary items, then at time t the output yt and hidden state ht are computed as:"}, {"id": "3372", "contributed_by": "group 2", "title": "", "section": "", "text": "The attention mechanism allows each hidden state of the decoder to see a different, dynamic, context, which is a function of all the encoder hidden states."}, {"id": "3152", "contributed_by": "group 2", "title": "", "section": "", "text": "Simple recurrent neural network after Elman (1990). The hidden layer includes a recurrent connection as part of its input. That is, the activation value of the hidden layer depends on the current input as well as the activation value of the hidden layer from the previous time step."}, {"id": "3216", "contributed_by": "group 2", "title": "", "section": "", "text": "The gates in an LSTM share a common design pattern; each consists of a feedforward layer, followed by a sigmoid activation function, followed by a pointwise multiplication with the layer being gated. The choice of the sigmoid as the activation function arises from its tendency to push its outputs to either 0 or 1. Combining this with a pointwise multiplication has an effect similar to that of a binary mask. Values in the layer being gated that align with values near 1 in the mask are passed through nearly unchanged; values corresponding to lower values are essentially erased."}, {"id": "3014", "contributed_by": "group 2", "title": "", "section": "", "text": "Auxiliary verbs mark semantic features of a main verb such as its tense, whether it is completed (aspect), whether it is negated (polarity), and whether an action is necessary, possible, suggested, or desired (mood). English auxiliaries include the copula verb be, the two verbs do and have, forms, as well as modal verbs used to mark the mood associated with the event depicted by the main verb: can indicates"}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "2727", "contributed_by": "group 10", "title": "", "section": "", "text": "One of the reasons we use nonlinear activation functions for each layer in a neural network is that if we did not, the resulting network is exactly equivalent to a single-layer network.This generalizes to any number of layers. So without non-linear activation functions, a multilayer network is just a notational variant of a single layer network with a different set of weights, and we lose all the representational power of multilayer networks."}, {"id": "3124", "contributed_by": "group 2", "title": "", "section": "", "text": "Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags."}, {"id": "2718", "contributed_by": "group 10", "title": "", "section": "", "text": "Finally, instead of using z, a linear function of x, as the output, neural units apply a non-linear function f to z.  We will refer to this function as activation function. We will refer to the output of this function as the activation value for the unit, a."}, {"id": "2765", "contributed_by": "group 10", "title": "", "section": "", "text": "Backpropagation Through Time (BPTT) is a general approach for training the weights in RNNs. It involves processing the input sequence in the forward direction to generate the output sequence, and then using the output sequence to compute the loss function. The loss function is then backpropagated through the network in the reverse direction using the chain rule of calculus, allowing the gradients to be computed for all the weights in the network. However, because the RNN is unrolled over time, the gradients can become unstable and either vanish or explode. To address this problem, various modifications have been proposed, such as gradient clipping and using different activation functions. Despite its challenges, BPTT remains a popular method for training RNNs and has been used in many applications, including speech recognition and machine translation."}], "metadata": {}, "id": "7"}
{"query": "question: Why do we need activation functions answer: <extra_id_0>", "answers": ["Without activation functions, any multi-layer neural network can be reduced to a single-layer network, and thus losing its representational power."], "generation": "Activation functions are useful for different language applications or neural network architectures. One of the reasons we use activation functions for each layer in a neural network is that Because Activation functions Because", "passages": [{"id": "2727", "contributed_by": "group 10", "title": "", "section": "", "text": "One of the reasons we use nonlinear activation functions for each layer in a neural network is that if we did not, the resulting network is exactly equivalent to a single-layer network.This generalizes to any number of layers. So without non-linear activation functions, a multilayer network is just a notational variant of a single layer network with a different set of weights, and we lose all the representational power of multilayer networks."}, {"id": "2949", "contributed_by": "group 2", "title": "", "section": "", "text": "Thus we can re-represent our 2-layer net from Eq. 7.12 as follows: The need for non-linear activation functions One of the reasons we use nonlinear activation functions for each layer in a neural network is that if we did not, the resulting network is exactly equivalent to a single-layer network. Let s see why this is true. Imagine the first two layers of such a network of purely linear layers: This generalizes to any number of layers. So without non-linear activation functions, a multilayer network is just a notational variant of a single layer network with a different set of weights, and we lose all the representational power of multilayer networks."}, {"id": "2925", "contributed_by": "group 2", "title": "", "section": "", "text": "These activation functions have different properties that make them useful for different language applications or network architectures. For example, the tanh function has the nice properties of being smoothly differentiable and mapping outlier values toward the mean. The rectifier function, on the other hand has nice properties that result from it being very close to linear. In the sigmoid or tanh functions, very high values of z saturated result in values of y that are saturated, i.e., extremely close to 1, and have derivatives very close to 0. Zero derivatives cause problems for learning, because as we will see in Section 7.6, we will train networks by propagating an error signal backwards, multiplying gradients (partial derivatives) from each layer of the network; gradients that are almost 0 cause the error signal to get smaller and smaller until it is vanishing too small to be used for training, a problem called the vanishing gradient problem."}, {"id": "3341", "contributed_by": "group 2", "title": "", "section": "", "text": "More formally, if g is an activation function like tanh or ReLU, a function of the input at time t and the hidden state at time t - 1, and f is a softmax over the set of possible vocabulary items, then at time t the output yt and hidden state ht are computed as:"}, {"id": "3216", "contributed_by": "group 2", "title": "", "section": "", "text": "The gates in an LSTM share a common design pattern; each consists of a feedforward layer, followed by a sigmoid activation function, followed by a pointwise multiplication with the layer being gated. The choice of the sigmoid as the activation function arises from its tendency to push its outputs to either 0 or 1. Combining this with a pointwise multiplication has an effect similar to that of a binary mask. Values in the layer being gated that align with values near 1 in the mask are passed through nearly unchanged; values corresponding to lower values are essentially erased."}, {"id": "2718", "contributed_by": "group 10", "title": "", "section": "", "text": "Finally, instead of using z, a linear function of x, as the output, neural units apply a non-linear function f to z.  We will refer to this function as activation function. We will refer to the output of this function as the activation value for the unit, a."}, {"id": "2765", "contributed_by": "group 10", "title": "", "section": "", "text": "Backpropagation Through Time (BPTT) is a general approach for training the weights in RNNs. It involves processing the input sequence in the forward direction to generate the output sequence, and then using the output sequence to compute the loss function. The loss function is then backpropagated through the network in the reverse direction using the chain rule of calculus, allowing the gradients to be computed for all the weights in the network. However, because the RNN is unrolled over time, the gradients can become unstable and either vanish or explode. To address this problem, various modifications have been proposed, such as gradient clipping and using different activation functions. Despite its challenges, BPTT remains a popular method for training RNNs and has been used in many applications, including speech recognition and machine translation."}, {"id": "3152", "contributed_by": "group 2", "title": "", "section": "", "text": "Simple recurrent neural network after Elman (1990). The hidden layer includes a recurrent connection as part of its input. That is, the activation value of the hidden layer depends on the current input as well as the activation value of the hidden layer from the previous time step."}, {"id": "305", "contributed_by": "group 11", "title": "Chapter 12 - Constituency Grammars", "section": "", "text": "To move beyond these limitations CCG includes operations that operate over functions."}, {"id": "2938", "contributed_by": "group 2", "title": "", "section": "", "text": "The advantage of using a single matrix W for the weights of the entire layer is that now the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations. In fact, the computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation function defined above).  The output of the hidden layer, the vector h, is thus the following (for this example we will use the sigmoid function s as our activation function):"}], "metadata": {}, "id": "8"}
{"query": "question: What is pre-training answer: <extra_id_0>", "answers": ["Pre-training is the idea of relying on another algorithm, such as Word2Vec or GloVe, to have already learned an embedding representation for our input words"], "generation": "Pre-training is the process of learning a model by pre-tuning it with the training data. Pre-training is the process of learning a model by Pre-training Pre-training Pre-training is the process of learning Using", "passages": [{"id": "3128", "contributed_by": "group 2", "title": "", "section": "", "text": "The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence"}, {"id": "2074", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "This can be done by sorting the vocabulary by frequency in the training set, and dening the top 10100 vocabulary entries as stop words, or alternatively by using one of the many predened stop word lists available online.."}, {"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "3593", "contributed_by": "group 2", "title": "", "section": "", "text": "Pretrained language models can be fine-tuned for specific applications by adding lightweight classifier layers on top of the outputs of the pretrained model."}, {"id": "3178", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech tagging as sequence labeling with a simple RNN. Pre-trained word embeddings serve as inputs and a softmax layer provides a probability distribution over the part-of-speech tags as output at each time step."}, {"id": "1474", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "The token learner takes a raw training corpus (sometimes roughly preseparated into words, for example by whitespace) and induces a vocabulary, a set of tokens."}, {"id": "3592", "contributed_by": "group 2", "title": "", "section": "", "text": "Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input."}, {"id": "3555", "contributed_by": "group 2", "title": "", "section": "", "text": "Fine-tuning an application for one of these tasks proceeds just as with pretraining using the NSP objective. During fine-tuning, pairs of labeled sentences from the supervised training data are presented to the model. As with sequence classification, the output vector associated with the prepended [CLS] token represents the model's view of the input pair. And as with NSP training, the two inputs are separated by the a [SEP] token. To perform classification, the [CLS] vector is multiplied by a set of learning classification weights and passed through a softmax to generate label predictions, which are then used to update the weights."}, {"id": "3097", "contributed_by": "group 2", "title": "", "section": "", "text": "The known-word templates are computed for every word seen in the training set; the unknown word features can also be computed for all words in training, or only on training words whose frequency is below some threshold. The result of the known-word templates and word-signature features is a very large set of features. Generally a feature cutoff is used in which features are thrown out if they have count < 5 in the training set."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}], "metadata": {}, "id": "9"}
{"query": "question: Why does neural network use embeddings answer: <extra_id_0>", "answers": ["Embeddings allows neural language models to generalize better to unseen data"], "generation": "Using embeddings allows neural language models to generalize better to unseen data. For example, suppose we have seen this sentence in training:", "passages": [{"id": "2991", "contributed_by": "group 2", "title": "", "section": "", "text": "Neural networks are trained by optimization algorithms like gradient descent.Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling."}, {"id": "2731", "contributed_by": "group 10", "title": "", "section": "", "text": "Neural language models represent words in this prior context by their embeddings, rather than just by their word identity as used in n-gram language models. Using embeddings allows neural language models to generalize better to unseen data."}, {"id": "2770", "contributed_by": "group 10", "title": "", "section": "", "text": "Weight tying is a technique used in neural network architectures to reduce the number of parameters in the model and improve its efficiency. In some architectures, such as language models, there are redundant sets of weights that perform similar functions, such as the input and output word embeddings. Weight tying dispenses with this redundancy and simply uses a single set of embeddings at both the input and output layers. This technique has been shown to be effective in reducing overfitting and improving generalization, particularly in tasks such as language modeling and machine translation. By sharing the same set of weights across different parts of the model, weight tying encourages the network to learn more meaningful representations of the input data and make more accurate predictions."}, {"id": "2800", "contributed_by": "group 10", "title": "", "section": "", "text": "This vector is sometimes called the sentence embedding since it refers to the entire sequence, although the term 'sentence embedding' is also used in other ways. In BERT, the [CLS] token plays the role of this embedding. This unique token is added to the vocabulary and is prepended to the start of all input sequences, both during pretraining and encoding. The output vector in the final layer of the model for the [CLS] input represents the entire input sequence and serves as the input to a classifier head, a logistic regression or neural network classifier that makes the relevant decision."}, {"id": "2965", "contributed_by": "group 2", "title": "", "section": "", "text": "Note that we formed the embedding layer e by concatenating the 3 embeddings for the three context vectors; we will often use semicolons to mean concatenation of vectors.  In the next section we will introduce a general algorithm for training neural networks, and then return to how to specifically train the neural language model in Section 7.7."}, {"id": "3574", "contributed_by": "group 2", "title": "", "section": "", "text": "A weakness of this approach is that it doesn't distinguish the use of a word's embedding as the beginning of a span from its use as the end of one. Therefore, more elaborate schemes for representing the span boundaries involve learned representations for start and end points through the use of two distinct feedforward networks:"}, {"id": "2557", "contributed_by": "group 1", "title": "Vector Semantics and Embeddings", "section": "", "text": "Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'."}, {"id": "2960", "contributed_by": "group 2", "title": "", "section": "", "text": "the probability Neural language models represent words in this prior context by their embeddings, rather than just by their word identity as used in n-gram language models.  Using embeddings allows neural language models to generalize better to unseen data. For example, suppose we ve seen this sentence in training: I have to make sure that the cat gets fed.  but have never seen the words 'gets fed' after the word 'dog'. Our test set has the prefix 'I forgot to make sure that the dog gets'. What s the next word? An n-gram language model will predict 'fed' after 'that the cat gets', but not after 'that the dog gets'. But a neural LM, knowing that 'cat' and 'dog' have similar embeddings, will be able to generalize from the 'cat' context to assign a high enough probability to 'fed' even after seeing 'dog'."}, {"id": "2832", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "Thesauruses have also been used to improve both static and contextual word embeddings. For example, static word embeddings have a problem with antonyms. A word like expensive is often very similar in embedding cosine to its antonym like cheap. Antonymy information from thesauruses can help solve this problem; There are two families of solutions. The first requires retraining: we modify the embedding training to incorporate thesaurus relations like synonymy, antonym, or supersenses. This can be done by modifying the static embedding loss function for word2vec (Yu and Dredze 2014, Nguyen et al. 2016) or by modifying contextual embedding training (Levine et al. 2020, Lauscher et al. 2019). The second, for static embeddings, is more light-weight; after the embeddings have been trained we learn a second mapping based on a thesaurus that shifts the embeddings of words in such a way that synonyms (according to the thesaurus) are pushed closer and antonyms further apart. Such methods are called retrofitting (Faruqui et al. 2015, Lengerich et al. 2018) or counterfitting (Mrksi c et al.  , 2016)."}, {"id": "3541", "contributed_by": "group 2", "title": "", "section": "", "text": "The result of this pretraining process consists of both learned word embeddings, as well as all the parameters of the bidirectional encoder that are used to produce contextual embeddings for novel inputs."}], "metadata": {}, "id": "10"}
{"query": "question: How does backprop (back propagation) works answer: <extra_id_0>", "answers": ["Backprop computes each of the partial derivatives along each edge of the computation graph from right to left using the chain rule in Calculus, until we have annotated the graph all the way to all the input variables."], "generation": "Backpropagation through time (BPTT) is a general approach for training weights in RNNs. It involves processing the input sequence in the forward direction Back BPTT BPTT BPTT", "passages": [{"id": "2765", "contributed_by": "group 10", "title": "", "section": "", "text": "Backpropagation Through Time (BPTT) is a general approach for training the weights in RNNs. It involves processing the input sequence in the forward direction to generate the output sequence, and then using the output sequence to compute the loss function. The loss function is then backpropagated through the network in the reverse direction using the chain rule of calculus, allowing the gradients to be computed for all the weights in the network. However, because the RNN is unrolled over time, the gradients can become unstable and either vanish or explode. To address this problem, various modifications have been proposed, such as gradient clipping and using different activation functions. Despite its challenges, BPTT remains a popular method for training RNNs and has been used in many applications, including speech recognition and machine translation."}, {"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "2764", "contributed_by": "group 10", "title": "", "section": "", "text": "The backpropagation algorithm for training weights in RNNs is a two-pass process. In the first pass, forward inference is performed to generate the output sequence. The output sequence is then compared to the target sequence, and the loss function is computed. In the second pass, the gradients of the loss function with respect to the weights are computed by backpropagating the errors through the network in reverse order. This is known as the backpropagation through time (BPTT) algorithm. However, because the RNN is unrolled over time, the gradients can become unstable and either vanish or explode. To address this problem, various modifications have been proposed, such as gradient clipping and using different activation functions. Despite its challenges, BPTT remains a popular method for training RNNs and has been used in many applications, including speech recognition and machine translation."}, {"id": "2991", "contributed_by": "group 2", "title": "", "section": "", "text": "Neural networks are trained by optimization algorithms like gradient descent.Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling."}, {"id": "2791", "contributed_by": "group 10", "title": "", "section": "", "text": "Back-translation works well when there is a large parallel corpus available for the target language. In such cases, the model can be trained to translate in both directions, which leads to better generalization. However, back-translation is not always feasible, as it requires a large parallel corpus for the target language."}, {"id": "3465", "contributed_by": "group 2", "title": "", "section": "", "text": "Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts."}, {"id": "3424", "contributed_by": "group 2", "title": "", "section": "", "text": "In general backtranslation works surprisingly well; one estimate suggests that a system trained on backtranslated text gets about 2/3 of the gain as would training on the same amount of natural bitext (Edunov et al., 2018)."}, {"id": "3228", "contributed_by": "group 2", "title": "", "section": "", "text": "This modularity is key to the power and widespread applicability of LSTM units. LSTM units (or other varieties, like GRUs) can be substituted into any of the network architectures described in Section 9.5. And, as with simple RNNs, multi-layered networks making use of gated units can be unrolled into deep feedforward networks and trained in the usual fashion with backpropagation."}, {"id": "1891", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "The probability for a backoff n-gram PBO is thus computed as follows: if C(wnN+1:n) > 0 Katz backoff is often combined with a smoothing method called Good-Turing.."}, {"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}], "metadata": {}, "id": "11"}
{"query": "question: What is dropout and what is its purpose answer: <extra_id_0>", "answers": ["It is to randomly drop some neural units and their connections from the network during training, a form of regularization to reduce overfitting"], "generation": "During training, some units and their connections are randomly dropped from the network during training.", "passages": [{"id": "2737", "contributed_by": "group 10", "title": "", "section": "", "text": "Various forms of regularization are used to prevent overfitting. One of the most important is dropout: randomly dropping some units and their connections from the network during training"}, {"id": "2983", "contributed_by": "group 2", "title": "", "section": "", "text": "Various forms of regularization are used to prevent overfitting. One of the most dropout important is dropout: randomly dropping some units and their connections from the network during training (Hinton et al. 2012, Srivastava et al. 2014). Tuning hyperparameter of hyperparameters is also important. The parameters of a neural network are the weightsW and biases b; those are learned by gradient descent. The hyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training set. Hyperparameters include the learning rate h, the mini-batch size, the model architecture (the number of layers, the number of hidden nodes per layer, the choice of activation functions), how to regularize, and so on. Gradient descent itself also has many architectural variants such as Adam (Kingma and Ba, 2015)."}, {"id": "44", "contributed_by": "group 9", "title": "", "section": "", "text": "The learning rate is a hyperparameter that must be adjusted. If its too high, the learner will take steps that are too large, overshooting the minimum of the loss function. If its too low, the learner will take steps that are too small, and take too long to get to the minimum. It is common to start with a higher learning rate and then slowly decrease it, so that it is a function of the iteration k of training; the notation can be used to mean the value of the learning rate at iteration k."}, {"id": "2465", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "It is common to start with a higher learning rate and then slowly decrease it, so that it is a function of the iteration k of training; the notation k can be used to mean the value of the learning rate at iteration k."}, {"id": "3128", "contributed_by": "group 2", "title": "", "section": "", "text": "The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence"}, {"id": "3014", "contributed_by": "group 2", "title": "", "section": "", "text": "Auxiliary verbs mark semantic features of a main verb such as its tense, whether it is completed (aspect), whether it is negated (polarity), and whether an action is necessary, possible, suggested, or desired (mood). English auxiliaries include the copula verb be, the two verbs do and have, forms, as well as modal verbs used to mark the mood associated with the event depicted by the main verb: can indicates"}, {"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}, {"id": "3198", "contributed_by": "group 2", "title": "", "section": "", "text": "Stacked recurrent networks. The output of a lower level serves as the input to higher levels with the output of the last network serving as the final output."}, {"id": "3210", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN for sequence classification. The final hidden units from the forward and backward passes are combined to represent the entire sequence. This combined representation serves as input to the subsequent classifier."}, {"id": "3125", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words."}], "metadata": {}, "id": "12"}
{"query": "question: What is the difference between parameters and hyperparameters answer: <extra_id_0>", "answers": ["The parameters of a neural network are the weights and biases learned by gradient descent, whereas hyperparameters are things that are chosen by the algorithm designer"], "generation": "Unlike regular parameters of a model (weightsW and biases b), which are learned by gradient descent, hyperparameters are things that", "passages": [{"id": "2467", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "Unlike regular parameters of a model (weights like w and b), which are learned by the algorithm from the training set, hyperparameters are special parameters chosen by the algorithm designer that affect how the algorithm works."}, {"id": "2983", "contributed_by": "group 2", "title": "", "section": "", "text": "Various forms of regularization are used to prevent overfitting. One of the most dropout important is dropout: randomly dropping some units and their connections from the network during training (Hinton et al. 2012, Srivastava et al. 2014). Tuning hyperparameter of hyperparameters is also important. The parameters of a neural network are the weightsW and biases b; those are learned by gradient descent. The hyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training set. Hyperparameters include the learning rate h, the mini-batch size, the model architecture (the number of layers, the number of hidden nodes per layer, the choice of activation functions), how to regularize, and so on. Gradient descent itself also has many architectural variants such as Adam (Kingma and Ba, 2015)."}, {"id": "2835", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "This chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights:  A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.  Many words are polysemous, having many senses.  Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.  WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.  Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.  The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.  Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.  An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.  Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target words neighborhood.  Word sense induction is the task of learning word senses unsupervised."}, {"id": "3435", "contributed_by": "group 2", "title": "", "section": "", "text": "The metric then computes an F-score by combining chrP and chrR using a weighting parameter beta . It is common to set beta = 2, thus weighing recall twice as much as precision:"}, {"id": "2157", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "The harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recipa1 + 1 a2 + 1 a3 +...+ 1 and hence F-measure is P +(1) 1 with  2 = 1 F = ( 2 +1)PR TEST SETS AND CROSS-VALIDATION Harmonic mean is used because it is a conservative metric; the harmonic mean of two values is closer to the minimum of the two values than the arithmetic mean is.."}, {"id": "2820", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "Another way word senses can be related is taxonomically. A word (or sense) is a hyponym of another word or sense if the first is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, hypernym and mango is a hyponym of fruit. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. It is unfortunate that the two words (hypernym and hyponym) are very similar and hence easily confused; for this reason, the word superordinate is often used instead of hypernym. We can define hypernymy more formally by saying that the class denoted by the superordinate extensionally includes the class denoted by the hyponym. Thus, the class of animals includes as members all dogs, and the class of moving actions includes all walking actions. Hypernymy can also be defined in terms of entailment. Under this definition, a sense A is a hyponym of a sense B if everything that is A is also B, and hence being an A entails being a B, or x A(x)  B(x). Hyponymy/hypernymy is usually a transitive relation; if A is a hyponym of B and B is a hyponym of C, then A is a hyponym of C. Another name for the hypernym/hyponym structure is the IS-A hierarchy, in which we say A IS-A B, or B subsumes A."}, {"id": "2466", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "We 'll discuss hyperparameters in more detail in Chapter 7, but briey they are a special kind of parameter for any machine learning model."}, {"id": "2686", "contributed_by": "group 8", "title": "", "section": "", "text": "Constants in FOL refer to specific objects in the world being described. Such constants are conventionally depicted as either single capitalized letters such as A and B or single capitalized words that are often reminiscent of proper nouns such as Maharani and Harry. Like programming language constants, FOL constants refer to exactly one object. Objects can, however, have multiple constants that refer to them."}, {"id": "3175", "contributed_by": "group 2", "title": "", "section": "", "text": "In addition to providing improved model perplexity, this approach significantly reduces the number of parameters required for the model."}, {"id": "2739", "contributed_by": "group 10", "title": "", "section": "", "text": "Part-of-speech taggers are evaluated by the standard metric of accuracy. Named entity recognizers are evaluated by recall, precision, and F1 measure. Recall that recall is the ratio of the number of correctly labeled responses to the total that should have been labeled; precision is the ratio of the number of correctly labeled responses to the total labeled; and F-measure is the harmonic mean of the two."}], "metadata": {}, "id": "13"}
{"query": "question: What are some examples of hyperparameters answer: <extra_id_0>", "answers": ["The learning rate, the mini-batch size, the number of layers, the number of hidden nodes per layer, the choice of activation functions, and the method of regularization"], "generation": "Hyperparameters include the learning rate h, the mini-batch size, the model architecture (the number of hidden nodes per layer), how", "passages": [{"id": "2467", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "Unlike regular parameters of a model (weights like w and b), which are learned by the algorithm from the training set, hyperparameters are special parameters chosen by the algorithm designer that affect how the algorithm works."}, {"id": "2835", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "This chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights:  A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.  Many words are polysemous, having many senses.  Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.  WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.  Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.  The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.  Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.  An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.  Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target words neighborhood.  Word sense induction is the task of learning word senses unsupervised."}, {"id": "2820", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "Another way word senses can be related is taxonomically. A word (or sense) is a hyponym of another word or sense if the first is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, hypernym and mango is a hyponym of fruit. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. It is unfortunate that the two words (hypernym and hyponym) are very similar and hence easily confused; for this reason, the word superordinate is often used instead of hypernym. We can define hypernymy more formally by saying that the class denoted by the superordinate extensionally includes the class denoted by the hyponym. Thus, the class of animals includes as members all dogs, and the class of moving actions includes all walking actions. Hypernymy can also be defined in terms of entailment. Under this definition, a sense A is a hyponym of a sense B if everything that is A is also B, and hence being an A entails being a B, or x A(x)  B(x). Hyponymy/hypernymy is usually a transitive relation; if A is a hyponym of B and B is a hyponym of C, then A is a hyponym of C. Another name for the hypernym/hyponym structure is the IS-A hierarchy, in which we say A IS-A B, or B subsumes A."}, {"id": "1058", "contributed_by": "group 3", "title": "chapter 12", "section": "", "text": "Ordinal numbers include first, second, third, and so on, but also words like next, last, past, other, and another. Some quantifiers (many, (a) few, several) occur only with plural count nouns. Adjectives occur after quantifiers but before nouns. Adjectives can also be grouped into a phrase called an adjective phrase or AP. APs can have an adverb before the adjective (see Chapter 8 for definitions of adjectives and adverbs): A head noun can be followed by postmodifiers. Three kinds of nominal postmodifiers are common in English: They are especially common in the ATIS corpus since they are used to mark the origin and destination of flights. Here are some examples of prepositional phrase postmodifiers, with brackets inserted to show the boundaries of each PP; note that two or more PPs can be strung together within a single NP. The three most common kinds of non-finite postmodifiers are the gerundive (- ing), -ed, and infinitive forms. Gerundive postmodifiers are so called because they consist of a verb phrase that begins with the gerundive (-ing) form of the verb. We can make rules for GerundVP constituents by duplicating all of our VP productions, substituting GerundV for V. The phrases in italics below are examples of the two other common kinds of non-finite clauses, infinitives and -ed forms. A postnominal relative clause (more correctly a restrictive relative clause), is a clause that often begins with a relative pronoun (that and who are the most common)."}, {"id": "2010", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "Various sets of subject categories exist, such as the MeSH (Medical Subject Headings) thesaurus.."}, {"id": "2983", "contributed_by": "group 2", "title": "", "section": "", "text": "Various forms of regularization are used to prevent overfitting. One of the most dropout important is dropout: randomly dropping some units and their connections from the network during training (Hinton et al. 2012, Srivastava et al. 2014). Tuning hyperparameter of hyperparameters is also important. The parameters of a neural network are the weightsW and biases b; those are learned by gradient descent. The hyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training set. Hyperparameters include the learning rate h, the mini-batch size, the model architecture (the number of layers, the number of hidden nodes per layer, the choice of activation functions), how to regularize, and so on. Gradient descent itself also has many architectural variants such as Adam (Kingma and Ba, 2015)."}, {"id": "2686", "contributed_by": "group 8", "title": "", "section": "", "text": "Constants in FOL refer to specific objects in the world being described. Such constants are conventionally depicted as either single capitalized letters such as A and B or single capitalized words that are often reminiscent of proper nouns such as Maharani and Harry. Like programming language constants, FOL constants refer to exactly one object. Objects can, however, have multiple constants that refer to them."}, {"id": "2640", "contributed_by": "group 8", "title": "Chapter 23", "section": "", "text": "chair dog We can define hypernymy more formally by saying that the class denoted by the superordinate extensionally includes the class denoted by the hyponym. Thus, the class of animals includes as members all dogs, and the class of moving actions in cludes all walking actions. Hypernymy can also be defined in terms of entailment . Under this definition, a sense Ais a hyponym of a sense Bif everything that is Ais alsoB, and hence being an Aentails being a B, or8x A(x))B(x). Hyponymy/hy pernymy is usually a transitive relation; if A is a hyponym of B and B is a hyponym of C, then A is a hyponym of C. Another name for the hypernym/hyponym structure is the IS-A hierarchy, in which we say A IS-A B, or B subsumes A . IS-A Hypernymy is useful for tasks like textual entailment or question answering; knowing that leukemia is a type of cancer , for example, would certainly be useful in answering questions about leukemia. Meronymy Another common relation is meronymy , the part-whole relation. A legis part of a part-whole chair ; awheel is part of a car. We say that wheel is ameronym ofcar, and caris a holonym ofwheel . Structured Polysemy The senses of a word can also be related semantically, in which case we call the relationship between them structured polysemy . Consider this sense bank :structured polysemy (23.11) The bank is on the corner of Nassau and Witherspoon. This sense, perhaps bank4, means something like the building belonging to a financial"}, {"id": "3000", "contributed_by": "group 2", "title": "", "section": "", "text": "Four major open classes occur in the languages of the world: nouns (including proper nouns), verbs, adjectives, and adverbs, as well as the smaller open class of interjections. English has all five, although not every language does."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}], "metadata": {}, "id": "14"}
{"query": "question: Why is the advantage of training the word embeddings as part of the optimization answer: <extra_id_0>", "answers": ["Doing so is useful when the task the network is designed for, such as sentiment classification, translation, or parsing, places strong constraints on what makes a good representation for words."], "generation": "Word2vec embeddings are static embeddings, meaning that the method learns one fixed embedding for each word in the vocabulary learned during the training process.", "passages": [{"id": "2835", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "This chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights:  A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.  Many words are polysemous, having many senses.  Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.  WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.  Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.  The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.  Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.  An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.  Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target words neighborhood.  Word sense induction is the task of learning word senses unsupervised."}, {"id": "2832", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "Thesauruses have also been used to improve both static and contextual word embeddings. For example, static word embeddings have a problem with antonyms. A word like expensive is often very similar in embedding cosine to its antonym like cheap. Antonymy information from thesauruses can help solve this problem; There are two families of solutions. The first requires retraining: we modify the embedding training to incorporate thesaurus relations like synonymy, antonym, or supersenses. This can be done by modifying the static embedding loss function for word2vec (Yu and Dredze 2014, Nguyen et al. 2016) or by modifying contextual embedding training (Levine et al. 2020, Lauscher et al. 2019). The second, for static embeddings, is more light-weight; after the embeddings have been trained we learn a second mapping based on a thesaurus that shifts the embeddings of words in such a way that synonyms (according to the thesaurus) are pushed closer and antonyms further apart. Such methods are called retrofitting (Faruqui et al. 2015, Lengerich et al. 2018) or counterfitting (Mrksi c et al.  , 2016)."}, {"id": "2989", "contributed_by": "group 2", "title": "", "section": "", "text": "Training the parameters to minimize loss will result both in an algorithm for language modeling (a word predictor) but also a new set of embeddings E that can be used as word representations for other tasks."}, {"id": "2656", "contributed_by": "group 8", "title": "Chapter 23", "section": "", "text": "hyponymy and hypernymy . WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages. Word-sense disambiguation (WSD ) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words ( lexical sample task ) or all words (all-words task ) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses. BIBLIOGRAPHICAL AND HISTORICAL NOTES 473 The standard supervised algorithm for WSD is nearest neighbors with contex tual embeddings. Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well. An important baseline for WSD is the most frequent sense , equivalent, in WordNet, to take the first sense . Another baseline is a knowledge-based WSD algorithm called the Lesk al gorithm which chooses the sense whose dictionary definition shares the most words with the target word's neighborhood. Word sense induction is the task of learning word senses unsupervised. Bibliographical and Historical Notes Word sense disambiguation traces its roots to some of the earliest applications of digital computers. The insight that underlies modern algorithms for word sense dis ambiguation was first articulated by Weaver (1949/1955) in the context of machine translation: If one examines the words in a book, one at a time as through an opaque mask with a hole in it one word wide, then it is obviously impossible to determine, one at a time, the meaning"}, {"id": "3541", "contributed_by": "group 2", "title": "", "section": "", "text": "The result of this pretraining process consists of both learned word embeddings, as well as all the parameters of the bidirectional encoder that are used to produce contextual embeddings for novel inputs."}, {"id": "2557", "contributed_by": "group 1", "title": "Vector Semantics and Embeddings", "section": "", "text": "Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'."}, {"id": "3173", "contributed_by": "group 2", "title": "", "section": "", "text": "The weights in the network are adjusted to minimize the average CE loss over the training sequence via gradient descent. Fig. 9.6 illustrates this training regimen. Careful readers may have noticed that the input embedding matrix E and the final layer matrix V, which feeds the output softmax, are quite similar. The columns of E represent the word embeddings for each word in the vocabulary learned during the training process with the goal that words that have similar meaning and function will have similar embeddings. And, since the length of these embeddings corresponds to"}, {"id": "3519", "contributed_by": "group 2", "title": "", "section": "", "text": "More specifically, the original input sequence is first tokenized using a subword model. The sampled items which drive the learning process are chosen from among the set of tokenized inputs. Word embeddings for all of the tokens in the input are retrieved from the word embedding matrix and then combined with positional embeddings to form the input to the transformer."}, {"id": "3178", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech tagging as sequence labeling with a simple RNN. Pre-trained word embeddings serve as inputs and a softmax layer provides a probability distribution over the part-of-speech tags as output at each time step."}, {"id": "2558", "contributed_by": "group 1", "title": "Vector Semantics and Embeddings", "section": "", "text": "In this section we introduce one method for computing embeddings: skip-gram SGNS with negative sampling, sometimes called SGNS. The skip-gram algorithm is one word2vec of two algorithms in a software package called word2vec, and so sometimes the algorithm is loosely referred to as word2vec (Mikolov et al. 2013a, Mikolov et al. 2013b). The word2vec methods are fast, efficient to train, and easily available online with code and pretrained embeddings. Word2vec embeddings are static embeddings, meaning that the method learns one fixed embedding for each word in the static embeddings vocabulary. word2vec simplifies the task (making it binary classification instead of word prediction). Second, word2vec simplifies the architecture (training a logistic regression classifier instead of a multi-layer neural network with hidden layers that demand more sophisticated training algorithms)."}], "metadata": {}, "id": "15"}
{"query": "question: What is neural language model answer: <extra_id_0>", "answers": ["A probabilistic classifier implemented as a neural network that computes the probability of the next word given the previous n words."], "generation": "Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. A neural language model", "passages": [{"id": "2731", "contributed_by": "group 10", "title": "", "section": "", "text": "Neural language models represent words in this prior context by their embeddings, rather than just by their word identity as used in n-gram language models. Using embeddings allows neural language models to generalize better to unseen data."}, {"id": "2991", "contributed_by": "group 2", "title": "", "section": "", "text": "Neural networks are trained by optimization algorithms like gradient descent.Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "2729", "contributed_by": "group 10", "title": "", "section": "", "text": "Compared to n-gram models, neural language models can handle much longer histories, can generalize better over contexts of similar words, and are more accurate at word-prediction. On the other hand, neural net language models are much more complex, are slower and need more energy to train, and are less interpretable than n-gram models, so for many (especially smaller) tasks an n-gram language model is still the right tool"}, {"id": "2730", "contributed_by": "group 10", "title": "", "section": "", "text": "Compared to n-gram models, neural language models can handle much longer histories, can generalize better over contexts of similar words, and are more accurate at word-prediction. On the other hand, neural net language models are much more complex, are slower and need more energy to train, and are less interpretable than n-gram models, so for many (especially smaller) tasks an n-gram language model is still the right tool"}, {"id": "3592", "contributed_by": "group 2", "title": "", "section": "", "text": "Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input."}, {"id": "3549", "contributed_by": "group 2", "title": "", "section": "", "text": "is added to the vocabulary and is prepended to the start of all input sequences, both during pretraining and encoding. The output vector in the final layer of the model for the [CLS] input represents the entire input sequence and serves as the input to a classifier head, a logistic regression or neural network classifier that makes the relevant decision."}, {"id": "1626", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "Models that assign probabilities to sequences of words are called language models or LMs.."}, {"id": "3290", "contributed_by": "group 2", "title": "", "section": "", "text": "Common language-based applications for RNNs and transformers include: Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. Auto-regressive generation using a trained language model. Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification."}, {"id": "2808", "contributed_by": "group 10", "title": "", "section": "", "text": "We call fine-tuning the process of taking the representations from these pretrained models, and further training the model, often via an added neural net classifier, to perform some down- stream task like named entity tagging or question answering or coreference. The intuition is that the pretraining phase learns a language model that instantiates a rich representations of word meaning, that thus enables the model to more easily learn ('be fine-tuned to') the requirements of a downstream language understanding task."}], "metadata": {}, "id": "16"}
{"query": "question: What is the perceptron answer: <extra_id_0>", "answers": ["The perceptron is a very simple neural unit that has a binary output and does not have a non-linear activation function"], "generation": "0 or 1? The perceptron is a very simple neural unit that has a binary output and does not have a non-linear activation function. 0", "passages": [{"id": "2929", "contributed_by": "group 2", "title": "", "section": "", "text": "This example was first shown for the perceptron, which is a very simple neural unit that has a binary output and does not have a non-linear activation function. The output y of a perceptron is 0 or 1, and is computed as follows (using the same weight w, input x, and bias b as in Eq. 7.2):"}, {"id": "2640", "contributed_by": "group 8", "title": "Chapter 23", "section": "", "text": "chair dog We can define hypernymy more formally by saying that the class denoted by the superordinate extensionally includes the class denoted by the hyponym. Thus, the class of animals includes as members all dogs, and the class of moving actions in cludes all walking actions. Hypernymy can also be defined in terms of entailment . Under this definition, a sense Ais a hyponym of a sense Bif everything that is Ais alsoB, and hence being an Aentails being a B, or8x A(x))B(x). Hyponymy/hy pernymy is usually a transitive relation; if A is a hyponym of B and B is a hyponym of C, then A is a hyponym of C. Another name for the hypernym/hyponym structure is the IS-A hierarchy, in which we say A IS-A B, or B subsumes A . IS-A Hypernymy is useful for tasks like textual entailment or question answering; knowing that leukemia is a type of cancer , for example, would certainly be useful in answering questions about leukemia. Meronymy Another common relation is meronymy , the part-whole relation. A legis part of a part-whole chair ; awheel is part of a car. We say that wheel is ameronym ofcar, and caris a holonym ofwheel . Structured Polysemy The senses of a word can also be related semantically, in which case we call the relationship between them structured polysemy . Consider this sense bank :structured polysemy (23.11) The bank is on the corner of Nassau and Witherspoon. This sense, perhaps bank4, means something like the building belonging to a financial"}, {"id": "2820", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "Another way word senses can be related is taxonomically. A word (or sense) is a hyponym of another word or sense if the first is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, hypernym and mango is a hyponym of fruit. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. It is unfortunate that the two words (hypernym and hyponym) are very similar and hence easily confused; for this reason, the word superordinate is often used instead of hypernym. We can define hypernymy more formally by saying that the class denoted by the superordinate extensionally includes the class denoted by the hyponym. Thus, the class of animals includes as members all dogs, and the class of moving actions includes all walking actions. Hypernymy can also be defined in terms of entailment. Under this definition, a sense A is a hyponym of a sense B if everything that is A is also B, and hence being an A entails being a B, or x A(x)  B(x). Hyponymy/hypernymy is usually a transitive relation; if A is a hyponym of B and B is a hyponym of C, then A is a hyponym of C. Another name for the hypernym/hyponym structure is the IS-A hierarchy, in which we say A IS-A B, or B subsumes A."}, {"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "3460", "contributed_by": "group 2", "title": "", "section": "", "text": "The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder."}, {"id": "2010", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "Various sets of subject categories exist, such as the MeSH (Medical Subject Headings) thesaurus.."}, {"id": "2101", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "Four popular lexicons are the General Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon of Hu and Liu (2004a) and the MPQA Subjectivity Lexicon (Wilson et al., 2005).."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "3128", "contributed_by": "group 2", "title": "", "section": "", "text": "The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence"}, {"id": "3288", "contributed_by": "group 2", "title": "", "section": "", "text": "Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, using attention heads that model how the surrounding words are relevant for the processing of the current word."}], "metadata": {}, "id": "17"}
{"query": "question: Why do we need deep neural network answer: <extra_id_0>", "answers": ["A neural unit cannot compute some very simple functions, such as XOR, but a multi-layer neural network with non-linear activation can"], "generation": "Deep neural network is a powerful classifier that can learn any function. Deep neural network is often deep (have many layers) Because deep neural network Firstly Because deep", "passages": [{"id": "2991", "contributed_by": "group 2", "title": "", "section": "", "text": "Neural networks are trained by optimization algorithms like gradient descent.Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling."}, {"id": "2727", "contributed_by": "group 10", "title": "", "section": "", "text": "One of the reasons we use nonlinear activation functions for each layer in a neural network is that if we did not, the resulting network is exactly equivalent to a single-layer network.This generalizes to any number of layers. So without non-linear activation functions, a multilayer network is just a notational variant of a single layer network with a different set of weights, and we lose all the representational power of multilayer networks."}, {"id": "3226", "contributed_by": "group 2", "title": "", "section": "", "text": "Basic neural units used in feedforward, simple recurrent networks (SRN), and long short-term memory (LSTM)."}, {"id": "3198", "contributed_by": "group 2", "title": "", "section": "", "text": "Stacked recurrent networks. The output of a lower level serves as the input to higher levels with the output of the last network serving as the final output."}, {"id": "3288", "contributed_by": "group 2", "title": "", "section": "", "text": "Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, using attention heads that model how the surrounding words are relevant for the processing of the current word."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "3549", "contributed_by": "group 2", "title": "", "section": "", "text": "is added to the vocabulary and is prepended to the start of all input sequences, both during pretraining and encoding. The output vector in the final layer of the model for the [CLS] input represents the entire input sequence and serves as the input to a classifier head, a logistic regression or neural network classifier that makes the relevant decision."}, {"id": "2921", "contributed_by": "group 2", "title": "", "section": "", "text": "At its heart, a neural unit is taking a weighted sum of its inputs, with one additional term in the sum called a bias bias term term. Given a set of inputs x1:::xn, a unit has a set of corresponding weights w1:::wn and a bias b, so the weighted sum z can be represented as:"}, {"id": "2777", "contributed_by": "group 10", "title": "", "section": "", "text": "Simple recurrent networks can fail on long inputs because of problems like vanishing gradients. Modern systems use more complex gated architectures such as LSTMs and GRUs that explicitly decide what to remember and forget in their hidden and context layers. These models also allow for better gradient flow through time and can handle longer sequences than their simpler counterparts."}, {"id": "2917", "contributed_by": "group 2", "title": "", "section": "", "text": "Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we feedforward introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often deep learning called deep learning, because modern networks are often deep (have many layers).  Neural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single 'hidden layer ) can be shown to learn any function."}], "metadata": {}, "id": "18"}
{"query": "question: What is the cross-entropy loss answer: <extra_id_0>", "answers": ["The cross-entropy loss is the negative log of the output probability corresponding to the correct class. It is also call the negative log likelihood loss"], "generation": "The cross-entropy loss is simply the negative log probability of the output probability corresponding to the correct class. The CE loss is simply the negative log 0 The 0", "passages": [{"id": "2406", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "The resulting loss function is the negative log likelihood loss, generally called the cross-entropy loss."}, {"id": "2969", "contributed_by": "group 2", "title": "", "section": "", "text": "In other words, the cross-entropy loss is simply the negative log of the output probability corresponding to the correct class, and we therefore also call this the negative negative log log likelihood loss:"}, {"id": "2734", "contributed_by": "group 10", "title": "", "section": "", "text": "In other words, the cross-entropy loss is simply the negative log of the output probability corresponding to the correct class, and we therefore also call this the negative log likelihood loss"}, {"id": "2735", "contributed_by": "group 10", "title": "", "section": "", "text": "In other words, the cross-entropy loss is simply the negative log of the output probability corresponding to the correct class, and we therefore also call this the negative log likelihood loss"}, {"id": "2513", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "Hence the cross-entropy loss is simply the log of the output probability corresponding to the correct class, and we therefore also call Eq. 5.45 the negative log likelihood loss."}, {"id": "2401", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "In the next section we 'll introduce the loss function that is commonly used for logistic regression and also for neural networks, the cross-entropy loss."}, {"id": "2767", "contributed_by": "group 10", "title": "", "section": "", "text": "In language modeling, the goal is to learn a model that can generate the correct next word in a sequence of words. One way to train such a model is to use the cross-entropy loss as the objective function. The cross-entropy loss is determined by the probability that the RNN language model assigns to the correct next word. Specifically, at each time step, the model generates a probability distribution over the possible next words in the sequence, and the cross-entropy loss is calculated by taking the negative log probability of the correct next word. By minimizing the cross-entropy loss, the model is trained to assign higher probabilities to the correct next words, and thus generate more accurate predictions. This approach has been used in a variety of language modeling tasks, and is a key component of many state-of-the-art language models."}, {"id": "2539", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": " Multinomial logistic regression uses the softmax function to compute proba The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized."}, {"id": "3171", "contributed_by": "group 2", "title": "", "section": "", "text": "In the case of language modeling, the correct distribution yt comes from knowing the next word. This is represented as a one-hot vector corresponding to the vocabulary where the entry for the actual next word is 1, and all the other entries are 0. Thus, the cross-entropy loss for language modeling is determined by the probability the model assigns to the correct next word. So at time t the CE loss is the negative log probability the model assigns to the next word in the training sequence."}, {"id": "1983", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "Cross-entropy is dened in the limit as the length of the observed word sequence goes to innity.."}], "metadata": {}, "id": "19"}
{"query": "question: What is a one-hot vector answer: <extra_id_0>", "answers": ["A one-hot vector is a vector that has one element equal to one, while all the other elements are set to zero. The index that is hot corresponds to the word’s index in the vocabulary"], "generation": "A one-hot vector is a vector that has one element equal to 1-in the dimension corresponding to the vocabulary where the entry for the actual next word is 1. A vector", "passages": [{"id": "2369", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "A vector like this y, with one value=1 and the rest 0, is called a one-hot vector."}, {"id": "2961", "contributed_by": "group 2", "title": "", "section": "", "text": "Forward inference is the task, given an input, of running a forward pass on the network to produce a probability distribution over possible outputs, in this case next words. We first represent each of the N previous words as a one-hot vector of length onehot vector , i.e., with one dimension for each word in the vocabulary. A one-hot vector is a vector that has one element equal to 1-in the dimension corresponding to that word s index in the vocabulary- while all the other elements are set to zero. Thus in a one-hot representation for the word 'toothpaste'"}, {"id": "2962", "contributed_by": "group 2", "title": "", "section": "", "text": "The feedforward neural language model (sketched in Fig. 7.13) has a moving window that can see N words into the past. we will let N equal 3, so the 3 words are each represented as a one-hot vector. We then multiply these one-hot vectors by the embedding matrix E. The embedding weight matrix E has a column for each word, each a column vector of d dimensions, and hence has dimensionality d . Multiplying by a one-hot vector that has only one non-zero element xi = 1 simply selects out the relevant column vector for word i, resulting in the embedding for word i, as shown in Fig. 7.12."}, {"id": "3167", "contributed_by": "group 2", "title": "", "section": "", "text": "Forward inference in a recurrent language model proceeds exactly as described in Section 9.2.1. The input sequence X = [x1;...;xt;...;xN] consists of a series of word embeddings each represented as a one-hot vector of size |V | times 1, and the output prediction, y, is a vector representing a probability distribution over the vocabulary. At each step, the model uses the word embedding matrix E to retrieve the embedding for the current word, and then combines it with the hidden layer from the previous step to compute a new hidden layer. This hidden layer is then used to generate an output layer which is passed through a softmax layer to generate a probability distribution over the entire vocabulary. That is, at time t:"}, {"id": "3171", "contributed_by": "group 2", "title": "", "section": "", "text": "In the case of language modeling, the correct distribution yt comes from knowing the next word. This is represented as a one-hot vector corresponding to the vocabulary where the entry for the actual next word is 1, and all the other entries are 0. Thus, the cross-entropy loss for language modeling is determined by the probability the model assigns to the correct next word. So at time t the CE loss is the negative log probability the model assigns to the next word in the training sequence."}, {"id": "2964", "contributed_by": "group 2", "title": "", "section": "", "text": "Here s the algorithm in detail for our mini example: 1. Select three embeddings from E: Given the three previous words, we look up their indices, create 3 one-hot vectors, and then multiply each by the embedding matrix E. Consider. The one-hot vector for 'for  (index 35) is multiplied by the embedding matrix E, to give the first part of the first hidden embedding layer, the embedding layer. Since each column of the input matrix E is an layer embedding for a word, and the input is a one-hot column vector xi for word Vi, the embedding layer for input w will be Exi = ei, the embedding for word i. We now concatenate the three embeddings for the three context words to produce the embedding layer e.  2. Multiply by W: We multiply by W (and add b) and pass through the ReLU (or other) activation function to get the hidden layer h.  3. Multiply by U: h is now multiplied by U 4. Apply softmax: After the softmax, each node i in the output layer estimates the probability"}, {"id": "2549", "contributed_by": "group 1", "title": "Vector Semantics and Embeddings", "section": "", "text": "The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived from the distributions of word neighbors. Vectors for representing words are called embeddings."}, {"id": "2567", "contributed_by": "group 1", "title": "Vector Semantics and Embeddings", "section": "", "text": "One of the fundamental tenets of semantics, called the principle of contrast states that a difference in linguistic form is always associated with some difference in meaning. For example, the word H2O is used in scientific contexts and would be inappropriate in a hiking guide."}, {"id": "3258", "contributed_by": "group 2", "title": "", "section": "", "text": "Given these values, the vector components are normalized by subtracting the mean from each and dividing by the standard deviation. The result of this computation is a new vector with zero mean and a standard deviation of one."}, {"id": "2292", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "The dot product of two vectors a and b, written as a  b is the sum of the products of the corresponding elements of each vector."}], "metadata": {}, "id": "20"}
{"query": "question: What are the parts of speech answer: <extra_id_0>", "answers": ["Parts of speech (POS) are the basis for descriptions of European languages, and most commonly include noun, verb, pronoun, preposition, adverb, conjunction, participle, and article."], "generation": "Parts of speech fall into two broad categories: closed class and open class. Parts of speech fall into two broad categories: closed class and open class Parts Parts", "passages": [{"id": "3125", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words."}, {"id": "3178", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech tagging as sequence labeling with a simple RNN. Pre-trained word embeddings serve as inputs and a softmax layer provides a probability distribution over the part-of-speech tags as output at each time step."}, {"id": "3124", "contributed_by": "group 2", "title": "", "section": "", "text": "Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags."}, {"id": "86", "contributed_by": "group 11", "title": "Chapter 12 - Constituency Grammars", "section": "", "text": "The non-terminal associated with each word in the lexicon is its lexical category, or part of speech."}, {"id": "3140", "contributed_by": "group 2", "title": "", "section": "", "text": "See Householder (1995) for historical notes on parts of speech, and Sampson (1987) and Garside et al. (1997) on the provenance of the Brown and other tagsets."}, {"id": "2738", "contributed_by": "group 10", "title": "", "section": "", "text": "Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions new prepositions are rarely coined. By contrast, nouns and verbs are open classes new nouns and verbs like iPhone or to fax are continually being created or borrowed. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar"}, {"id": "2992", "contributed_by": "group 2", "title": "", "section": "", "text": "Dionysius Thrax of Alexandria (c. 100 B.C.), or perhaps someone else (it was a long time ago), wrote a grammatical sketch of Greek (a 'techne') that summarized the linguistic knowledge of his day. This work is the source of an astonishing proportion of modern linguistic vocabulary, including the words syntax, diphthong, clitic, and analogy. Also included are a description of eight parts of speech: noun, verb, pronoun, preposition, adverb, conjunction, participle, and article. Although earlier scholars (including Aristotle as well as the Stoics) had their own lists of parts of speech, it was Thrax's set of eight that became the basis for descriptions of European languages for the next 2000 years. (All the way to the Schoolhouse Rock educational television shows of our childhood, which had songs about 8 parts of speech, like the late great Bob Dorough's Conjunction Junction.) The durability of parts of speech through two millennia speaks to their centrality in models of human language."}, {"id": "2999", "contributed_by": "group 2", "title": "", "section": "", "text": "Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions- new prepositions are rarely coined. By contrast, nouns and verbs are open classes- new nouns and verbs like iPhone or to fax are continually being created or borrowed. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar."}, {"id": "3123", "contributed_by": "group 2", "title": "", "section": "", "text": "This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:"}, {"id": "3059", "contributed_by": "group 2", "title": "", "section": "", "text": "For part-of-speech tagging, the goal of HMM decoding is to choose the tag sequence t1 . . . tn that is most probable given the observation sequence of n words"}], "metadata": {}, "id": "21"}
{"query": "question: What is a named entity answer: <extra_id_0>", "answers": ["A named entity is a word or a multi-word phrase that is anything that can be referred to with a proper name: a person, a location, an organization."], "generation": "A named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. A named entity A named entity anything that", "passages": [{"id": "2741", "contributed_by": "group 10", "title": "", "section": "", "text": "A named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The task of named entity recognition (NER) is to find spans of text that constitute proper names and tag the type of the entity. Four entity tags are most common: PER (person), LOC (location), ORG (organization), or GPE (geo-political entity)."}, {"id": "3126", "contributed_by": "group 2", "title": "", "section": "", "text": "Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns."}, {"id": "2743", "contributed_by": "group 10", "title": "", "section": "", "text": "Unlike part-of-speech tagging, where there is no segmentation problem since each word gets one tag, the task of named entity recognition is to find and label spans of text, and is difficult partly because of the ambiguity of segmentation; we need to decide what's an entity and what isn't, and where the boundaries are."}, {"id": "3026", "contributed_by": "group 2", "title": "", "section": "", "text": "A named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The task of named entity recognition (NER) is to find spans of text that constitute proper names and tag the type of the entity. Four entity tags are most common: PER (person), LOC (location), ORG (organization), or GPE (geo-political entity). However, the term named entity is commonly extended to include things that aren't entities per se, including dates, times, and other kinds of temporal expressions, and even numerical expressions like prices. Here's an example of the output of an NER tagger:"}, {"id": "3113", "contributed_by": "group 2", "title": "", "section": "", "text": "The fact that named entity tagging has a segmentation component which is not present in tasks like text categorization or part-of-speech tagging causes some problems with evaluation. For example, a system that labeled Jane but not Jane Villanueva as a person would cause two errors, a false positive for O and a false negative for I-PER. In addition, using entities as the unit of response but words as the unit of training means that there is a mismatch between the training and test conditions."}, {"id": "2993", "contributed_by": "group 2", "title": "", "section": "", "text": "Proper names are another important and anciently studied linguistic category. While parts of speech are generally assigned to individual words or morphemes, a proper name is often an entire multiword phrase, like the name 'Marie Curie', the location 'New York City', or the organization 'Stanford University'. We'll use the term named entity for, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization, although as we'll see the term is commonly extended to include things that aren't entities per se."}, {"id": "3582", "contributed_by": "group 2", "title": "", "section": "", "text": "The second advantage to span-based approaches is that they naturally accommodate embedded named entities. For example, in this example both United Airlines and United Airlines Holding are legitimate named entities. The BIO approach has no way of encoding this embedded structure. But the span-based approach can naturally label both since the spans are labeled separately."}, {"id": "3578", "contributed_by": "group 2", "title": "", "section": "", "text": "A span-oriented approach to named entity classification. The figure only illustrates the computation for 2 spans corresponding to ground truth named entities. In reality, the network scores all of the spans in the text. That is, all the unigrams, bigrams, trigrams, etc. up to the length limit."}, {"id": "3562", "contributed_by": "group 2", "title": "", "section": "", "text": "A complication with this approach arises from the use of subword tokenization such as WordPiece or Byte Pair Encoding. Supervised training data for tasks like named entity recognition (NER) is typically in the form of BIO tags associated with text segmented at the word level. For example the following sentence containing two named entities:"}, {"id": "3123", "contributed_by": "group 2", "title": "", "section": "", "text": "This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:"}], "metadata": {}, "id": "22"}
{"query": "question: Why do we need POS answer: <extra_id_0>", "answers": ["They provide useful clues to sentence structure and meaning, and thus tagging parts of speech is a key part of parsing"], "generation": "POS tagging is a very important task in NER, and it is essential for the efficient use of NER. POS POS is POS", "passages": [{"id": "3099", "contributed_by": "group 2", "title": "", "section": "", "text": "A CRF for NER makes use of very similar features to a POS tagger, as shown in Figure 8.15."}, {"id": "3137", "contributed_by": "group 2", "title": "", "section": "", "text": "State of the art POS taggers use neural algorithms, either bidirectional RNNs or Transformers like BERT; see Chapter 9 and Chapter 11. HMM (Brants 2000; Thede and Harper 1999) and CRF tagger accuracies are likely just a tad lower."}, {"id": "219", "contributed_by": "group 11", "title": "Chapter 12 - Constituency Grammars", "section": "", "text": "The Penn Treebank project (whose POS tagset we introduced in Chapter 8) has produced treebanks from the Brown, Switchboard, ATIS, and Wall Street Journal corpora of English, as well as treebanks in Arabic and Chinese."}, {"id": "2754", "contributed_by": "group 10", "title": "", "section": "", "text": "While the HMM is a useful and powerful model, it turns out that HMMs need a number of augmentations to achieve high accuracy. For example, in POS tagging as in other tasks, we often run into unknown words: proper names and acronyms are created very often, and even new common nouns and verbs enter the language at a surprising rate."}, {"id": "3079", "contributed_by": "group 2", "title": "", "section": "", "text": "While the HMM is a useful and powerful model, it turns out that HMMs need a number of augmentations to achieve high accuracy. For example, in POS tagging as in other tasks, we often run into unknown words: proper names and acronyms are created very often, and even new common nouns and verbs enter the language at a surprising rate. It would be great to have ways to add arbitrary features to help with this, perhaps based on capitalization or morphology (words starting with capital letters are likely to be proper nouns, words ending with -ed tend to be past tense (VBD or VBN), etc.) Or knowing the previous or following words might be a useful feature (if the previous word is the, the current tag is unlikely to be a verb)."}, {"id": "807", "contributed_by": "group 11", "title": "Chapter 15 - Logical Representations of Sentence Meaning", "section": "", "text": "In backward chaining, modus ponens is run in reverse to prove specic propositions called queries."}, {"id": "3102", "contributed_by": "group 2", "title": "", "section": "", "text": "Some NER features for a sample sentence, assuming that Chicago and Villanueva are listed as locations in a gazetteer. We assume features only take on the values 0 or 1, so the first POS feature, for example, would be represented as 1{POS = NNP}."}, {"id": "3179", "contributed_by": "group 2", "title": "", "section": "", "text": "In this figure, the inputs at each time step are pre-trained word embeddings corresponding to the input tokens. The RNN block is an abstraction that represents an unrolled simple recurrent network consisting of an input layer, hidden layer, and output layer at each time step, as well as the shared U, V and W weight matrices that comprise the network. The outputs of the network at each time step represent the distribution over the POS tagset generated by a softmax layer."}, {"id": "2694", "contributed_by": "group 8", "title": "", "section": "", "text": "Modus ponens can be put to practical use in one of two ways: forward chaining and backward chaining. In forward chaining systems, modus ponens is used in precisely the manner just described. As individual facts are added to the knowledge base, modus ponens is used to fire all applicable implication rules. In this kind of arrangement, as soon as a new fact is added to the knowledge base, all applicable implication rules are found and applied, each resulting in the addition of new facts to the knowledge base. These new propositions in turn can be used to fire implication rules applicable to them. The process continues until no further facts can be deduced. The forward chaining approach has the advantage that facts will be present in the knowledge base when needed, because, in a sense all inference is performed in advance. This can substantially reduce the time needed to answer subsequent queries since they should all amount to simple lookups. The disadvantage of this approach is that facts that will never be needed may be inferred and stored."}, {"id": "170", "contributed_by": "group 11", "title": "Chapter 12 - Constituency Grammars", "section": "", "text": "Examples of cardinalordinal numbers quantiers numbers: two friends one stop  "}], "metadata": {}, "id": "23"}
{"query": "question: What is named entity recognition (NER) answer: <extra_id_0>", "answers": ["To recognize named entities in a sentence, and assign them tags like person, location, or organization."], "generation": "Named entity recognition (NER) is a task of text segmentation and named entity tagging. A named entity is, roughly speaking, anything that can be referred to with a proper name: NER NER is", "passages": [{"id": "2741", "contributed_by": "group 10", "title": "", "section": "", "text": "A named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The task of named entity recognition (NER) is to find spans of text that constitute proper names and tag the type of the entity. Four entity tags are most common: PER (person), LOC (location), ORG (organization), or GPE (geo-political entity)."}, {"id": "3026", "contributed_by": "group 2", "title": "", "section": "", "text": "A named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The task of named entity recognition (NER) is to find spans of text that constitute proper names and tag the type of the entity. Four entity tags are most common: PER (person), LOC (location), ORG (organization), or GPE (geo-political entity). However, the term named entity is commonly extended to include things that aren't entities per se, including dates, times, and other kinds of temporal expressions, and even numerical expressions like prices. Here's an example of the output of an NER tagger:"}, {"id": "3562", "contributed_by": "group 2", "title": "", "section": "", "text": "A complication with this approach arises from the use of subword tokenization such as WordPiece or Byte Pair Encoding. Supervised training data for tasks like named entity recognition (NER) is typically in the form of BIO tags associated with text segmented at the word level. For example the following sentence containing two named entities:"}, {"id": "2743", "contributed_by": "group 10", "title": "", "section": "", "text": "Unlike part-of-speech tagging, where there is no segmentation problem since each word gets one tag, the task of named entity recognition is to find and label spans of text, and is difficult partly because of the ambiguity of segmentation; we need to decide what's an entity and what isn't, and where the boundaries are."}, {"id": "3031", "contributed_by": "group 2", "title": "", "section": "", "text": "The standard approach to sequence labeling for a span-recognition problem like NER is BIO tagging (Ramshaw and Marcus, 1995). This is a method that allows us to treat NER like a word-by-word sequence labeling task, via tags that capture both the boundary and the named entity type. Consider the following sentence:"}, {"id": "2995", "contributed_by": "group 2", "title": "", "section": "", "text": "In this chapter we'll introduce the task of part-of-speech tagging, taking a sequence of words and assigning each word a part of speech like NOUN or VERB, and the task of named entity recognition (NER), assigning words or phrases tags like PERSON, LOCATION, or ORGANIZATION."}, {"id": "2739", "contributed_by": "group 10", "title": "", "section": "", "text": "Part-of-speech taggers are evaluated by the standard metric of accuracy. Named entity recognizers are evaluated by recall, precision, and F1 measure. Recall that recall is the ratio of the number of correctly labeled responses to the total that should have been labeled; precision is the ratio of the number of correctly labeled responses to the total labeled; and F-measure is the harmonic mean of the two."}, {"id": "3110", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech taggers are evaluated by the standard metric of accuracy. Named entity recognizers are evaluated by recall, precision, and F1 measure. Recall that recall is the ratio of the number of correctly labeled responses to the total that should have been labeled; precision is the ratio of the number of correctly labeled responses to the total labeled; and F-measure is the harmonic mean of the two."}, {"id": "3123", "contributed_by": "group 2", "title": "", "section": "", "text": "This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:"}, {"id": "3135", "contributed_by": "group 2", "title": "", "section": "", "text": "Log-linear models for POS tagging were introduced by Ratnaparkhi (1996), who introduced a system called MXPOST which implemented a maximum entropy Markov model (MEMM), a slightly simpler version of a CRF. Around the same time, sequence labelers were applied to the task of named entity tagging, first with HMMs (Bikel et al., 1997) and MEMMs (McCallum et al., 2000), and then once CRFs were developed (Lafferty et al. 2001), they were also applied to NER (Mc- Callum and Li, 2003). A wide exploration of features followed (Zhou et al., 2005). Neural approaches to NER mainly follow from the pioneering results of Collobert et al. (2011), who applied a CRF on top of a convolutional net. BiLSTMs with word and character-based embeddings as input followed shortly and became a standard neural algorithm for NER (Huang et al. 2015, Ma and Hovy 2016, Lample et al. 2016) followed by the more recent use of Transformers and BERT."}], "metadata": {}, "id": "24"}
{"query": "question: Why is tagging a disambiguation task answer: <extra_id_0>", "answers": ["Words are ambiguous, as they can be assigned to several possible part-of-speech tags. The goal of tagging is to resolve these ambiguities, choosing the proper tag for the context"], "generation": "Tagging is a disambiguation task; words are ambiguous -have more than one possible part-of-speech-and the goal is to find the correct tag for the situation. tagging is tagging is Tagging is tagging", "passages": [{"id": "3017", "contributed_by": "group 2", "title": "", "section": "", "text": "Tagging is a disambiguation task; words are ambiguous -have more than one possible part-of-speech-and the goal is to find the correct tag for the situation. For example, book can be a verb (book that flight) or a noun (hand me that book). That can be a determiner (Does that flight serve dinner) or a complementizer (I thought that your flight was earlier). The goal of POS-tagging is to resolve these ambiguities, choosing the proper tag for the context."}, {"id": "3128", "contributed_by": "group 2", "title": "", "section": "", "text": "The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence"}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "520", "contributed_by": "group 11", "title": "Chapter 13 - Constituency Parsing", "section": "", "text": "Chapter 8 introduced the task of part-of-speech tagging and supertagging for highly lexicalized grammar frameworks, where the assigned tags often dictate much of the derivation for a sentence."}, {"id": "3125", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words."}, {"id": "3113", "contributed_by": "group 2", "title": "", "section": "", "text": "The fact that named entity tagging has a segmentation component which is not present in tasks like text categorization or part-of-speech tagging causes some problems with evaluation. For example, a system that labeled Jane but not Jane Villanueva as a person would cause two errors, a false positive for O and a false negative for I-PER. In addition, using entities as the unit of response but words as the unit of training means that there is a mismatch between the training and test conditions."}, {"id": "3562", "contributed_by": "group 2", "title": "", "section": "", "text": "A complication with this approach arises from the use of subword tokenization such as WordPiece or Byte Pair Encoding. Supervised training data for tasks like named entity recognition (NER) is typically in the form of BIO tags associated with text segmented at the word level. For example the following sentence containing two named entities:"}, {"id": "2752", "contributed_by": "group 10", "title": "", "section": "", "text": "hidden Markov model taggers make two further simplifying assumptions. The first is that the probability of a word appearing depends only on its own tag and is independent of neighboring words and tags.The second assumption, the bigram assumption, is that the probability of a tag is dependent only on the previous tag, rather than the entire tag sequence."}, {"id": "2751", "contributed_by": "group 10", "title": "", "section": "", "text": "HMM taggers make two further simplifying assumptions. The first is that the probability of a word appearing depends only on its own tag and is independent of neighboring words and tags.The second assumption, the bigram assumption, is that the probability of a tag is dependent only on the previous tag, rather than the entire tag sequence."}, {"id": "3062", "contributed_by": "group 2", "title": "", "section": "", "text": "The second assumption, the bigram assumption, is that the probability of a tag is"}], "metadata": {}, "id": "25"}
{"query": "question: What are some generic named entity types answer: <extra_id_0>", "answers": ["People, location, organization, geo-political entity, money, time, and so on"], "generation": "PER (person), LOC (location), ORG (organization), or GPE (geo-political entity)", "passages": [{"id": "3126", "contributed_by": "group 2", "title": "", "section": "", "text": "Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns."}, {"id": "2741", "contributed_by": "group 10", "title": "", "section": "", "text": "A named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The task of named entity recognition (NER) is to find spans of text that constitute proper names and tag the type of the entity. Four entity tags are most common: PER (person), LOC (location), ORG (organization), or GPE (geo-political entity)."}, {"id": "3026", "contributed_by": "group 2", "title": "", "section": "", "text": "A named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The task of named entity recognition (NER) is to find spans of text that constitute proper names and tag the type of the entity. Four entity tags are most common: PER (person), LOC (location), ORG (organization), or GPE (geo-political entity). However, the term named entity is commonly extended to include things that aren't entities per se, including dates, times, and other kinds of temporal expressions, and even numerical expressions like prices. Here's an example of the output of an NER tagger:"}, {"id": "2743", "contributed_by": "group 10", "title": "", "section": "", "text": "Unlike part-of-speech tagging, where there is no segmentation problem since each word gets one tag, the task of named entity recognition is to find and label spans of text, and is difficult partly because of the ambiguity of segmentation; we need to decide what's an entity and what isn't, and where the boundaries are."}, {"id": "3582", "contributed_by": "group 2", "title": "", "section": "", "text": "The second advantage to span-based approaches is that they naturally accommodate embedded named entities. For example, in this example both United Airlines and United Airlines Holding are legitimate named entities. The BIO approach has no way of encoding this embedded structure. But the span-based approach can naturally label both since the spans are labeled separately."}, {"id": "3113", "contributed_by": "group 2", "title": "", "section": "", "text": "The fact that named entity tagging has a segmentation component which is not present in tasks like text categorization or part-of-speech tagging causes some problems with evaluation. For example, a system that labeled Jane but not Jane Villanueva as a person would cause two errors, a false positive for O and a false negative for I-PER. In addition, using entities as the unit of response but words as the unit of training means that there is a mismatch between the training and test conditions."}, {"id": "2600", "contributed_by": "group 8", "title": "Title 33: Section 33", "section": "Section 33", "text": "Coherence relations are structured into a tree and parsed using shift-reduce and other parsing algorithms. The Penn Discourse Treebank (PDTB) labels only relations between pairs of spans and assigns labels using sequence models. Entity-based coherence models describe how discourse entities are salient and continue to be mentioned from sentence to sentence. Centering Theory models salience for entities and achieves coherence by keeping the same discourse entities salient throughout the discourse. The entity grid model computes which entity realization transitions lead to coherence. Different genres have different types of global coherence, such as persuasive essays and scientific articles. Coherence relations arose from the independent development of scholars like Hobbs, Mann, and Thompson. Other approaches to coherence relations include Segmented Discourse Representation Theory (SORT) and Linguistic Discourse Model. Wolf and Gibson argue that coherence structure includes"}, {"id": "3028", "contributed_by": "group 2", "title": "", "section": "", "text": "The text contains 13 mentions of named entities including 5 organizations, 4 locations, 2 times, 1 person, and 1 mention of money. Figure 8.5 shows typical generic named entity types. Many applications will also need to use specific entity types like proteins, genes, commercial products, or works of art."}, {"id": "3030", "contributed_by": "group 2", "title": "", "section": "", "text": "Unlike part-of-speech tagging, where there is no segmentation problem since each word gets one tag, the task of named entity recognition is to find and label spans of text, and is difficult partly because of the ambiguity of segmentation; we need to decide what's an entity and what isn't, and where the boundaries are. Indeed, most words in a text will not be named entities. Another difficulty is caused by type ambiguity. The mention JFK can refer to a person, the airport in New York, or any number of schools, bridges, and streets around the United States. Some examples of this kind of cross-type confusion are given in Figure 8.6."}, {"id": "3562", "contributed_by": "group 2", "title": "", "section": "", "text": "A complication with this approach arises from the use of subword tokenization such as WordPiece or Byte Pair Encoding. Supervised training data for tasks like named entity recognition (NER) is typically in the form of BIO tags associated with text segmented at the word level. For example the following sentence containing two named entities:"}], "metadata": {}, "id": "26"}
{"query": "question: What is Hidden Markov Model (HMM) answer: <extra_id_0>", "answers": ["An HMM is a probabilistic sequence model. Given a sequence of words, it computes a probability distribution over possible sequences of labels and chooses the best label sequence."], "generation": "A hidden Markov model (HMM) allows us to talk about both observed events (like words that we see in the input) and hidden events (like part-of-speech tags) A hidden A", "passages": [{"id": "2752", "contributed_by": "group 10", "title": "", "section": "", "text": "hidden Markov model taggers make two further simplifying assumptions. The first is that the probability of a word appearing depends only on its own tag and is independent of neighboring words and tags.The second assumption, the bigram assumption, is that the probability of a tag is dependent only on the previous tag, rather than the entire tag sequence."}, {"id": "3044", "contributed_by": "group 2", "title": "", "section": "", "text": "A first-order hidden Markov model instantiates two simplifying assumptions. First, as with a first-order Markov chain, the probability of a particular state depends only on the previous state:"}, {"id": "3056", "contributed_by": "group 2", "title": "", "section": "", "text": "For any model, such as an HMM, that contains hidden variables, the task of determining the hidden variables sequence corresponding to the sequence of observations is called decoding. More formally,"}, {"id": "3043", "contributed_by": "group 2", "title": "", "section": "", "text": "A hidden Markov model (HMM) allows us to talk about both observed events (like words that we see in the input) and hidden events (like part-of-speech tags) that we think of as causal factors in our probabilistic model. An HMM is specified by the following components:"}, {"id": "3034", "contributed_by": "group 2", "title": "", "section": "", "text": "In this section we introduce our first sequence labeling algorithm, the Hidden Markov Model, and show how to apply it to part-of-speech tagging. Recall that a sequence labeler is a model whose job is to assign a label to each unit in a sequence, thus mapping a sequence of observations to a sequence of labels of the same length. The HMM is a classic model that introduces many of the key concepts of sequence modeling that we will see again in more modern models."}, {"id": "2750", "contributed_by": "group 10", "title": "", "section": "", "text": "A hidden Markov model (HMM) allows us to talk about both observed events(like words that we see in the input) and hidden events (like part-of-speech tags) that we think of as causal factors in our probabilistic model.A first-order hidden Markov model instantiates two simplifying assumptions. First, as with a first-order Markov chain, the probability of a particular state depends only on the previous state. Second, the probability of an output observation oi depends only on the state that produced the observation qi and not on any other states or any other observations."}, {"id": "2749", "contributed_by": "group 10", "title": "", "section": "", "text": "A hidden Markov model (HMM) allows us to talk about both observed events(like words that we see in the input) and hidden events (like part-of-speech tags) that we think of as causal factors in our probabilistic model.A first-order hidden Markov model instantiates two simplifying assumptions. First, as with a first-order Markov chain, the probability of a particular state depends only on the previous state. Second, the probability of an output observation oi depends only on the state that produced the observation qi and not on any other states or any other observations."}, {"id": "1998", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "Based on Shannon 's work, Markov models were commonly used in engineering, linguistic, and psychological work on modeling word sequences by the 1950s.."}, {"id": "1987", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "N-GRAM LANGUAGE MODELS  n-grams are Markov models that estimate words from a xed window of previous words.."}, {"id": "1996", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "Markov classied 20,000 letters as V or C and computed the bigram and trigram probability that a given letter would be a vowel given the previous one or two letters.."}], "metadata": {}, "id": "27"}
{"query": "question: What is the Markov assumption answer: <extra_id_0>", "answers": ["The assumption on the probabilities of this sequence that, when predicting the future, the past doesn’t matter, only the present."], "generation": "The Markov assumption is that when predicting the future in the sequence, all that matters is the current state. All the states before the current state The Markov assumption The", "passages": [{"id": "2752", "contributed_by": "group 10", "title": "", "section": "", "text": "hidden Markov model taggers make two further simplifying assumptions. The first is that the probability of a word appearing depends only on its own tag and is independent of neighboring words and tags.The second assumption, the bigram assumption, is that the probability of a tag is dependent only on the previous tag, rather than the entire tag sequence."}, {"id": "3044", "contributed_by": "group 2", "title": "", "section": "", "text": "A first-order hidden Markov model instantiates two simplifying assumptions. First, as with a first-order Markov chain, the probability of a particular state depends only on the previous state:"}, {"id": "3038", "contributed_by": "group 2", "title": "", "section": "", "text": "More formally, consider a sequence of state variables q1,q2,...,qi. A Markov model embodies the Markov assumption on the probabilities of this sequence: that when predicting the future, the past doesn't matter, only the present."}, {"id": "3062", "contributed_by": "group 2", "title": "", "section": "", "text": "The second assumption, the bigram assumption, is that the probability of a tag is"}, {"id": "1996", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "Markov classied 20,000 letters as V or C and computed the bigram and trigram probability that a given letter would be a vowel given the previous one or two letters.."}, {"id": "3045", "contributed_by": "group 2", "title": "", "section": "", "text": "Markov Assumption: P(qi|q1,...,qi-1) = P(qi|qi-1) (8.6) Second, the probability of an output observation oi depends only on the state that"}, {"id": "1650", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "In other words, instead of computing the probability P(the|Walden Pond 's water is so transparent that) we approximate it with the probability When we use a bigram model to predict the conditional probability of the next word, we are thus making the following approximation: P(wn|w1:n1)  P(wn|wn1) The assumption that the probability of a word depends only on the previous word is called a Markov assumption.."}, {"id": "2751", "contributed_by": "group 10", "title": "", "section": "", "text": "HMM taggers make two further simplifying assumptions. The first is that the probability of a word appearing depends only on its own tag and is independent of neighboring words and tags.The second assumption, the bigram assumption, is that the probability of a tag is dependent only on the previous tag, rather than the entire tag sequence."}, {"id": "2747", "contributed_by": "group 10", "title": "", "section": "", "text": "A Markov chain is a model that tells us something about the probabilities of sequences of random variables, states, each of which can take on values from some set. These sets can be words, or tags, or symbols representing anything, for example the weather. A Markov chain makes a very strong assumption that if we want to predict the future in the sequence, all that matters is the current state. All the states before the current state have no impact on the future except via the current state"}, {"id": "2748", "contributed_by": "group 10", "title": "", "section": "", "text": "A Markov chain is a model that tells us something about the probabilities of sequences of random variables, states, each of which can take on values from some set. These sets can be words, or tags, or symbols representing anything, for example the weather. A Markov chain makes a very strong assumption that if we want to predict the future in the sequence, all that matters is the current state. All the states before the current state have no impact on the future except via the current state"}], "metadata": {}, "id": "28"}
{"query": "question: What is the transition probability matrix of a Markov Model answer: <extra_id_0>", "answers": ["A matrix representation such that the element at the ith row and the jth column is the probability of moving from state i to state j"], "generation": "The transition probability matrix of a Markov Model is the matrix of the probability of each state leaving a given state.", "passages": [{"id": "3039", "contributed_by": "group 2", "title": "", "section": "", "text": "Figure 8.8a shows a Markov chain for assigning a probability to a sequence of weather events, for which the vocabulary consists of HOT, COLD, and WARM. The states are represented as nodes in the graph, and the transitions, with their probabilities, as edges. The transitions are probabilities: the values of arcs leaving a given state must sum to 1. Figure 8.8b shows a Markov chain for assigning a probability to a sequence of words w1 ...wt . This Markov chain should be familiar; in fact, it represents a bigram language model, with each edge expressing the probability p(wi|wj)! Given the two models in Fig. 8.8, we can assign a probability to any sequence from our vocabulary."}, {"id": "1996", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "Markov classied 20,000 letters as V or C and computed the bigram and trigram probability that a given letter would be a vowel given the previous one or two letters.."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "2752", "contributed_by": "group 10", "title": "", "section": "", "text": "hidden Markov model taggers make two further simplifying assumptions. The first is that the probability of a word appearing depends only on its own tag and is independent of neighboring words and tags.The second assumption, the bigram assumption, is that the probability of a tag is dependent only on the previous tag, rather than the entire tag sequence."}, {"id": "3038", "contributed_by": "group 2", "title": "", "section": "", "text": "More formally, consider a sequence of state variables q1,q2,...,qi. A Markov model embodies the Markov assumption on the probabilities of this sequence: that when predicting the future, the past doesn't matter, only the present."}, {"id": "3044", "contributed_by": "group 2", "title": "", "section": "", "text": "A first-order hidden Markov model instantiates two simplifying assumptions. First, as with a first-order Markov chain, the probability of a particular state depends only on the previous state:"}, {"id": "1998", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "Based on Shannon 's work, Markov models were commonly used in engineering, linguistic, and psychological work on modeling word sequences by the 1950s.."}, {"id": "2748", "contributed_by": "group 10", "title": "", "section": "", "text": "A Markov chain is a model that tells us something about the probabilities of sequences of random variables, states, each of which can take on values from some set. These sets can be words, or tags, or symbols representing anything, for example the weather. A Markov chain makes a very strong assumption that if we want to predict the future in the sequence, all that matters is the current state. All the states before the current state have no impact on the future except via the current state"}, {"id": "2747", "contributed_by": "group 10", "title": "", "section": "", "text": "A Markov chain is a model that tells us something about the probabilities of sequences of random variables, states, each of which can take on values from some set. These sets can be words, or tags, or symbols representing anything, for example the weather. A Markov chain makes a very strong assumption that if we want to predict the future in the sequence, all that matters is the current state. All the states before the current state have no impact on the future except via the current state"}, {"id": "1987", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "N-GRAM LANGUAGE MODELS  n-grams are Markov models that estimate words from a xed window of previous words.."}], "metadata": {}, "id": "29"}
{"query": "question: What assumptions are made in the HMM answer: <extra_id_0>", "answers": ["First, the probability of a particular state depends only on the previous state. Second, the probability of an output observation depends only on the state that produced the observation. They are also known as the Markov assumption and the Bigram assumption"], "generation": "HMM taggers make two further simplifying assumptions: the probability of a word appearing depends only on its own tag and is independent of neighboring words and tags Several Several Several Several", "passages": [{"id": "3128", "contributed_by": "group 2", "title": "", "section": "", "text": "The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence"}, {"id": "2751", "contributed_by": "group 10", "title": "", "section": "", "text": "HMM taggers make two further simplifying assumptions. The first is that the probability of a word appearing depends only on its own tag and is independent of neighboring words and tags.The second assumption, the bigram assumption, is that the probability of a tag is dependent only on the previous tag, rather than the entire tag sequence."}, {"id": "3060", "contributed_by": "group 2", "title": "", "section": "", "text": "HMM taggers make two further simplifying assumptions. The first is that the probability of a word appearing depends only on its own tag and is independent of neighboring words and tags:"}, {"id": "3058", "contributed_by": "group 2", "title": "", "section": "", "text": "An illustration of the two parts of an HMM representation: the A transition probabilities used to compute the prior probability, and the B observation likelihoods that are associated with each state, one likelihood for each possible observation word."}, {"id": "3059", "contributed_by": "group 2", "title": "", "section": "", "text": "For part-of-speech tagging, the goal of HMM decoding is to choose the tag sequence t1 . . . tn that is most probable given the observation sequence of n words"}, {"id": "3065", "contributed_by": "group 2", "title": "", "section": "", "text": "Viterbi algorithm for finding the optimal sequence of tags. Given an observation sequence and an HMM lambda = (A, B), the algorithm returns the state path through the HMM that assigns maximum likelihood to the observation sequence."}, {"id": "2753", "contributed_by": "group 10", "title": "", "section": "", "text": "The decoding algorithm for HMMs is the Viterbi algorithm shown. As an instance of dynamic programming, Viterbi resembles the dynamic programming minimum edit distance algorithm.The Viterbi algorithm first sets up a probability matrix or lattice, with one column for each observation ot and one row for each state in the state graph."}, {"id": "3035", "contributed_by": "group 2", "title": "", "section": "", "text": "An HMM is a probabilistic sequence model: given a sequence of units (words, letters, morphemes, sentences, whatever), it computes a probability distribution over possible sequences of labels and chooses the best label sequence."}, {"id": "3133", "contributed_by": "group 2", "title": "", "section": "", "text": "DeRose (1988) developed a quasi-HMM algorithm, including the use of dynamic programming, although computing P(t|w)P(w) instead of P(w|t)P(w). The same year, the probabilistic PARTS tagger of Church 1988, 1989 was probably the first implemented HMM tagger, described correctly in Church (1989), although Church (1988) also described the computation incorrectly as P(t|w)P(w) instead of P(w|t)P(w). Church (p.c.) explained that he had simplified for pedagogical purposes because using the probability P(t|w) made the idea seem more understandable as 'storing a lexicon in an almost standard form'."}, {"id": "2754", "contributed_by": "group 10", "title": "", "section": "", "text": "While the HMM is a useful and powerful model, it turns out that HMMs need a number of augmentations to achieve high accuracy. For example, in POS tagging as in other tasks, we often run into unknown words: proper names and acronyms are created very often, and even new common nouns and verbs enter the language at a surprising rate."}], "metadata": {}, "id": "30"}
{"query": "question: What does the Viterbi algorithm do answer: <extra_id_0>", "answers": ["The Viterbi algorithm is a dynamic programming algorithm to solve the HMM part-of-speech problem. Mathematically, given as an input the HMM which is made of the probability transition matrix and the emission probability matrix, and a sequence of observations, it finds the most probable sequence of states"], "generation": "The Viterbi algorithm first sets up a probability matrix or lattice, with one column for each observation ot and one row for each state The The algorithm", "passages": [{"id": "1597", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "The Viterbi algorithm is a probabilistic extension of minimum edit distance."}, {"id": "1598", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "Instead of computing the minimum edit distance between two strings, Viterbi computes the maximum probability alignment of one string with another."}, {"id": "2753", "contributed_by": "group 10", "title": "", "section": "", "text": "The decoding algorithm for HMMs is the Viterbi algorithm shown. As an instance of dynamic programming, Viterbi resembles the dynamic programming minimum edit distance algorithm.The Viterbi algorithm first sets up a probability matrix or lattice, with one column for each observation ot and one row for each state in the state graph."}, {"id": "3065", "contributed_by": "group 2", "title": "", "section": "", "text": "Viterbi algorithm for finding the optimal sequence of tags. Given an observation sequence and an HMM lambda = (A, B), the algorithm returns the state path through the HMM that assigns maximum likelihood to the observation sequence."}, {"id": "3128", "contributed_by": "group 2", "title": "", "section": "", "text": "The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence"}, {"id": "3129", "contributed_by": "group 2", "title": "", "section": "", "text": "Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training,"}, {"id": "3064", "contributed_by": "group 2", "title": "", "section": "", "text": "The decoding algorithm for HMMs is the Viterbi algorithm shown in Fig. 8.10. As an instance of dynamic programming, Viterbi resembles the dynamic programming minimum edit distance algorithm of Chapter 2."}, {"id": "524", "contributed_by": "group 11", "title": "Chapter 13 - Constituency Parsing", "section": "", "text": "The CRF tagging model can also be used, using similar features and training by maximizing log-likelihood of the training corpus and decoding via the Viterbi algorithm."}, {"id": "3066", "contributed_by": "group 2", "title": "", "section": "", "text": "The Viterbi algorithm first sets up a probability matrix or lattice, with one column for each observation ot and one row for each state in the state graph. Each column thus has a cell for each state qi in the single combined automaton. Figure 8.11 shows an intuition of this lattice for the sentence Janet will back the bill."}, {"id": "3069", "contributed_by": "group 2", "title": "", "section": "", "text": "Viterbi fills each cell recursively. Given that we had already computed the probability of being in every state at time t - 1, we compute the Viterbi probability by taking the most probable of the extensions of the paths that lead to the current cell. For a given state qj at time t, the value vt(j) is computed as"}], "metadata": {}, "id": "31"}
{"query": "question: What is Conditional Random Field (CRF) answer: <extra_id_0>", "answers": ["A CRF is a log-linear model that assigns a probability to an entire output (tag) sequence Y, out of all possible sequences Y, given the entire input (word) sequence X"], "generation": "Conditional Random Field (CRF) is a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on Conditional Random A", "passages": [{"id": "2756", "contributed_by": "group 10", "title": "", "section": "", "text": "However, the conditional random field does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "2755", "contributed_by": "group 10", "title": "", "section": "", "text": "However, the conditional random field does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "3081", "contributed_by": "group 2", "title": "", "section": "", "text": "Luckily, there is a discriminative sequence model based on log-linear models: CRF the conditional random field (CRF). We'll describe here the linear chain CRF, the version of the CRF most commonly used for language processing, and the one whose conditioning closely matches the HMM."}, {"id": "3108", "contributed_by": "group 2", "title": "", "section": "", "text": "The CRF requires only a slight change to this latter formula, replacing the a and b prior and likelihood probabilities with the CRF features:"}, {"id": "3129", "contributed_by": "group 2", "title": "", "section": "", "text": "Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training,"}, {"id": "3084", "contributed_by": "group 2", "title": "", "section": "", "text": "However, the CRF does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "3560", "contributed_by": "group 2", "title": "", "section": "", "text": "Alternatively, the distribution over labels provided by the softmax for each input token can be passed to a conditional random field (CRF) layer which can take global tag-level transitions into account."}, {"id": "524", "contributed_by": "group 11", "title": "Chapter 13 - Constituency Parsing", "section": "", "text": "The CRF tagging model can also be used, using similar features and training by maximizing log-likelihood of the training corpus and decoding via the Viterbi algorithm."}, {"id": "525", "contributed_by": "group 11", "title": "Chapter 13 - Constituency Parsing", "section": "", "text": "The large number of possible supertags and high per-word ambiguity leads the naive CRF algorithm to error rates that are too high for practical use in a parser."}, {"id": "3083", "contributed_by": "group 2", "title": "", "section": "", "text": "In a CRF, by contrast, we compute the posterior p(Y|X) directly, training the CRF to discriminate among the possible tag sequences:"}], "metadata": {}, "id": "32"}
{"query": "question: What is the advantage of CRF over HMM answer: <extra_id_0>", "answers": ["It is hard for generative models like HMMs to add arbitrary features directly into the model in a clean way, but it is easier to incorporate a lot of features into discriminative models like CRF"], "generation": "CRF is a conditional random field that computes log-linear functions over a set of relevant features at each time step. CRF Unlike HMM CRF Unlike HMM CRF", "passages": [{"id": "3108", "contributed_by": "group 2", "title": "", "section": "", "text": "The CRF requires only a slight change to this latter formula, replacing the a and b prior and likelihood probabilities with the CRF features:"}, {"id": "524", "contributed_by": "group 11", "title": "Chapter 13 - Constituency Parsing", "section": "", "text": "The CRF tagging model can also be used, using similar features and training by maximizing log-likelihood of the training corpus and decoding via the Viterbi algorithm."}, {"id": "3099", "contributed_by": "group 2", "title": "", "section": "", "text": "A CRF for NER makes use of very similar features to a POS tagger, as shown in Figure 8.15."}, {"id": "3084", "contributed_by": "group 2", "title": "", "section": "", "text": "However, the CRF does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "528", "contributed_by": "group 11", "title": "Chapter 13 - Constituency Parsing", "section": "", "text": "This can be done with the forward-backward algorithm that is also used to train the CRF."}, {"id": "2755", "contributed_by": "group 10", "title": "", "section": "", "text": "However, the conditional random field does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "2756", "contributed_by": "group 10", "title": "", "section": "", "text": "However, the conditional random field does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "525", "contributed_by": "group 11", "title": "Chapter 13 - Constituency Parsing", "section": "", "text": "The large number of possible supertags and high per-word ambiguity leads the naive CRF algorithm to error rates that are too high for practical use in a parser."}, {"id": "3081", "contributed_by": "group 2", "title": "", "section": "", "text": "Luckily, there is a discriminative sequence model based on log-linear models: CRF the conditional random field (CRF). We'll describe here the linear chain CRF, the version of the CRF most commonly used for language processing, and the one whose conditioning closely matches the HMM."}, {"id": "3109", "contributed_by": "group 2", "title": "", "section": "", "text": "Learning in CRFs relies on the same supervised learning algorithms we presented for logistic regression. Given a sequence of observations, feature functions, and corresponding outputs, we use stochastic gradient descent to train the weights to maximize the log-likelihood of the training corpus. The local nature of linear-chain CRFs means that the forward-backward algorithm introduced for HMMs in Appendix A can be extended to a CRF version that will efficiently compute the necessary derivatives. As with logistic regression, L1 or L2 regularization is important."}], "metadata": {}, "id": "33"}
{"query": "question: What is linear-chain CRF answer: <extra_id_0>", "answers": ["Linear chain CRF is a CRF such that the features only depend on the current output and previous output. This restriction allows the Viterbi algorithm to be used"], "generation": "A linear-chain CRF is a version of the conditional random field (CRF) that only depends on the current and previous output tokens A linear chain CRF", "passages": [{"id": "3086", "contributed_by": "group 2", "title": "", "section": "", "text": "It's common to also describe the same equation by pulling out the denominator into linear chain CRF"}, {"id": "3084", "contributed_by": "group 2", "title": "", "section": "", "text": "However, the CRF does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "3081", "contributed_by": "group 2", "title": "", "section": "", "text": "Luckily, there is a discriminative sequence model based on log-linear models: CRF the conditional random field (CRF). We'll describe here the linear chain CRF, the version of the CRF most commonly used for language processing, and the one whose conditioning closely matches the HMM."}, {"id": "2755", "contributed_by": "group 10", "title": "", "section": "", "text": "However, the conditional random field does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "2756", "contributed_by": "group 10", "title": "", "section": "", "text": "However, the conditional random field does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "3108", "contributed_by": "group 2", "title": "", "section": "", "text": "The CRF requires only a slight change to this latter formula, replacing the a and b prior and likelihood probabilities with the CRF features:"}, {"id": "3090", "contributed_by": "group 2", "title": "", "section": "", "text": "Again, in a linear-chain CRF, each local feature fk at position i can depend on any information from: (yi-1,yi,X,i). So some legal features representing common situations might be the following:"}, {"id": "3088", "contributed_by": "group 2", "title": "", "section": "", "text": "Each of these local features fk in a linear-chain CRF is allowed to make use of the current output token yi, the previous output token yi-1, the entire input string X (or any subpart of it), and the current position i. This constraint to only depend on the current and previous output tokens yi and yi-1 are what characterizes a linear chain CRF. As we will see, this limitation makes it possible to use versions of the efficient Viterbi and Forward-Backwards algorithms from the HMM. A general CRF, by contrast, allows a feature to make use of any output token, and are thus necessary for tasks in which the decision depend on distant output tokens, like yi-4. General CRFs require more complex inference, and are less commonly used for language processing."}, {"id": "524", "contributed_by": "group 11", "title": "Chapter 13 - Constituency Parsing", "section": "", "text": "The CRF tagging model can also be used, using similar features and training by maximizing log-likelihood of the training corpus and decoding via the Viterbi algorithm."}, {"id": "3135", "contributed_by": "group 2", "title": "", "section": "", "text": "Log-linear models for POS tagging were introduced by Ratnaparkhi (1996), who introduced a system called MXPOST which implemented a maximum entropy Markov model (MEMM), a slightly simpler version of a CRF. Around the same time, sequence labelers were applied to the task of named entity tagging, first with HMMs (Bikel et al., 1997) and MEMMs (McCallum et al., 2000), and then once CRFs were developed (Lafferty et al. 2001), they were also applied to NER (Mc- Callum and Li, 2003). A wide exploration of features followed (Zhou et al., 2005). Neural approaches to NER mainly follow from the pioneering results of Collobert et al. (2011), who applied a CRF on top of a convolutional net. BiLSTMs with word and character-based embeddings as input followed shortly and became a standard neural algorithm for NER (Huang et al. 2015, Ma and Hovy 2016, Lample et al. 2016) followed by the more recent use of Transformers and BERT."}], "metadata": {}, "id": "34"}
{"query": "question: What are word shape features? answer: <extra_id_0>", "answers": ["Word shape features represent the abstract letter pattern of the word by mapping lower-case letters to ‘x’, upper-case to ‘X’, numbers to ’d’, and retaining punctuation"], "generation": "Word shape features represent the abstract letter pattern of the word by mapping lower-case letters to 'x', upper-case letters to 'X', numbers to 'd Word shape features", "passages": [{"id": "2835", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "This chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights:  A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.  Many words are polysemous, having many senses.  Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.  WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.  Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.  The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.  Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.  An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.  Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target words neighborhood.  Word sense induction is the task of learning word senses unsupervised."}, {"id": "2757", "contributed_by": "group 10", "title": "", "section": "", "text": "It's also important to have features that help with unknown words. One of the most important is word shape features, which represent the abstract letter pattern of the word by mapping lower-case letters to 'x', upper-case to 'X', numbers to 'd', and retaining punctuation. Thus for example I.M.F would map to X.X.X. and DC10-30 would map to XXdd-dd."}, {"id": "1392", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "Another measure of the number of words in the language is the number of lemmas instead of wordform types."}, {"id": "1377", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "The wordform is the full inected or derived form of the word."}, {"id": "1379", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "For many tasks in English, however, wordforms are sufcient."}, {"id": "3097", "contributed_by": "group 2", "title": "", "section": "", "text": "The known-word templates are computed for every word seen in the training set; the unknown word features can also be computed for all words in training, or only on training words whose frequency is below some threshold. The result of the known-word templates and word-signature features is a very large set of features. Generally a feature cutoff is used in which features are thrown out if they have count < 5 in the training set."}, {"id": "3094", "contributed_by": "group 2", "title": "", "section": "", "text": "It's also important to have features that help with unknown words. One of the most important is word shape features, which represent the abstract letter pattern of the word by mapping lower-case letters to 'x', upper-case to 'X', numbers to 'd', and retaining punctuation. Thus for example I.M.F would map to X.X.X. and DC10-30 would map to XXdd-dd. A second class of shorter word shape features is also used. In these features consecutive character types are removed, so words in all caps map to X, words with initial-caps map to Xx, DC10-30 would be mapped to Xd-d but I.M.F would still map to X.X.X. Prefix and suffix features are also useful. In summary, here are some sample feature templates that help with unknown words:"}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "1604", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "word segmentation and normalization, sentence segmentation, and stemming."}, {"id": "2112", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "A common solution here, rather than using all the words as individual features, is to predene likely sets of words or phrases as features, combined with features that are not purely linguistic.."}], "metadata": {}, "id": "35"}
{"query": "question: What is a gazetteer answer: <extra_id_0>", "answers": ["A gazetteer is a list of place names, often providing millions of entries for locations with detailed geographical and political information. It is an important feature in CRF-based named entity recognition"], "generation": "A gazetteer is a list of place names, often providing millions of entries with detailed geographical and political information. It is useful for locations A list of place names", "passages": [{"id": "1162", "contributed_by": "group 3", "title": "chapter 17", "section": "", "text": "For example the crowdsourced DBpedia is an ontology derived from Wikipedia containing over 2 billion RDF triples. Another dataset from Wikipedia infoboxes, Freebase, now part of Wikidata, has relations between people and their nationality, or locations, and other locations they are contained in."}, {"id": "3140", "contributed_by": "group 2", "title": "", "section": "", "text": "See Householder (1995) for historical notes on parts of speech, and Sampson (1987) and Garside et al. (1997) on the provenance of the Brown and other tagsets."}, {"id": "3101", "contributed_by": "group 2", "title": "", "section": "", "text": "The sample named entity token L'Occitane would generate the following nonzero valued feature values (assuming that L'Occitane is neither in the gazetteer nor the census)."}, {"id": "2820", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "Another way word senses can be related is taxonomically. A word (or sense) is a hyponym of another word or sense if the first is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, hypernym and mango is a hyponym of fruit. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. It is unfortunate that the two words (hypernym and hyponym) are very similar and hence easily confused; for this reason, the word superordinate is often used instead of hypernym. We can define hypernymy more formally by saying that the class denoted by the superordinate extensionally includes the class denoted by the hyponym. Thus, the class of animals includes as members all dogs, and the class of moving actions includes all walking actions. Hypernymy can also be defined in terms of entailment. Under this definition, a sense A is a hyponym of a sense B if everything that is A is also B, and hence being an A entails being a B, or x A(x)  B(x). Hyponymy/hypernymy is usually a transitive relation; if A is a hyponym of B and B is a hyponym of C, then A is a hyponym of C. Another name for the hypernym/hyponym structure is the IS-A hierarchy, in which we say A IS-A B, or B subsumes A."}, {"id": "2640", "contributed_by": "group 8", "title": "Chapter 23", "section": "", "text": "chair dog We can define hypernymy more formally by saying that the class denoted by the superordinate extensionally includes the class denoted by the hyponym. Thus, the class of animals includes as members all dogs, and the class of moving actions in cludes all walking actions. Hypernymy can also be defined in terms of entailment . Under this definition, a sense Ais a hyponym of a sense Bif everything that is Ais alsoB, and hence being an Aentails being a B, or8x A(x))B(x). Hyponymy/hy pernymy is usually a transitive relation; if A is a hyponym of B and B is a hyponym of C, then A is a hyponym of C. Another name for the hypernym/hyponym structure is the IS-A hierarchy, in which we say A IS-A B, or B subsumes A . IS-A Hypernymy is useful for tasks like textual entailment or question answering; knowing that leukemia is a type of cancer , for example, would certainly be useful in answering questions about leukemia. Meronymy Another common relation is meronymy , the part-whole relation. A legis part of a part-whole chair ; awheel is part of a car. We say that wheel is ameronym ofcar, and caris a holonym ofwheel . Structured Polysemy The senses of a word can also be related semantically, in which case we call the relationship between them structured polysemy . Consider this sense bank :structured polysemy (23.11) The bank is on the corner of Nassau and Witherspoon. This sense, perhaps bank4, means something like the building belonging to a financial"}, {"id": "341", "contributed_by": "group 11", "title": "Chapter 12 - Constituency Grammars", "section": "", "text": "It was created by automatically translating phrase-structure trees from the Penn Treebank via a rule-based approach."}, {"id": "2045", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "Naive Bayes is a probabilistic classier, meaning that for a document d, out of all classes c  C the classier returns the class c which has the maximum posterior probability given the document.."}, {"id": "3100", "contributed_by": "group 2", "title": "", "section": "", "text": "One feature that is especially useful for locations is a gazetteer, a list of place names, often providing millions of entries for locations with detailed geographical and political information.3 This can be implemented as a binary feature indicating a phrase appears in the list. Other related resources like name-lists, for example from the United States Census Bureau4, can be used, as can other entity dictionaries like lists of corporations or products, although they may not be as helpful as a gazetteer (Mikheev et al., 1999)."}, {"id": "1163", "contributed_by": "group 3", "title": "chapter 17", "section": "", "text": "WordNet or other ontologies offer useful ontological relations that express hierarchical relations between words or concepts. For example WordNet has the is-a or hypernym relation between classes, Giraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate "}, {"id": "1368", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "Words like uh and um are called llers or lled pauses."}], "metadata": {}, "id": "36"}
{"query": "question: What is the algorithm used to find the most likely tag sequence in CRF answer: <extra_id_0>", "answers": ["The Viterbi algorithm can be used to solve CRF"], "generation": "The Viterbi algorithm is used for inference, to choose the most likely tag sequence in CRF. The CRF uses the forward-backward algorithm The :", "passages": [{"id": "3128", "contributed_by": "group 2", "title": "", "section": "", "text": "The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence"}, {"id": "3083", "contributed_by": "group 2", "title": "", "section": "", "text": "In a CRF, by contrast, we compute the posterior p(Y|X) directly, training the CRF to discriminate among the possible tag sequences:"}, {"id": "3084", "contributed_by": "group 2", "title": "", "section": "", "text": "However, the CRF does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "2755", "contributed_by": "group 10", "title": "", "section": "", "text": "However, the conditional random field does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "2756", "contributed_by": "group 10", "title": "", "section": "", "text": "However, the conditional random field does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "524", "contributed_by": "group 11", "title": "Chapter 13 - Constituency Parsing", "section": "", "text": "The CRF tagging model can also be used, using similar features and training by maximizing log-likelihood of the training corpus and decoding via the Viterbi algorithm."}, {"id": "1097", "contributed_by": "group 3", "title": "chapter 13", "section": "", "text": "As with traditional part-of-speech tagging, the standard approach to building a CCG supertagger is to use supervised machine learning to build a sequence labeler from hand-annotated training data. To find the most likely sequence of tags given a sentence, it is most common to use a neural sequence model, either RNN or Transformer Its also possible, however, to use the CRF tagging model described in Chapter 8, using similar features; the current word wi , its surrounding words within l words, local POS tags and character suffixes, and the supertag from the prior timestep, training by maximizing log-likelihood of the training corpus and decoding via the Viterbi algorithm as described in Chapter 8. Unfortunately the large number of possible supertags combined with high perword ambiguity leads the naive CRF algorithm to error rates that are too high for practical use in a parser. The single best tag sequence T will typically contain too many incorrect tags for effective parsing to take place. To overcome this, we instead return a probability distribution over the possible supertags for each word in the input. The following table illustrates an example distribution for a simple sentence, in which each column represents the probability of each supertag for a given word in the context of the input sentence. The ... represent all the remaining supertags possible for each word"}, {"id": "2740", "contributed_by": "group 10", "title": "", "section": "", "text": "The accuracy of part-of-speech tagging algorithms (the percentage of test set tags that match human gold labels) is extremely high. One study found accuracies over 97% across 15 languages from the Universal Dependency (UD) treebank (Wu and Dredze, 2019). Accuracies on various English treebanks are also 97% (no matter the algorithm; HMMs, CRFs, BERT perform similarly)"}, {"id": "3129", "contributed_by": "group 2", "title": "", "section": "", "text": "Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training,"}, {"id": "3018", "contributed_by": "group 2", "title": "", "section": "", "text": "The accuracy of part-of-speech tagging algorithms (the percentage of test set tags that match human gold labels) is extremely high. One study found accuracies over 97% across 15 languages from the Universal Dependency (UD) treebank (Wu and Dredze, 2019). Accuracies on various English treebanks are also 97% (no matter the algorithm; HMMs, CRFs, BERT perform similarly). This 97% number is also about the human performance on this task, at least for English (Manning, 2011)."}], "metadata": {}, "id": "37"}
{"query": "question: Can features in the CRF depend on earlier output tags (such as the beginning tag)? answer: <extra_id_0>", "answers": ["If the features depend on other features than the current and prior tags, then the CRF is not a linear chain CRF, and hence the Viterbi algorithm cannot be used"], "generation": "Feature dependencies are possible, but they are not explicitly specified in the CRF.", "passages": [{"id": "3084", "contributed_by": "group 2", "title": "", "section": "", "text": "However, the CRF does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "3108", "contributed_by": "group 2", "title": "", "section": "", "text": "The CRF requires only a slight change to this latter formula, replacing the a and b prior and likelihood probabilities with the CRF features:"}, {"id": "2755", "contributed_by": "group 10", "title": "", "section": "", "text": "However, the conditional random field does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "2756", "contributed_by": "group 10", "title": "", "section": "", "text": "However, the conditional random field does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "524", "contributed_by": "group 11", "title": "Chapter 13 - Constituency Parsing", "section": "", "text": "The CRF tagging model can also be used, using similar features and training by maximizing log-likelihood of the training corpus and decoding via the Viterbi algorithm."}, {"id": "3129", "contributed_by": "group 2", "title": "", "section": "", "text": "Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training,"}, {"id": "3099", "contributed_by": "group 2", "title": "", "section": "", "text": "A CRF for NER makes use of very similar features to a POS tagger, as shown in Figure 8.15."}, {"id": "3083", "contributed_by": "group 2", "title": "", "section": "", "text": "In a CRF, by contrast, we compute the posterior p(Y|X) directly, training the CRF to discriminate among the possible tag sequences:"}, {"id": "2740", "contributed_by": "group 10", "title": "", "section": "", "text": "The accuracy of part-of-speech tagging algorithms (the percentage of test set tags that match human gold labels) is extremely high. One study found accuracies over 97% across 15 languages from the Universal Dependency (UD) treebank (Wu and Dredze, 2019). Accuracies on various English treebanks are also 97% (no matter the algorithm; HMMs, CRFs, BERT perform similarly)"}, {"id": "3135", "contributed_by": "group 2", "title": "", "section": "", "text": "Log-linear models for POS tagging were introduced by Ratnaparkhi (1996), who introduced a system called MXPOST which implemented a maximum entropy Markov model (MEMM), a slightly simpler version of a CRF. Around the same time, sequence labelers were applied to the task of named entity tagging, first with HMMs (Bikel et al., 1997) and MEMMs (McCallum et al., 2000), and then once CRFs were developed (Lafferty et al. 2001), they were also applied to NER (Mc- Callum and Li, 2003). A wide exploration of features followed (Zhou et al., 2005). Neural approaches to NER mainly follow from the pioneering results of Collobert et al. (2011), who applied a CRF on top of a convolutional net. BiLSTMs with word and character-based embeddings as input followed shortly and became a standard neural algorithm for NER (Huang et al. 2015, Ma and Hovy 2016, Lample et al. 2016) followed by the more recent use of Transformers and BERT."}], "metadata": {}, "id": "38"}
{"query": "question: What are feature templates in CRF answer: <extra_id_0>", "answers": ["The system designer would specify templates to be filled in, and they are automatically populated as the words are processed."], "generation": "feature templates are abstract specications of features. They are computed for every word seen in the training set; the unknown word templates are computed feature templates feature templates", "passages": [{"id": "3108", "contributed_by": "group 2", "title": "", "section": "", "text": "The CRF requires only a slight change to this latter formula, replacing the a and b prior and likelihood probabilities with the CRF features:"}, {"id": "3084", "contributed_by": "group 2", "title": "", "section": "", "text": "However, the CRF does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "2755", "contributed_by": "group 10", "title": "", "section": "", "text": "However, the conditional random field does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "2756", "contributed_by": "group 10", "title": "", "section": "", "text": "However, the conditional random field does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence."}, {"id": "3099", "contributed_by": "group 2", "title": "", "section": "", "text": "A CRF for NER makes use of very similar features to a POS tagger, as shown in Figure 8.15."}, {"id": "524", "contributed_by": "group 11", "title": "Chapter 13 - Constituency Parsing", "section": "", "text": "The CRF tagging model can also be used, using similar features and training by maximizing log-likelihood of the training corpus and decoding via the Viterbi algorithm."}, {"id": "2330", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "Often these are created automatically via feature templates, abstract specications of features."}, {"id": "3090", "contributed_by": "group 2", "title": "", "section": "", "text": "Again, in a linear-chain CRF, each local feature fk at position i can depend on any information from: (yi-1,yi,X,i). So some legal features representing common situations might be the following:"}, {"id": "3097", "contributed_by": "group 2", "title": "", "section": "", "text": "The known-word templates are computed for every word seen in the training set; the unknown word features can also be computed for all words in training, or only on training words whose frequency is below some threshold. The result of the known-word templates and word-signature features is a very large set of features. Generally a feature cutoff is used in which features are thrown out if they have count < 5 in the training set."}, {"id": "3091", "contributed_by": "group 2", "title": "", "section": "", "text": "For simplicity, we'll assume all CRF features take on the value 1 or 0. Above, we explicitly use the notation 1{x} to mean '1 if x is true, and 0 otherwise'. From now on, we'll leave off the 1 when we define features, but you can assume each feature has it there implicitly."}], "metadata": {}, "id": "39"}
{"query": "question: What is BIO tagging answer: <extra_id_0>", "answers": ["BIO tagging is a standard approach to sequence labeling for a span-recognition problem. Tokens that begin a span of interest are tagged with a B, tokens that occur inside a span are tagged with an I, and any tokens outside of any span of interest are labeled O"], "generation": "BIO tagging is the task of assigning semantic role labels to the constituents of a sentence segmented at the word level. BIO tagging BIO", "passages": [{"id": "3128", "contributed_by": "group 2", "title": "", "section": "", "text": "The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence"}, {"id": "2746", "contributed_by": "group 10", "title": "", "section": "", "text": "We've also shown two variant tagging schemes: IO tagging, which loses some information by eliminating the B tag, and BIOES tagging, which adds an end tag E for the end of a span, and a span tag S for a span consisting of only one word."}, {"id": "3125", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words."}, {"id": "3033", "contributed_by": "group 2", "title": "", "section": "", "text": "We've also shown two variant tagging schemes: IO tagging, which loses some information by eliminating the B tag, and BIOES tagging, which adds an end tag E for the end of a span, and a span tag S for a span consisting of only one word. A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label each token in a text with tags that indicate the presence (or absence) of particular kinds of named entities."}, {"id": "3032", "contributed_by": "group 2", "title": "", "section": "", "text": "Figure 8.7 shows the same excerpt represented with BIO tagging, as well as variants called IO tagging and BIOES tagging. In BIO tagging we label any token that begins a span of interest with the label B, tokens that occur inside a span are tagged with an I, and any tokens outside of any span of interest are labeled O. While there is only one O tag, we'll have distinct B and I tags for each named entity class. The number of tags is thus 2n + 1 tags, where n is the number of entity types. BIO tagging can represent exactly the same information as the bracketed notation, but has the advantage that we can represent the task in the same simple sequence modeling way as part-of-speech tagging: assigning a single label yi to each input word xi:"}, {"id": "3566", "contributed_by": "group 2", "title": "", "section": "", "text": "'Mt' would be assigned to 'Mt.' and the tag assigned to 'San' would be assigned to 'Sanitas', effectively ignoring the information in the tags assigned to '.' and '##itas'. More complex approaches combine the distribution of tag probabilities across the subwords in an attempt to find an optimal word-level tag."}, {"id": "3562", "contributed_by": "group 2", "title": "", "section": "", "text": "A complication with this approach arises from the use of subword tokenization such as WordPiece or Byte Pair Encoding. Supervised training data for tasks like named entity recognition (NER) is typically in the form of BIO tags associated with text segmented at the word level. For example the following sentence containing two named entities:"}, {"id": "3062", "contributed_by": "group 2", "title": "", "section": "", "text": "The second assumption, the bigram assumption, is that the probability of a tag is"}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "3059", "contributed_by": "group 2", "title": "", "section": "", "text": "For part-of-speech tagging, the goal of HMM decoding is to choose the tag sequence t1 . . . tn that is most probable given the observation sequence of n words"}], "metadata": {}, "id": "40"}
{"query": "question: What is a recurrent neural network (RNN) answer: <extra_id_0>", "answers": ["A network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input"], "generation": "A recurrent neural network (RNN) is a type of neural network architecture that uses information from the current context to make predictions at a given time A A A type", "passages": [{"id": "2773", "contributed_by": "group 10", "title": "", "section": "", "text": "The Recurrent Neural Network (RNN) is a type of neural network architecture that uses information from the current context to make predictions at a given time t. The network processes input data sequentially, with each step taking into account the current input and the state of the network at the previous time step. The RNN maintains an internal state or memory that it updates at each time step, allowing it to capture temporal dependencies in the input data. The output of the RNN at each time step depends on the current input and the current state of the network, and can be used for a variety of tasks such as sequence modeling, time series forecasting, and language modeling."}, {"id": "2774", "contributed_by": "group 10", "title": "", "section": "", "text": "The idea behind stacked RNNs is to combine multiple recurrent layers to capture more complex temporal dependencies in sequential data. In a stacked RNN, each layer receives the output of the previous layer as its input, allowing the network to process the sequence hierarchically. This means that the output of one layer serves as the input to a subsequent layer, allowing the network to build increasingly abstract representations of the sequence. Stacked RNNs have been shown to be effective in a variety of sequence modeling tasks, such as speech recognition and natural language processing."}, {"id": "3196", "contributed_by": "group 2", "title": "", "section": "", "text": "Recurrent networks are quite flexible. By combining the feedforward nature of unrolled computational graphs with vectors as common inputs and outputs, complex networks can be treated as modules that can be combined in creative ways. This section introduces two of the more common network architectures used in language processing with RNNs."}, {"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "3226", "contributed_by": "group 2", "title": "", "section": "", "text": "Basic neural units used in feedforward, simple recurrent networks (SRN), and long short-term memory (LSTM)."}, {"id": "2778", "contributed_by": "group 10", "title": "", "section": "", "text": "The basic idea behind sequence-to-sequence models is to use two recurrent neural networks (RNNs) together, called an encoder and a decoder, to map an input sequence to an output sequence. The encoder network takes an input sequence and creates a contextualized representation of it, often called the context. This representation is then passed to a decoder which generates a task-specific output sequence. The encoder and decoder are trained jointly to optimize a suitable objective function, such as maximizing the likelihood of the correct output given the input, or minimizing the distance between the correct output and the predicted output."}, {"id": "3291", "contributed_by": "group 2", "title": "", "section": "", "text": "Influential investigations of RNNs were conducted in the context of the Parallel Distributed Processing (PDP) group at UC San Diego in the 1980's. Much of this work was directed at human cognitive modeling rather than practical NLP applications Rumelhart and McClelland 1986c McClelland and Rumelhart 1986. Models using recurrence at the hidden layer in a feedforward network (Elman networks) were introduced by Elman (1990). Similar architectures were investigated by Jordan (1986) with a recurrence from the output layer, and Mathis and Mozer (1995) with the addition of a recurrent context layer prior to the hidden layer. The possibility of unrolling a recurrent network into an equivalent feedforward network is discussed in (Rumelhart and McClelland, 1986c)."}, {"id": "3198", "contributed_by": "group 2", "title": "", "section": "", "text": "Stacked recurrent networks. The output of a lower level serves as the input to higher levels with the output of the last network serving as the final output."}, {"id": "3152", "contributed_by": "group 2", "title": "", "section": "", "text": "Simple recurrent neural network after Elman (1990). The hidden layer includes a recurrent connection as part of its input. That is, the activation value of the hidden layer depends on the current input as well as the activation value of the hidden layer from the previous time step."}, {"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}], "metadata": {}, "id": "41"}
{"query": "question: What is the main difference between a feed-forward network and a recurrent network answer: <extra_id_0>", "answers": ["In a feed-forward network, all computation goes forward, whereas in a recurrent network, there is a link that augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time."], "generation": "A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer", "passages": [{"id": "3198", "contributed_by": "group 2", "title": "", "section": "", "text": "Stacked recurrent networks. The output of a lower level serves as the input to higher levels with the output of the last network serving as the final output."}, {"id": "3226", "contributed_by": "group 2", "title": "", "section": "", "text": "Basic neural units used in feedforward, simple recurrent networks (SRN), and long short-term memory (LSTM)."}, {"id": "3196", "contributed_by": "group 2", "title": "", "section": "", "text": "Recurrent networks are quite flexible. By combining the feedforward nature of unrolled computational graphs with vectors as common inputs and outputs, complex networks can be treated as modules that can be combined in creative ways. This section introduces two of the more common network architectures used in language processing with RNNs."}, {"id": "3291", "contributed_by": "group 2", "title": "", "section": "", "text": "Influential investigations of RNNs were conducted in the context of the Parallel Distributed Processing (PDP) group at UC San Diego in the 1980's. Much of this work was directed at human cognitive modeling rather than practical NLP applications Rumelhart and McClelland 1986c McClelland and Rumelhart 1986. Models using recurrence at the hidden layer in a feedforward network (Elman networks) were introduced by Elman (1990). Similar architectures were investigated by Jordan (1986) with a recurrence from the output layer, and Mathis and Mozer (1995) with the addition of a recurrent context layer prior to the hidden layer. The possibility of unrolling a recurrent network into an equivalent feedforward network is discussed in (Rumelhart and McClelland, 1986c)."}, {"id": "2936", "contributed_by": "group 2", "title": "", "section": "", "text": "Let s now walk through a slightly more formal presentation of the simplest kind of feedforward neural network, the feedforward network. A feedforward network is a multilayer network network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. (In Chapter 9 we will introduce networks with cycles, called recurrent neural networks.) For historical reasons multilayer networks, especially feedforward networks, are multilayer sometimes called multi-layer perceptrons (orMLPs); this is a technical misnomer, perceptrons MLP since the units in modern multilayer networks aren t perceptrons (perceptrons are purely linear, but modern networks are made up of units with non-linearities like sigmoids), but at some point the name stuck.  Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units. Fig. 7.8 shows a picture.  The input layer x is a vector of simple scalar values just as we saw in Fig. 7.2.  hidden layer"}, {"id": "3230", "contributed_by": "group 2", "title": "", "section": "", "text": "Transformers map sequences of input vectors (x1,...,xn) to sequences of output vectors (y1,...,yn) of the same length. Transformers are made up of stacks of transformer blocks, which are multilayer networks made by combining simple linear layers, feedforward networks, and self-attention layers, the key innovation of transformers. Self-attention allows a network to directly extract and use information from arbitrarily large contexts without the need to pass it through intermediate recurrent connections as in RNNs. We'll start by describing how self-attention works and then return to how it fits into larger transformer blocks."}, {"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}, {"id": "3228", "contributed_by": "group 2", "title": "", "section": "", "text": "This modularity is key to the power and widespread applicability of LSTM units. LSTM units (or other varieties, like GRUs) can be substituted into any of the network architectures described in Section 9.5. And, as with simple RNNs, multi-layered networks making use of gated units can be unrolled into deep feedforward networks and trained in the usual fashion with backpropagation."}, {"id": "3289", "contributed_by": "group 2", "title": "", "section": "", "text": "A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks."}, {"id": "2722", "contributed_by": "group 10", "title": "", "section": "", "text": "Let's now walk through a slightly more formal presentation of the simplest kind of neural network, the feedforward network. A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers."}], "metadata": {}, "id": "42"}
{"query": "question: Why is RNN computation done sequentially answer: <extra_id_0>", "answers": ["The fact that the computation at time t requires the value of the hidden layer from time t − 1 mandates an incremental inference algorithm that proceeds from the start of the sequence to the end"], "generation": "RNN computation is done sequentially because the computations at each time step are independent of all the other steps. This is Sequence computation is Sequence computation", "passages": [{"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "2774", "contributed_by": "group 10", "title": "", "section": "", "text": "The idea behind stacked RNNs is to combine multiple recurrent layers to capture more complex temporal dependencies in sequential data. In a stacked RNN, each layer receives the output of the previous layer as its input, allowing the network to process the sequence hierarchically. This means that the output of one layer serves as the input to a subsequent layer, allowing the network to build increasingly abstract representations of the sequence. Stacked RNNs have been shown to be effective in a variety of sequence modeling tasks, such as speech recognition and natural language processing."}, {"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}, {"id": "3233", "contributed_by": "group 2", "title": "", "section": "", "text": "Information flow in a causal (or masked) self-attention model. In processing each element of the sequence, the model attends to all the inputs up to, and including, the current one. Unlike RNNs, the computations at each time step are independent of all the other steps and therefore can be performed in parallel."}, {"id": "3196", "contributed_by": "group 2", "title": "", "section": "", "text": "Recurrent networks are quite flexible. By combining the feedforward nature of unrolled computational graphs with vectors as common inputs and outputs, complex networks can be treated as modules that can be combined in creative ways. This section introduces two of the more common network architectures used in language processing with RNNs."}, {"id": "3210", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN for sequence classification. The final hidden units from the forward and backward passes are combined to represent the entire sequence. This combined representation serves as input to the subsequent classifier."}, {"id": "2775", "contributed_by": "group 10", "title": "", "section": "", "text": "a key limitation of RNNs in that they have difficulty capturing long-term dependencies in sequential data. While they can provide useful information at each step of the sequence, they may lose important information from earlier steps as the sequence progresses. This limitation has led to the development of more advanced sequence models, such as LSTMs and Transformers, which can better capture long-term dependencies and process sequential data more effectively."}, {"id": "3290", "contributed_by": "group 2", "title": "", "section": "", "text": "Common language-based applications for RNNs and transformers include: Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. Auto-regressive generation using a trained language model. Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification."}, {"id": "2765", "contributed_by": "group 10", "title": "", "section": "", "text": "Backpropagation Through Time (BPTT) is a general approach for training the weights in RNNs. It involves processing the input sequence in the forward direction to generate the output sequence, and then using the output sequence to compute the loss function. The loss function is then backpropagated through the network in the reverse direction using the chain rule of calculus, allowing the gradients to be computed for all the weights in the network. However, because the RNN is unrolled over time, the gradients can become unstable and either vanish or explode. To address this problem, various modifications have been proposed, such as gradient clipping and using different activation functions. Despite its challenges, BPTT remains a popular method for training RNNs and has been used in many applications, including speech recognition and machine translation."}, {"id": "3209", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN. Separate models are trained in the forward and backward directions, with the output of each model at each time point concatenated to represent the bidirectional state at that time point."}], "metadata": {}, "id": "43"}
{"query": "question: What is backpropagation through time answer: <extra_id_0>", "answers": ["RNN weights are trained in two passes. In the first pass, we perform forward inference, accumulating the loss at each step in time. In the second phase, we process the sequence in reverse, computing the required gradients as we go"], "generation": "Backpropagation through time (BPTT) is a general approach for training the weights in RNNs. It involves processing the input sequence in the forward direction BPTT BPTT BPTT", "passages": [{"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "2764", "contributed_by": "group 10", "title": "", "section": "", "text": "The backpropagation algorithm for training weights in RNNs is a two-pass process. In the first pass, forward inference is performed to generate the output sequence. The output sequence is then compared to the target sequence, and the loss function is computed. In the second pass, the gradients of the loss function with respect to the weights are computed by backpropagating the errors through the network in reverse order. This is known as the backpropagation through time (BPTT) algorithm. However, because the RNN is unrolled over time, the gradients can become unstable and either vanish or explode. To address this problem, various modifications have been proposed, such as gradient clipping and using different activation functions. Despite its challenges, BPTT remains a popular method for training RNNs and has been used in many applications, including speech recognition and machine translation."}, {"id": "2765", "contributed_by": "group 10", "title": "", "section": "", "text": "Backpropagation Through Time (BPTT) is a general approach for training the weights in RNNs. It involves processing the input sequence in the forward direction to generate the output sequence, and then using the output sequence to compute the loss function. The loss function is then backpropagated through the network in the reverse direction using the chain rule of calculus, allowing the gradients to be computed for all the weights in the network. However, because the RNN is unrolled over time, the gradients can become unstable and either vanish or explode. To address this problem, various modifications have been proposed, such as gradient clipping and using different activation functions. Despite its challenges, BPTT remains a popular method for training RNNs and has been used in many applications, including speech recognition and machine translation."}, {"id": "3163", "contributed_by": "group 2", "title": "", "section": "", "text": "Tailoring the backpropagation algorithm to this situation leads to a two-pass algorithm for training the weights in RNNs. In the first pass, we perform forward inference, computing ht , yt , accumulating the loss at each step in time, saving the value of the hidden layer at each step for use at the next time step. In the second phase, we process the sequence in reverse, computing the required gradients as we go, computing and saving the error term for use in the hidden layer for each step backward in time. This general approach is commonly referred to as Backpropagation Through Time (Werbos 1974, Rumelhart et al. 1986, Werbos 1990)."}, {"id": "2991", "contributed_by": "group 2", "title": "", "section": "", "text": "Neural networks are trained by optimization algorithms like gradient descent.Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling."}, {"id": "3213", "contributed_by": "group 2", "title": "", "section": "", "text": "A second difficulty with training RNNs arises from the need to backpropagate the error signal back through time. Recall from Section 9.2.2 that the hidden layer at time t contributes to the loss at the next time step since it takes part in that calculation. As a result, during the backward pass of training, the hidden layers are subject to repeated multiplications, as determined by the length of the sequence. A frequent result of this process is that the gradients are eventually driven to zero, a situation called the vanishing gradients problem."}, {"id": "3465", "contributed_by": "group 2", "title": "", "section": "", "text": "Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts."}, {"id": "2762", "contributed_by": "group 10", "title": "", "section": "", "text": "For applications that involve much longer input sequences, such as speech recognition, character-level processing, or streaming of continuous inputs, unrolling an entire input sequence is not always feasible. This is because the number of time steps required to process the entire sequence may be too large for the available memory or processing power of the system. In these cases, alternative approaches such as truncated backpropagation through time (BPTT) or the use of convolutional neural networks (CNNs) may be necessary."}, {"id": "1891", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "The probability for a backoff n-gram PBO is thus computed as follows: if C(wnN+1:n) > 0 Katz backoff is often combined with a smoothing method called Good-Turing.."}, {"id": "3160", "contributed_by": "group 2", "title": "", "section": "", "text": "Forward inference in a simple recurrent network. The matrices U, V and W are shared across time, while new values for h and y are calculated with each time step."}], "metadata": {}, "id": "44"}
{"query": "question: What is an RNN language model answer: <extra_id_0>", "answers": ["RNN language models process the input sequence one word at a time, attempting to predict the next word from the current word and the previous hidden state"], "generation": "An RNN language model is a language model that processes the input sequence one word at a time, attempting to predict the next word from the current word and the previous hidden state. An", "passages": [{"id": "3290", "contributed_by": "group 2", "title": "", "section": "", "text": "Common language-based applications for RNNs and transformers include: Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. Auto-regressive generation using a trained language model. Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification."}, {"id": "3187", "contributed_by": "group 2", "title": "", "section": "", "text": "a new vector whose kth element is the max of the kth elements of all the n vectors. 9.4.3 Generation with RNN-Based Language Models"}, {"id": "2767", "contributed_by": "group 10", "title": "", "section": "", "text": "In language modeling, the goal is to learn a model that can generate the correct next word in a sequence of words. One way to train such a model is to use the cross-entropy loss as the objective function. The cross-entropy loss is determined by the probability that the RNN language model assigns to the correct next word. Specifically, at each time step, the model generates a probability distribution over the possible next words in the sequence, and the cross-entropy loss is calculated by taking the negative log probability of the correct next word. By minimizing the cross-entropy loss, the model is trained to assign higher probabilities to the correct next words, and thus generate more accurate predictions. This approach has been used in a variety of language modeling tasks, and is a key component of many state-of-the-art language models."}, {"id": "3209", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN. Separate models are trained in the forward and backward directions, with the output of each model at each time point concatenated to represent the bidirectional state at that time point."}, {"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "2763", "contributed_by": "group 10", "title": "", "section": "", "text": "One of the main advantages of RNNs over n-gram models is that RNNs don't have the limited context problem. Because the hidden state of an RNN depends on all the previous inputs in the sequence, it can in principle represent information about all of the preceding words, all the way back to the beginning of the sequence. This allows the model to capture long-range dependencies in the input, which can be crucial for many sequence modeling tasks, such as speech recognition and machine translation. However, in practice, the RNN's ability to remember information from the distant past is limited by the vanishing gradient problem, which can cause the model to lose track of long-term dependencies over time."}, {"id": "3188", "contributed_by": "group 2", "title": "", "section": "", "text": "RNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, and conversational dialogue; any ask where a system needs to produce text, conditioned on some other text."}, {"id": "3166", "contributed_by": "group 2", "title": "", "section": "", "text": "RNN language models (Mikolov et al., 2010) process the input sequence one word at a time, attempting to predict the next word from the current word and the previous hidden state. RNNs don't have the limited context problem that n-gram models have, since the hidden state can in principle represent information about all of the preceding words all the way back to the beginning of the sequence."}, {"id": "3350", "contributed_by": "group 2", "title": "", "section": "", "text": "A more formal version of translating a sentence at inference time in the basic RNN-based encoder-decoder architecture. The final hidden state of the encoder RNN, hen, serves as the context for the decoder in its role as hd0 in the decoder RNN."}, {"id": "2766", "contributed_by": "group 10", "title": "", "section": "", "text": "In an RNN language model, the current word is represented as a vector using a word embedding matrix E. At each time step, the model retrieves the embedding for the current word from E and combines it with the hidden layer from the previous time step to compute a new hidden layer. This process allows the model to capture contextual information from the previous words in the sequence and update its representation of the current word accordingly. The new hidden layer is then used to generate a probability distribution over the possible next words in the sequence using a softmax function. By training the model to predict the correct next word in the sequence, the embeddings and hidden layers are learned to capture meaningful patterns in the data and make accurate predictions."}], "metadata": {}, "id": "45"}
{"query": "question: What is teacher forcing answer: <extra_id_0>", "answers": ["Rather than feeding the model its best case from the previous time step, we always give the model the correct history sequence to predict the next word"], "generation": "Teacher forcing is the practice of training a sequence-to-sequence model by feeding it the ground truth output from the previous time step as input during training, rather", "passages": [{"id": "2782", "contributed_by": "group 10", "title": "", "section": "", "text": "the practice of feeding the model its own previous output as input during training, instead of using the correct next token in the training sequence. In other words, the system is 'forced' to rely on its own predictions, even if they are incorrect. This is contrasted with allowing the model to use the correct target token as input at each time step, which is referred to as 'not using teacher forcing'. The sentence highlights that teacher forcing may result in the model learning to rely on its own errors and may lead to inferior performance on test data."}, {"id": "3364", "contributed_by": "group 2", "title": "", "section": "", "text": "tend to deviate more and more from the gold target sentence as it keeps generating more tokens. In training, therefore, it is more common to use teacher forcing in the decoder. Teacher forcing means that we force the system to use the gold target token from training as the next input, rather than allowing it to rely on the (possibly erroneous) decoder output. This speeds up training."}, {"id": "3014", "contributed_by": "group 2", "title": "", "section": "", "text": "Auxiliary verbs mark semantic features of a main verb such as its tense, whether it is completed (aspect), whether it is negated (polarity), and whether an action is necessary, possible, suggested, or desired (mood). English auxiliaries include the copula verb be, the two verbs do and have, forms, as well as modal verbs used to mark the mood associated with the event depicted by the main verb: can indicates"}, {"id": "2792", "contributed_by": "group 10", "title": "", "section": "", "text": "During the training phase of a neural machine translation model, the decoder is fed with the ground truth target token as input at each timestep, a technique known as teacher forcing. However, during inference when the model is generating translations for unseen data, the decoder has to rely on its own predictions as input for the next step. This tendency of the decoder to deviate from the actual target sentence with increasing generation steps is known as the exposure bias problem. As a result, the generated translations may drift away from the desired output, leading to decreased quality. This issue can be addressed through various methods, such as using scheduled sampling or reinforcement learning. Nevertheless, teacher forcing remains the most commonly used technique during training to achieve high quality translations"}, {"id": "3172", "contributed_by": "group 2", "title": "", "section": "", "text": "Thus at each word position t of the input, the model takes as input the correct sequence of tokens w1:t, and uses them to compute a probability distribution over possible next words so as to compute the model's loss for the next token wt+1. Then we move to the next word, we ignore what the model predicted for the next word and instead use the correct sequence of tokens w1:t+1 to estimate the probability of token wt+2. This idea that we always give the model the correct history sequence to predict the next word (rather than feeding the model its best case from the previous time step) is called teacher forcing."}, {"id": "2771", "contributed_by": "group 10", "title": "", "section": "", "text": "The practice of training a sequence-to-sequence model by feeding it the ground truth output from the previous time step, rather than the model's own prediction, is called teacher forcing. This approach is used to stabilize the training process and improve the accuracy of the model's predictions, particularly in the early stages of training when the model's predictions may be unreliable. By providing the model with the correct output at each time step during training, we ensure that it receives the best possible input for making predictions at the next time step. However, during inference, when the ground truth output is not available, the model must rely on its own predictions, which may differ from the ground truth and lead to error accumulation over time. Various techniques, such as scheduled sampling and curriculum learning, have been proposed to address this issue and improve the performance of sequence-to-sequence models during inference."}, {"id": "2793", "contributed_by": "group 10", "title": "", "section": "", "text": "In a typical encoder-decoder model, the encoder processes the source sentence and outputs a set of hidden states, which are used by the decoder to generate the target sentence. Both the encoder and decoder are often implemented as recurrent neural networks (RNNs), with the encoder processing the source sentence in a left-to-right or bidirectional manner, and the decoder generating the target sentence one word at a time, conditioned on the source sentence and the previously generated words. Encoder-decoder architectures are trained in a supervised manner with paired source and target strings, using maximum likelihood estimation (MLE) to minimize the cross-entropy loss between the predicted and true target sequences. During training, the decoder is typically forced to generate the true target sequence at each time step, a technique known as teacher forcing."}, {"id": "2577", "contributed_by": "group 8", "title": "Title 9: Section 9", "section": "Section 9", "text": "The PDTB annotates discourse structure up to the span-pair level and does not commit to higher-level discourse structure. Similar methods are used in other language treebanks, such as the Chinese Discourse TreeBank. Chinese has a smaller percentage of explicit discourse connectives than English, marking only 22% of all discourse relations with explicit connectives. Examples of implicit connectives include temporal contingency, comparison, expansion, synchronous reason, contrast, and conjunction. The PDTB sense hierarchy has four high-level semantic distinctions and 16 types, with 23 subtypes. Not all types are commonly used for implicit argument classification; the five types in the figure are. In some works, actors stand outside their characters and express their disagreement with them, even standing on their heads. Mr. Breeden has a better chance of achieving his agenda than Mr. Ruder because of his experience as a former White House aide who worked closely with Congress. The U.S. seeks to remove perceived investment barriers, while Japan denies their existence."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "3405", "contributed_by": "group 2", "title": "", "section": "", "text": "In training, just as for RNN encoder-decoders, we use teacher forcing, and train autoregressively, at each time step predicting the next token in the target language, using cross-entropy loss."}], "metadata": {}, "id": "46"}
{"query": "question: What is weight tying answer: <extra_id_0>", "answers": ["Instead of having one embedding after the input layer, and one embedding at output prior to softmax, we simply use a single set of embeddings at both layers"], "generation": "Weight tying is a technique used in neural network architectures to reduce the number of parameters in the model and improve its efficiency. Weight tying Weight", "passages": [{"id": "2770", "contributed_by": "group 10", "title": "", "section": "", "text": "Weight tying is a technique used in neural network architectures to reduce the number of parameters in the model and improve its efficiency. In some architectures, such as language models, there are redundant sets of weights that perform similar functions, such as the input and output word embeddings. Weight tying dispenses with this redundancy and simply uses a single set of embeddings at both the input and output layers. This technique has been shown to be effective in reducing overfitting and improving generalization, particularly in tasks such as language modeling and machine translation. By sharing the same set of weights across different parts of the model, weight tying encourages the network to learn more meaningful representations of the input data and make more accurate predictions."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "2355", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "By contrast, logistic regression is much more robust to correlated features; if two features f1 and f2 are perfectly correlated, regression will simply assign part of the weight to w1 and part to w2."}, {"id": "2158", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "Thus it weighs the lower of the two numbers more heavily.."}, {"id": "3010", "contributed_by": "group 2", "title": "", "section": "", "text": "Conjunctions join two phrases, clauses, or sentences. Coordinating conjunctions like and, or, and but join two elements of equal status. Subordinating conjunctions are used when one of the elements has some embedded status. For example, the subordinating conjunction that in 'I thought that you might like some milk' links the main clause I thought with the subordinate clause you might like some milk. This clause is called subordinate because this entire clause is the 'content' of the main verb thought. Subordinating conjunctions like that which link a verb to its argument in this way are also called complementizers."}, {"id": "34", "contributed_by": "group 9", "title": "", "section": "", "text": "Logistic regression solves this task by learning, from a training set, a vector of weights and a bias term. Each weight wi is a real number, and is associated with one of the input features xi. The weight wi represents how important that input feature is to the classification decision, and can be positive (providing evidence that the in- stance being classified belongs in the positive class) or negative (providing evidence that the instance being classified belongs in the negative class). Thus we might expect in a sentiment task the word awesome to have a high positive weight, and abysmal to have a very negative weight. The bias term, also called the intercept, is another real number thats added to the weighted inputs."}, {"id": "3125", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words."}, {"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "2287", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "Thus we might expect in a sentiment task the word awesome to have a high positive weight, and abysmal to have a very negative weight."}, {"id": "2765", "contributed_by": "group 10", "title": "", "section": "", "text": "Backpropagation Through Time (BPTT) is a general approach for training the weights in RNNs. It involves processing the input sequence in the forward direction to generate the output sequence, and then using the output sequence to compute the loss function. The loss function is then backpropagated through the network in the reverse direction using the chain rule of calculus, allowing the gradients to be computed for all the weights in the network. However, because the RNN is unrolled over time, the gradients can become unstable and either vanish or explode. To address this problem, various modifications have been proposed, such as gradient clipping and using different activation functions. Despite its challenges, BPTT remains a popular method for training RNNs and has been used in many applications, including speech recognition and machine translation."}], "metadata": {}, "id": "47"}
{"query": "question: What is the advantage of weight tying answer: <extra_id_0>", "answers": ["Weight tying improves model perplexity and significantly reduces the number of parameters required for the model"], "generation": "Weight tying reduces the number of parameters in the model and improves its efficiency. In some architectures, there are redundant sets of weights Weight Weight tying Using", "passages": [{"id": "2770", "contributed_by": "group 10", "title": "", "section": "", "text": "Weight tying is a technique used in neural network architectures to reduce the number of parameters in the model and improve its efficiency. In some architectures, such as language models, there are redundant sets of weights that perform similar functions, such as the input and output word embeddings. Weight tying dispenses with this redundancy and simply uses a single set of embeddings at both the input and output layers. This technique has been shown to be effective in reducing overfitting and improving generalization, particularly in tasks such as language modeling and machine translation. By sharing the same set of weights across different parts of the model, weight tying encourages the network to learn more meaningful representations of the input data and make more accurate predictions."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "2355", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "By contrast, logistic regression is much more robust to correlated features; if two features f1 and f2 are perfectly correlated, regression will simply assign part of the weight to w1 and part to w2."}, {"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "2765", "contributed_by": "group 10", "title": "", "section": "", "text": "Backpropagation Through Time (BPTT) is a general approach for training the weights in RNNs. It involves processing the input sequence in the forward direction to generate the output sequence, and then using the output sequence to compute the loss function. The loss function is then backpropagated through the network in the reverse direction using the chain rule of calculus, allowing the gradients to be computed for all the weights in the network. However, because the RNN is unrolled over time, the gradients can become unstable and either vanish or explode. To address this problem, various modifications have been proposed, such as gradient clipping and using different activation functions. Despite its challenges, BPTT remains a popular method for training RNNs and has been used in many applications, including speech recognition and machine translation."}, {"id": "3010", "contributed_by": "group 2", "title": "", "section": "", "text": "Conjunctions join two phrases, clauses, or sentences. Coordinating conjunctions like and, or, and but join two elements of equal status. Subordinating conjunctions are used when one of the elements has some embedded status. For example, the subordinating conjunction that in 'I thought that you might like some milk' links the main clause I thought with the subordinate clause you might like some milk. This clause is called subordinate because this entire clause is the 'content' of the main verb thought. Subordinating conjunctions like that which link a verb to its argument in this way are also called complementizers."}, {"id": "34", "contributed_by": "group 9", "title": "", "section": "", "text": "Logistic regression solves this task by learning, from a training set, a vector of weights and a bias term. Each weight wi is a real number, and is associated with one of the input features xi. The weight wi represents how important that input feature is to the classification decision, and can be positive (providing evidence that the in- stance being classified belongs in the positive class) or negative (providing evidence that the instance being classified belongs in the negative class). Thus we might expect in a sentiment task the word awesome to have a high positive weight, and abysmal to have a very negative weight. The bias term, also called the intercept, is another real number thats added to the weighted inputs."}, {"id": "3382", "contributed_by": "group 2", "title": "", "section": "", "text": "The weights Ws, which are then trained during normal end-to-end training, give the network the ability to learn which aspects of similarity between the decoder and encoder states are important to the current application. This bilinear model also allows the encoder and decoder to use different dimensional vectors, whereas the simple dot-product attention requires that the encoder and decoder hidden states have the same dimensionality."}, {"id": "2486", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "If a feature is perfectly predictive of the outcome because it happens to only occur in one class, it will be assigned a very high weight."}, {"id": "2158", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "Thus it weighs the lower of the two numbers more heavily.."}], "metadata": {}, "id": "48"}
{"query": "question: What kind of tasks can RNN solve answer: <extra_id_0>", "answers": ["Examples include sequence classification, sequence labeling, and text generation tasks"], "generation": "Several tasks can be solved by RNNs, such as speech recognition, machine translation, and sequence modeling.", "passages": [{"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}, {"id": "2763", "contributed_by": "group 10", "title": "", "section": "", "text": "One of the main advantages of RNNs over n-gram models is that RNNs don't have the limited context problem. Because the hidden state of an RNN depends on all the previous inputs in the sequence, it can in principle represent information about all of the preceding words, all the way back to the beginning of the sequence. This allows the model to capture long-range dependencies in the input, which can be crucial for many sequence modeling tasks, such as speech recognition and machine translation. However, in practice, the RNN's ability to remember information from the distant past is limited by the vanishing gradient problem, which can cause the model to lose track of long-term dependencies over time."}, {"id": "2774", "contributed_by": "group 10", "title": "", "section": "", "text": "The idea behind stacked RNNs is to combine multiple recurrent layers to capture more complex temporal dependencies in sequential data. In a stacked RNN, each layer receives the output of the previous layer as its input, allowing the network to process the sequence hierarchically. This means that the output of one layer serves as the input to a subsequent layer, allowing the network to build increasingly abstract representations of the sequence. Stacked RNNs have been shown to be effective in a variety of sequence modeling tasks, such as speech recognition and natural language processing."}, {"id": "3212", "contributed_by": "group 2", "title": "", "section": "", "text": "One reason for the inability of RNNs to carry forward critical information is that the hidden layers, and, by extension, the weights that determine the values in the hidden layer, are being asked to perform two tasks simultaneously: provide information useful for the current decision, and updating and carrying forward information required for future decisions."}, {"id": "2775", "contributed_by": "group 10", "title": "", "section": "", "text": "a key limitation of RNNs in that they have difficulty capturing long-term dependencies in sequential data. While they can provide useful information at each step of the sequence, they may lose important information from earlier steps as the sequence progresses. This limitation has led to the development of more advanced sequence models, such as LSTMs and Transformers, which can better capture long-term dependencies and process sequential data more effectively."}, {"id": "3210", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN for sequence classification. The final hidden units from the forward and backward passes are combined to represent the entire sequence. This combined representation serves as input to the subsequent classifier."}, {"id": "3196", "contributed_by": "group 2", "title": "", "section": "", "text": "Recurrent networks are quite flexible. By combining the feedforward nature of unrolled computational graphs with vectors as common inputs and outputs, complex networks can be treated as modules that can be combined in creative ways. This section introduces two of the more common network architectures used in language processing with RNNs."}, {"id": "3209", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN. Separate models are trained in the forward and backward directions, with the output of each model at each time point concatenated to represent the bidirectional state at that time point."}, {"id": "3290", "contributed_by": "group 2", "title": "", "section": "", "text": "Common language-based applications for RNNs and transformers include: Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. Auto-regressive generation using a trained language model. Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification."}], "metadata": {}, "id": "49"}
{"query": "question: What is sequence labeling answer: <extra_id_0>", "answers": ["To assign a label chosen from a small fixed set of labels to each element of a sequence, like the part-of-speech tagging and named entity recognition"], "generation": "Sequence labeling is a common task in natural language processing that involves assigning labels to each element of a sequence.", "passages": [{"id": "3290", "contributed_by": "group 2", "title": "", "section": "", "text": "Common language-based applications for RNNs and transformers include: Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. Auto-regressive generation using a trained language model. Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification."}, {"id": "2769", "contributed_by": "group 10", "title": "", "section": "", "text": "Sequence labeling is a common task in natural language processing that involves assigning labels to each element of a sequence. In an RNN approach to sequence labeling, inputs are typically word embeddings that represent the input sequence, and the outputs are tag probabilities generated by a softmax layer over the given tagset. The RNN processes the input sequence one token at a time, and the hidden state at each time step is updated based on the current input and the previous hidden state. The output at each time step is a probability distribution over the possible tags, and the tag with the highest probability is selected as the prediction. This approach has been used in a variety of sequence labeling tasks, such as named entity recognition and part-of-speech tagging, and has been shown to be effective in capturing the contextual dependencies between the input tokens."}, {"id": "3178", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech tagging as sequence labeling with a simple RNN. Pre-trained word embeddings serve as inputs and a softmax layer provides a probability distribution over the part-of-speech tags as output at each time step."}, {"id": "3125", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words."}, {"id": "3177", "contributed_by": "group 2", "title": "", "section": "", "text": "In sequence labeling, the network's task is to assign a label chosen from a small fixed set of labels to each element of a sequence, like the part-of-speech tagging and named entity recognition tasks from Chapter 8. In an RNN approach to sequence labeling, inputs are word embeddings and the outputs are tag probabilities generated by a softmax layer over the given tagset, as illustrated in Fig. 9.7."}, {"id": "3210", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN for sequence classification. The final hidden units from the forward and backward passes are combined to represent the entire sequence. This combined representation serves as input to the subsequent classifier."}, {"id": "3128", "contributed_by": "group 2", "title": "", "section": "", "text": "The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence"}, {"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "2996", "contributed_by": "group 2", "title": "", "section": "", "text": "Such tasks in which we assign, to each word xi in an input word sequence, a label yi, so that the output sequence Y has the same length as the input sequence X are called sequence labeling tasks. We'll introduce classic sequence labeling algorithms, one generative- the Hidden Markov Model (HMM)-and one discriminative- the Conditional Random Field (CRF). In following chapters we'll introduce modern sequence labelers based on RNNs and Transformers."}], "metadata": {}, "id": "50"}
{"query": "question: How does RNN solve sequence labeling answer: <extra_id_0>", "answers": ["The outputs of the network at each time step represent the distribution over the POS tagset generated by a softmax layer"], "generation": "RNN solves sequence labeling by processing the input sequence one token at a time, and the hidden state at each time step is tag probabilities generated by a softmax layer Using Using Using", "passages": [{"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}, {"id": "3210", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN for sequence classification. The final hidden units from the forward and backward passes are combined to represent the entire sequence. This combined representation serves as input to the subsequent classifier."}, {"id": "3290", "contributed_by": "group 2", "title": "", "section": "", "text": "Common language-based applications for RNNs and transformers include: Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. Auto-regressive generation using a trained language model. Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification."}, {"id": "3177", "contributed_by": "group 2", "title": "", "section": "", "text": "In sequence labeling, the network's task is to assign a label chosen from a small fixed set of labels to each element of a sequence, like the part-of-speech tagging and named entity recognition tasks from Chapter 8. In an RNN approach to sequence labeling, inputs are word embeddings and the outputs are tag probabilities generated by a softmax layer over the given tagset, as illustrated in Fig. 9.7."}, {"id": "3178", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech tagging as sequence labeling with a simple RNN. Pre-trained word embeddings serve as inputs and a softmax layer provides a probability distribution over the part-of-speech tags as output at each time step."}, {"id": "2769", "contributed_by": "group 10", "title": "", "section": "", "text": "Sequence labeling is a common task in natural language processing that involves assigning labels to each element of a sequence. In an RNN approach to sequence labeling, inputs are typically word embeddings that represent the input sequence, and the outputs are tag probabilities generated by a softmax layer over the given tagset. The RNN processes the input sequence one token at a time, and the hidden state at each time step is updated based on the current input and the previous hidden state. The output at each time step is a probability distribution over the possible tags, and the tag with the highest probability is selected as the prediction. This approach has been used in a variety of sequence labeling tasks, such as named entity recognition and part-of-speech tagging, and has been shown to be effective in capturing the contextual dependencies between the input tokens."}, {"id": "2774", "contributed_by": "group 10", "title": "", "section": "", "text": "The idea behind stacked RNNs is to combine multiple recurrent layers to capture more complex temporal dependencies in sequential data. In a stacked RNN, each layer receives the output of the previous layer as its input, allowing the network to process the sequence hierarchically. This means that the output of one layer serves as the input to a subsequent layer, allowing the network to build increasingly abstract representations of the sequence. Stacked RNNs have been shown to be effective in a variety of sequence modeling tasks, such as speech recognition and natural language processing."}, {"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "2779", "contributed_by": "group 10", "title": "", "section": "", "text": "The Encoder-Decoder architecture is a popular approach for solving sequence-to-sequence tasks in natural language processing with RNNs. It involves an encoder network that takes an input sequence and generates a contextualized representation of it, which is then passed to a decoder that generates a task-specific output sequence. However, while this architecture has been successful for tasks such as machine translation and speech recognition, it has its limitations. The Encoder-Decoder architecture is designed specifically for sequence-to-sequence tasks and cannot be used for other NLP tasks, such as sentiment analysis or named entity recognition. Additionally, it may struggle with longer input sequences or those that contain complex dependencies, as the encoder's output representation must encapsulate all information needed for the decoder to generate the correct output sequence"}, {"id": "2775", "contributed_by": "group 10", "title": "", "section": "", "text": "a key limitation of RNNs in that they have difficulty capturing long-term dependencies in sequential data. While they can provide useful information at each step of the sequence, they may lose important information from earlier steps as the sequence progresses. This limitation has led to the development of more advanced sequence models, such as LSTMs and Transformers, which can better capture long-term dependencies and process sequential data more effectively."}], "metadata": {}, "id": "51"}
{"query": "question: How does RNN solve sequence classification answer: <extra_id_0>", "answers": ["We can take the hidden layer for the last token of the text as a compressed representation of the text. Then, such representation is fed through a feed-forward classification network"], "generation": "RNN solves sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN Sequence classification using", "passages": [{"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}, {"id": "3210", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN for sequence classification. The final hidden units from the forward and backward passes are combined to represent the entire sequence. This combined representation serves as input to the subsequent classifier."}, {"id": "3290", "contributed_by": "group 2", "title": "", "section": "", "text": "Common language-based applications for RNNs and transformers include: Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. Auto-regressive generation using a trained language model. Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification."}, {"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "2774", "contributed_by": "group 10", "title": "", "section": "", "text": "The idea behind stacked RNNs is to combine multiple recurrent layers to capture more complex temporal dependencies in sequential data. In a stacked RNN, each layer receives the output of the previous layer as its input, allowing the network to process the sequence hierarchically. This means that the output of one layer serves as the input to a subsequent layer, allowing the network to build increasingly abstract representations of the sequence. Stacked RNNs have been shown to be effective in a variety of sequence modeling tasks, such as speech recognition and natural language processing."}, {"id": "2765", "contributed_by": "group 10", "title": "", "section": "", "text": "Backpropagation Through Time (BPTT) is a general approach for training the weights in RNNs. It involves processing the input sequence in the forward direction to generate the output sequence, and then using the output sequence to compute the loss function. The loss function is then backpropagated through the network in the reverse direction using the chain rule of calculus, allowing the gradients to be computed for all the weights in the network. However, because the RNN is unrolled over time, the gradients can become unstable and either vanish or explode. To address this problem, various modifications have been proposed, such as gradient clipping and using different activation functions. Despite its challenges, BPTT remains a popular method for training RNNs and has been used in many applications, including speech recognition and machine translation."}, {"id": "3207", "contributed_by": "group 2", "title": "", "section": "", "text": "Bidirectional RNNs have also proven to be quite effective for sequence classification. Recall from Fig. 9.8 that for sequence classification we used the final hidden state of the RNN as the input to a subsequent feedforward classifier. A difficulty with this approach is that the final state naturally reflects more information about the end of the sentence than its beginning. Bidirectional RNNs provide a simple solution to this problem; as shown in Fig. 9.12, we simply combine the final hidden states from the forward and backward passes (for example by concatenation) and use that as input for follow-on processing."}, {"id": "3178", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech tagging as sequence labeling with a simple RNN. Pre-trained word embeddings serve as inputs and a softmax layer provides a probability distribution over the part-of-speech tags as output at each time step."}, {"id": "2775", "contributed_by": "group 10", "title": "", "section": "", "text": "a key limitation of RNNs in that they have difficulty capturing long-term dependencies in sequential data. While they can provide useful information at each step of the sequence, they may lose important information from earlier steps as the sequence progresses. This limitation has led to the development of more advanced sequence models, such as LSTMs and Transformers, which can better capture long-term dependencies and process sequential data more effectively."}, {"id": "3181", "contributed_by": "group 2", "title": "", "section": "", "text": "Another use of RNNs is to classify entire sequences rather than the tokens within them. We've already encountered sentiment analysis in Chapter 4, in which we classify a text as positive or negative. Other sequence classification tasks for mapping sequences of text to one from a small set of categories include document-level topic classification, spam detection, or message routing for customer service applications."}], "metadata": {}, "id": "52"}
{"query": "question: What is end-to-end training answer: <extra_id_0>", "answers": ["The training regimen that uses the loss from a downstream application to adjust the weights all the way through the network"], "generation": "End-to-end training is the process of training a network end-to-end, using the same training data as with the RNN language models. End End", "passages": [{"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "2074", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "This can be done by sorting the vocabulary by frequency in the training set, and dening the top 10100 vocabulary entries as stop words, or alternatively by using one of the many predened stop word lists available online.."}, {"id": "3128", "contributed_by": "group 2", "title": "", "section": "", "text": "The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence"}, {"id": "3382", "contributed_by": "group 2", "title": "", "section": "", "text": "The weights Ws, which are then trained during normal end-to-end training, give the network the ability to learn which aspects of similarity between the decoder and encoder states are important to the current application. This bilinear model also allows the encoder and decoder to use different dimensional vectors, whereas the simple dot-product attention requires that the encoder and decoder hidden states have the same dimensionality."}, {"id": "524", "contributed_by": "group 11", "title": "Chapter 13 - Constituency Parsing", "section": "", "text": "The CRF tagging model can also be used, using similar features and training by maximizing log-likelihood of the training corpus and decoding via the Viterbi algorithm."}, {"id": "2258", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "Classiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.."}, {"id": "3358", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder architectures are trained end-to-end, just as with the RNN language models of Chapter 9. Each training example is a tuple of paired strings, a source and a target. Concatenated with a separator token, these source-target pairs can now serve as training data."}, {"id": "3537", "contributed_by": "group 2", "title": "", "section": "", "text": "During training, the output vector from the final layer associated with the [CLS] token represents the next sentence prediction. As with the MLM objective, a learned set of classification weights WNSP is used to produce a two-class prediction from the raw [CLS] vector."}, {"id": "3579", "contributed_by": "group 2", "title": "", "section": "", "text": "With this approach, fine-tuning entails using supervised training data to learn the parameters of the final classifier, as well as the weights used to generate the boundary representations, and the weights in the self-attention layer that generates the span content representation. During training, the model's predictions for all spans are compared to their gold-standard labels and cross-entropy loss is used to drive the training."}, {"id": "1474", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "The token learner takes a raw training corpus (sometimes roughly preseparated into words, for example by whitespace) and induces a vocabulary, a set of tokens."}], "metadata": {}, "id": "53"}
{"query": "question: What is autoregression generation answer: <extra_id_0>", "answers": ["It is the use of a language model to incrementally generate words by repeatedly sampling the next word conditioned on the previous choices"], "generation": "Autoregressive generation is a type of sequential generative modeling where the output at each time step is conditioned on the output from previous time steps. Autoregression generation is GEN Gen Autoregression generation", "passages": [{"id": "3353", "contributed_by": "group 2", "title": "", "section": "", "text": "hidden state of the decoder. That is, the first decoder RNN cell uses c as its prior hidden state hd0. The decoder autoregressively generates a sequence of outputs, an element at a time, until an end-of-sequence marker is generated. Each hidden state is conditioned on the previous hidden state and the output generated in the previous state."}, {"id": "2772", "contributed_by": "group 10", "title": "", "section": "", "text": "The approach of using a language model to incrementally generate words by repeatedly sampling the next word conditioned on our previous choices is called autoregressive generation. Autoregressive generation is a type of sequential generative modeling where the output at each time step is conditioned on the output from previous time steps. The output is generated one token at a time, with each token being conditioned on the previously generated tokens. This approach has been used in a variety of natural language processing tasks, such as machine translation, image captioning, and text generation. The term 'autoregressive autoregressive generation' in the sentence you provided appears to be a repetition or error, as the correct term for this approach is simply 'autoregressive generation'"}, {"id": "3190", "contributed_by": "group 2", "title": "", "section": "", "text": "Today, this approach of using a language model to incrementally generate words by repeatedly sampling the next word conditioned on our previous choices is called autoregressive generation. The procedure is basically the same as that described on 39, in a neural context:"}, {"id": "3270", "contributed_by": "group 2", "title": "", "section": "", "text": "A simple variation on autoregressive generation that underlies a number of practical applications uses a prior context to prime the autoregressive generation process. Fig. 9.22 illustrates this with the task of text completion. Here a standard language model is given the prefix to some text and is asked to generate a possible completion to it. Note that as the generation process proceeds, the model has direct access to the priming context as well as to all of its own subsequently generated outputs. This ability to incorporate the entirety of the earlier context and generated outputs at each time step is the key to the power of these models."}, {"id": "3290", "contributed_by": "group 2", "title": "", "section": "", "text": "Common language-based applications for RNNs and transformers include: Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. Auto-regressive generation using a trained language model. Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification."}, {"id": "3232", "contributed_by": "group 2", "title": "", "section": "", "text": "independent of all the other computations. The first point ensures that we can use this approach to create language models and use them for autoregressive generation, and the second point means that we can easily parallelize both forward inference and training of such models."}, {"id": "3194", "contributed_by": "group 2", "title": "", "section": "", "text": "Technically an autoregressive model is a model that predicts a value at time t based on a linear function of the previous values at times t - 1, t - 2, and so on. Although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive generation since the word generated at each time step is conditioned on the word selected by the network from the previous step. Fig. 9.9 illustrates this approach. In this figure, the details of the RNN's hidden layers and recurrent connections are hidden within the blue block."}, {"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}, {"id": "3271", "contributed_by": "group 2", "title": "", "section": "", "text": "Text summarization is a practical application of context-based autoregressive generation. The task is to take a full-length article and produce an effective summary of it. To train a transformer-based autoregressive model to perform this task, we start with a corpus consisting of full-length articles accompanied by their corresponding summaries. Fig. 9.23 shows an example of this kind of data from a widely used summarization corpus consisting of CNN and Daily Mirror news articles."}, {"id": "2786", "contributed_by": "group 10", "title": "", "section": "", "text": "In the Transformer, the decoder is also an autoregressive model. However, unlike the seq2seq model with RNN-based decoders, the decoder in the Transformer is feedforward, with multiple layers of self-attention followed by a point-wise feedforward network. The Transformer blocks in the decoder have an extra cross-attention layer to attend to the source language (i.e., the input language), similar to the encoder. To generate the target sequence, the Transformer decoder uses masked self-attention over the target side to prevent information from the future leaking into the decoding process. The outputs from the decoder are then fed into a final linear layer followed by a softmax activation function to produce the output probabilities."}], "metadata": {}, "id": "54"}
{"query": "question: What is bidirectional RNN answer: <extra_id_0>", "answers": ["It combines two independent RNNs, one processing the input from the start to the end, and the other from the end to the start. The representations computed by the networks are then combined into a single vector"], "generation": "A bidirectional RNN for sequence classification. Separate models are trained in the forward and backward directions, with the final hidden units A :", "passages": [{"id": "3209", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN. Separate models are trained in the forward and backward directions, with the output of each model at each time point concatenated to represent the bidirectional state at that time point."}, {"id": "3210", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN for sequence classification. The final hidden units from the forward and backward passes are combined to represent the entire sequence. This combined representation serves as input to the subsequent classifier."}, {"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}, {"id": "3205", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN (Schuster and Paliwal, 1997) combines two independent RNNs, one where the input is processed from the start to the end, and the other from the end to the start. We then concatenate the two representations computed by the networks into a single vector that captures both the left and right contexts of an input at each point in time. Here we use either the semicolon ';' or the equivalent symbol to mean vector concatenation:"}, {"id": "3350", "contributed_by": "group 2", "title": "", "section": "", "text": "A more formal version of translating a sentence at inference time in the basic RNN-based encoder-decoder architecture. The final hidden state of the encoder RNN, hen, serves as the context for the decoder in its role as hd0 in the decoder RNN."}, {"id": "3460", "contributed_by": "group 2", "title": "", "section": "", "text": "The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder."}, {"id": "2775", "contributed_by": "group 10", "title": "", "section": "", "text": "a key limitation of RNNs in that they have difficulty capturing long-term dependencies in sequential data. While they can provide useful information at each step of the sequence, they may lose important information from earlier steps as the sequence progresses. This limitation has led to the development of more advanced sequence models, such as LSTMs and Transformers, which can better capture long-term dependencies and process sequential data more effectively."}, {"id": "3196", "contributed_by": "group 2", "title": "", "section": "", "text": "Recurrent networks are quite flexible. By combining the feedforward nature of unrolled computational graphs with vectors as common inputs and outputs, complex networks can be treated as modules that can be combined in creative ways. This section introduces two of the more common network architectures used in language processing with RNNs."}, {"id": "2774", "contributed_by": "group 10", "title": "", "section": "", "text": "The idea behind stacked RNNs is to combine multiple recurrent layers to capture more complex temporal dependencies in sequential data. In a stacked RNN, each layer receives the output of the previous layer as its input, allowing the network to process the sequence hierarchically. This means that the output of one layer serves as the input to a subsequent layer, allowing the network to build increasingly abstract representations of the sequence. Stacked RNNs have been shown to be effective in a variety of sequence modeling tasks, such as speech recognition and natural language processing."}], "metadata": {}, "id": "55"}
{"query": "question: What problem does LSTM solve answer: <extra_id_0>", "answers": ["The inability of RNNs to carry forward critical information and the vanishing gradient problem"], "generation": "LSTMs solve the context management problem by learning how to manage this context rather than hard-coding a strategy into the architecture.", "passages": [{"id": "3225", "contributed_by": "group 2", "title": "", "section": "", "text": "A single LSTM unit displayed as a computation graph. The inputs to each unit consists of the current input, x, the previous hidden state, ht-1, and the previous context, ct-1. The outputs are a new hidden state, ht and an updated context, ct ."}, {"id": "3222", "contributed_by": "group 2", "title": "", "section": "", "text": "Fig. 9.13 illustrates the complete computation for a single LSTM unit. Given the appropriate weights for the various gates, an LSTM accepts as input the context layer, and hidden layer from the previous time step, along with the current input vector. It then generates updated context and hidden vectors as output. The hidden layer, ht , can be used as input to subsequent layers in a stacked RNN, or to generate an output for the final layer of a network."}, {"id": "3227", "contributed_by": "group 2", "title": "", "section": "", "text": "The increased complexity of the LSTM units is encapsulated within the unit itself. The only additional external complexity for the LSTM over the basic recurrent unit (b) is the presence of the additional context vector as an input and output."}, {"id": "3228", "contributed_by": "group 2", "title": "", "section": "", "text": "This modularity is key to the power and widespread applicability of LSTM units. LSTM units (or other varieties, like GRUs) can be substituted into any of the network architectures described in Section 9.5. And, as with simple RNNs, multi-layered networks making use of gated units can be unrolled into deep feedforward networks and trained in the usual fashion with backpropagation."}, {"id": "3216", "contributed_by": "group 2", "title": "", "section": "", "text": "The gates in an LSTM share a common design pattern; each consists of a feedforward layer, followed by a sigmoid activation function, followed by a pointwise multiplication with the layer being gated. The choice of the sigmoid as the activation function arises from its tendency to push its outputs to either 0 or 1. Combining this with a pointwise multiplication has an effect similar to that of a binary mask. Values in the layer being gated that align with values near 1 in the mask are passed through nearly unchanged; values corresponding to lower values are essentially erased."}, {"id": "3226", "contributed_by": "group 2", "title": "", "section": "", "text": "Basic neural units used in feedforward, simple recurrent networks (SRN), and long short-term memory (LSTM)."}, {"id": "3215", "contributed_by": "group 2", "title": "", "section": "", "text": "The most commonly used such extension to RNNs is the Long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997). LSTMs divide the context management problem into two sub-problems: removing information no longer needed from the context, and adding information likely to be needed for later decision making. The key to solving both problems is to learn how to manage this context rather than hard-coding a strategy into the architecture. LSTMs accomplish this by first adding an explicit context layer to the architecture (in addition to the usual recurrent hidden layer), and through the use of specialized neural units that make use of gates to control the flow of information into and out of the units that comprise the network layers. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers."}, {"id": "2835", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "This chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights:  A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.  Many words are polysemous, having many senses.  Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.  WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.  Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.  The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.  Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.  An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.  Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target words neighborhood.  Word sense induction is the task of learning word senses unsupervised."}, {"id": "3229", "contributed_by": "group 2", "title": "", "section": "", "text": "While the addition of gates allows LSTMs to handle more distant information than RNNs, they don't completely solve the underlying problem: passing information through an extended series of recurrent connections leads to information loss and difficulties in training. Moreover, the inherently sequential nature of recurrent networks makes it hard to do computation in parallel. These considerations led to the development of transformers - an approach to sequence processing that eliminates recurrent connections and returns to architectures reminiscent of the fully connected networks described earlier in Chapter 7."}, {"id": "3336", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks consist of three components: 1. An encoder that accepts an input sequence, x1n, and generates a corresponding sequence of contextualized representations, hn1. LSTMs, convolutional networks, and Transformers can all be employed as encoders."}], "metadata": {}, "id": "56"}
{"query": "question: What is the key advantages of Transformers answer: <extra_id_0>", "answers": ["Transformers were designed to eliminate recurrent connections and return to architectures reminiscent of the fully connected networks"], "generation": "Transformers are non-recurrent networks based on self-attention. A selfattention layer maps sequences of input vectors to output vectors of the same length. Transformers", "passages": [{"id": "3460", "contributed_by": "group 2", "title": "", "section": "", "text": "The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder."}, {"id": "3230", "contributed_by": "group 2", "title": "", "section": "", "text": "Transformers map sequences of input vectors (x1,...,xn) to sequences of output vectors (y1,...,yn) of the same length. Transformers are made up of stacks of transformer blocks, which are multilayer networks made by combining simple linear layers, feedforward networks, and self-attention layers, the key innovation of transformers. Self-attention allows a network to directly extract and use information from arbitrarily large contexts without the need to pass it through intermediate recurrent connections as in RNNs. We'll start by describing how self-attention works and then return to how it fits into larger transformer blocks."}, {"id": "3288", "contributed_by": "group 2", "title": "", "section": "", "text": "Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, using attention heads that model how the surrounding words are relevant for the processing of the current word."}, {"id": "3509", "contributed_by": "group 2", "title": "", "section": "", "text": "Finally, a fundamental issue with transformers is that the size of the input layer dictates the complexity of model. Both the time and memory requirements in a transformer grow quadratically with the length of the input. It's necessary, therefore, to set a fixed input length that is long enough to provide sufficient context for the model to function and yet still be computationally tractable. For BERT, a fixed input size of 512 subword tokens was used."}, {"id": "3260", "contributed_by": "group 2", "title": "", "section": "", "text": "The different words in a sentence can relate to each other in many different ways simultaneously. For example, distinct syntactic, semantic, and discourse relationships can hold between verbs and their arguments in a sentence. It would be difficult for a single transformer block to learn to capture all of the different kinds of parallel relations among its inputs. Transformers address this issue with multihead selfattention layers. These are sets of self-attention layers, called heads, that reside in parallel layers at the same depth in a model, each with its own set of parameters. Given these distinct sets of parameters, each head can learn different aspects of the relationships that exist among inputs at the same level of abstraction."}, {"id": "2775", "contributed_by": "group 10", "title": "", "section": "", "text": "a key limitation of RNNs in that they have difficulty capturing long-term dependencies in sequential data. While they can provide useful information at each step of the sequence, they may lose important information from earlier steps as the sequence progresses. This limitation has led to the development of more advanced sequence models, such as LSTMs and Transformers, which can better capture long-term dependencies and process sequential data more effectively."}, {"id": "3244", "contributed_by": "group 2", "title": "", "section": "", "text": "To capture these three different roles, transformers introduce weight matrices WQ, WK, and WV. These weights will be used to project each input vector xi into a representation of its role as a key, query, or value."}, {"id": "3459", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence."}, {"id": "2803", "contributed_by": "group 10", "title": "", "section": "", "text": "This will require, as we'll see, that for some NLP tasks that require notions of words (like named entity tagging, or parsing) we will occasionally need to map subwords back to words. Finally, a fundamental issue with transformers is that the size of the input layer dictates the complexity of model. Both the time and memory requirements in a transformer grow quadratically with the length of the input. It's necessary, therefore, to set a fixed input length that is long enough to provide sufficient context for the model to function and yet still be computationally tractable. For BERT, a fixed input size of 512 subword tokens was used."}, {"id": "3240", "contributed_by": "group 2", "title": "", "section": "", "text": "But transformers allow us to create a more sophisticated way of representing how words can contribute to the representation of longer inputs. Consider the three different roles that each input embedding plays during the course of the attention process."}], "metadata": {}, "id": "57"}
{"query": "question: What is self-attention answer: <extra_id_0>", "answers": ["The use of self-attention layers allows a network to directly extract and use information from arbitrarily large contexts without the need to pass it through intermediate recurrent connections as in RNNs."], "generation": "Self-attention is a form of attention-based processing that focuses on the inputs of interest to a collection of other inputs. Self-attention Self-attention Self-attention", "passages": [{"id": "3493", "contributed_by": "group 2", "title": "", "section": "", "text": "Information flow in a bidirectional self-attention model. In processing each element of the sequence, the model attends to all inputs, both before and after the current one."}, {"id": "3288", "contributed_by": "group 2", "title": "", "section": "", "text": "Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, using attention heads that model how the surrounding words are relevant for the processing of the current word."}, {"id": "3233", "contributed_by": "group 2", "title": "", "section": "", "text": "Information flow in a causal (or masked) self-attention model. In processing each element of the sequence, the model attends to all the inputs up to, and including, the current one. Unlike RNNs, the computations at each time step are independent of all the other steps and therefore can be performed in parallel."}, {"id": "3370", "contributed_by": "group 2", "title": "", "section": "", "text": "The idea of attention is instead to create the single fixed-length vector c by taking a weighted sum of all the encoder hidden states. The weights focus on ('attend to') a particular part of the source text that is relevant for the token the decoder is currently producing. Attention thus replaces the static context vector with one that is dynamically derived from the encoder hidden states, different for each token in decoding."}, {"id": "2790", "contributed_by": "group 10", "title": "", "section": "", "text": "The first attention mechanism we describe is dot-product attention. In this form of attention, the relevance of each encoder state to the current step of the decoder is measured using dot-product attention, which computes the dot product between the decoder hidden state and each encoder hidden state. The dot product is then normalized across the encoder hidden states, and these normalized scores are used as attention weights to compute a weighted sum of the encoder hidden states. The resulting context vector is a weighted sum of the encoder hidden states, where the weights are determined by the similarity between the decoder hidden state and each encoder hidden state."}, {"id": "3460", "contributed_by": "group 2", "title": "", "section": "", "text": "The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder."}, {"id": "3255", "contributed_by": "group 2", "title": "", "section": "", "text": "The self-attention calculation lies at the core of what's called a transformer block, which, in addition to the self-attention layer, includes additional feedforward layers, residual connections, and normalizing layers. The input and output dimensions of these blocks are matched so they can be stacked just as was the case for stacked RNNs."}, {"id": "3234", "contributed_by": "group 2", "title": "", "section": "", "text": "At the core of an attention-based approach is the ability to compare an item of interest to a collection of other items in a way that reveals their relevance in the current context. In the case of self-attention, the set of comparisons are to other elements within a given sequence. The result of these comparisons is then used to compute an output for the current input. For example, returning to Fig. 9.15, the computation of y3 is based on a set of comparisons between the input x3 and its preceding elements x1 and x2, and to x3 itself. The simplest form of comparison between elements in a self-attention layer is a dot product. Let's refer to the result of this comparison as a score (we'll be updating this equation to add attention to the computation of this score):"}, {"id": "3243", "contributed_by": "group 2", "title": "", "section": "", "text": "And finally, as a value used to compute the output for the current focus of attention."}, {"id": "3231", "contributed_by": "group 2", "title": "", "section": "", "text": "Fig. 9.15 illustrates the flow of information in a single causal, or backward looking, self-attention layer. As with the overall transformer, a self-attention layer maps input sequences (x1 , ..., xn ) to output sequences of the same length (y1 , ..., yn ). When processing each item in the input, the model has access to all of the inputs up to and including the one under consideration, but no access to information about inputs beyond the current one. In addition, the computation performed for each item is"}], "metadata": {}, "id": "58"}
{"query": "question: What layer normalization is important answer: <extra_id_0>", "answers": ["It can improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training"], "generation": "Layer normalization is important to facilitate learning and improve model stability. Layer normalization is one of many forms of normalization that can be used to improve training performance Layer norm", "passages": [{"id": "3257", "contributed_by": "group 2", "title": "", "section": "", "text": "Layer normalization (or layer norm) is one of many forms of normalization that can be used to improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training. Layer norm is a variation of the standard score, or z-score, from statistics applied to a single hidden layer. The first step in layer normalization is to calculate the mean, mu, and standard deviation, sigma, over the elements of the vector to be normalized. Given a hidden layer with dimensionality dh, these values are calculated as follows."}, {"id": "3259", "contributed_by": "group 2", "title": "", "section": "", "text": "Finally, in the standard implementation of layer normalization, two learnable parameters, gamma and beta , representing gain and offset values, are introduced."}, {"id": "3289", "contributed_by": "group 2", "title": "", "section": "", "text": "A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks."}, {"id": "2983", "contributed_by": "group 2", "title": "", "section": "", "text": "Various forms of regularization are used to prevent overfitting. One of the most dropout important is dropout: randomly dropping some units and their connections from the network during training (Hinton et al. 2012, Srivastava et al. 2014). Tuning hyperparameter of hyperparameters is also important. The parameters of a neural network are the weightsW and biases b; those are learned by gradient descent. The hyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training set. Hyperparameters include the learning rate h, the mini-batch size, the model architecture (the number of layers, the number of hidden nodes per layer, the choice of activation functions), how to regularize, and so on. Gradient descent itself also has many architectural variants such as Adam (Kingma and Ba, 2015)."}, {"id": "2776", "contributed_by": "group 10", "title": "", "section": "", "text": "A transformer block consists of three main components: a self-attention layer, a feedforward layer, and a set of normalization layers. The self-attention layer computes a weighted sum of the input sequence based on its own learned attention weights, while the feedforward layer applies a nonlinear transformation to the resulting representation. Residual connections are used to ensure that the original input sequence is preserved throughout the transformation process, while layer normalization is used to facilitate learning and improve model stability"}, {"id": "3255", "contributed_by": "group 2", "title": "", "section": "", "text": "The self-attention calculation lies at the core of what's called a transformer block, which, in addition to the self-attention layer, includes additional feedforward layers, residual connections, and normalizing layers. The input and output dimensions of these blocks are matched so they can be stacked just as was the case for stacked RNNs."}, {"id": "3256", "contributed_by": "group 2", "title": "", "section": "", "text": "Fig. 9.18 illustrates a standard transformer block consisting of a single attention layer followed by a fully-connected feedforward layer with residual connections and layer normalizations following each. We've already seen feedforward layers in Chapter 7, but what are residual connections and layer norm? In deep networks, residual connections are connections that pass information from a lower layer to a higher layer without going through the intermediate layer. Allowing information from the activation going forward and the gradient going backwards to skip a layer improves learning and gives higher level layers direct access to information from lower layers (He et al., 2016). Residual connections in transformers are implemented by added a layer's input vector to its output vector before passing it forward. In the transformer block shown in Fig. 9.18, residual connections are used with both the attention and feedforward sublayers. These summed vectors are then normalized using layer normalization (Ba et al., 2016). If we think of a layer as one long vector of units, the resulting function computed in a transformer block can be expressed as:"}, {"id": "3575", "contributed_by": "group 2", "title": "", "section": "", "text": "Similarly, a simple average of the vectors in a span is unlikely to be an optimal representation of a span since it treats all of a span's embeddings as equally important. For many applications, a more useful representation would be centered around the head of the phrase corresponding to the span. One method for getting at such information in the absence of a syntactic parse is to use a standard self-attention layer to generate a span representation."}, {"id": "1174", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "Another part of text normalization is lemmatization, the task of determining that two words have the same root, despite their surface differences."}, {"id": "1445", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "One commonly used tokenization standard is known as the Penn Treebank tokenization standard, used for the parsed corpora (treebanks) released by the Linguistic Data Consortium (LDC), the source of many useful datasets."}], "metadata": {}, "id": "59"}
{"query": "question: What is pre-training answer: <extra_id_0>", "answers": ["It is to first train a transformer language model on a large corpus of text, and only afterwards add a linear or feedforward layer on top that we finetune on a smaller dataset"], "generation": "Pre-training is the process of learning a model by pre-tuning it with the training data. Pre-training is the process of learning a model by Pre-training Pre-training Pre-training is the process of learning Using", "passages": [{"id": "3128", "contributed_by": "group 2", "title": "", "section": "", "text": "The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence"}, {"id": "2074", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "This can be done by sorting the vocabulary by frequency in the training set, and dening the top 10100 vocabulary entries as stop words, or alternatively by using one of the many predened stop word lists available online.."}, {"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "3593", "contributed_by": "group 2", "title": "", "section": "", "text": "Pretrained language models can be fine-tuned for specific applications by adding lightweight classifier layers on top of the outputs of the pretrained model."}, {"id": "3178", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech tagging as sequence labeling with a simple RNN. Pre-trained word embeddings serve as inputs and a softmax layer provides a probability distribution over the part-of-speech tags as output at each time step."}, {"id": "1474", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "The token learner takes a raw training corpus (sometimes roughly preseparated into words, for example by whitespace) and induces a vocabulary, a set of tokens."}, {"id": "3592", "contributed_by": "group 2", "title": "", "section": "", "text": "Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input."}, {"id": "3555", "contributed_by": "group 2", "title": "", "section": "", "text": "Fine-tuning an application for one of these tasks proceeds just as with pretraining using the NSP objective. During fine-tuning, pairs of labeled sentences from the supervised training data are presented to the model. As with sequence classification, the output vector associated with the prepended [CLS] token represents the model's view of the input pair. And as with NSP training, the two inputs are separated by the a [SEP] token. To perform classification, the [CLS] vector is multiplied by a set of learning classification weights and passed through a softmax to generate label predictions, which are then used to update the weights."}, {"id": "3097", "contributed_by": "group 2", "title": "", "section": "", "text": "The known-word templates are computed for every word seen in the training set; the unknown word features can also be computed for all words in training, or only on training words whose frequency is below some threshold. The result of the known-word templates and word-signature features is a very large set of features. Generally a feature cutoff is used in which features are thrown out if they have count < 5 in the training set."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}], "metadata": {}, "id": "60"}
{"query": "question: What are encoder-decoder or sequence-to-sequence models answer: <extra_id_0>", "answers": ["Neural networks of which the output sequence is a complex function of the entire input sequencer; The output sequence is not merely a direct mapping from individual input words"], "generation": "Encoder-decoder models are used for a different kind of sequence modeling in which the output sequence is a complex function of the entire input sequencer; we must map", "passages": [{"id": "3333", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks, or sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences. Encoder-decoder networks have been applied to a very wide range of applications including machine translation, summarization, question answering, and dialogue."}, {"id": "2778", "contributed_by": "group 10", "title": "", "section": "", "text": "The basic idea behind sequence-to-sequence models is to use two recurrent neural networks (RNNs) together, called an encoder and a decoder, to map an input sequence to an output sequence. The encoder network takes an input sequence and creates a contextualized representation of it, often called the context. This representation is then passed to a decoder which generates a task-specific output sequence. The encoder and decoder are trained jointly to optimize a suitable objective function, such as maximizing the likelihood of the correct output given the input, or minimizing the distance between the correct output and the predicted output."}, {"id": "3303", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder or sequence-to-sequence models are used for a different kind of sequence modeling in which the output sequence is a complex function of the entire input sequencer; we must map from a sequence of input words or tokens to a sequence of tags that are not merely direct mappings from individual words."}, {"id": "3459", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence."}, {"id": "3382", "contributed_by": "group 2", "title": "", "section": "", "text": "The weights Ws, which are then trained during normal end-to-end training, give the network the ability to learn which aspects of similarity between the decoder and encoder states are important to the current application. This bilinear model also allows the encoder and decoder to use different dimensional vectors, whereas the simple dot-product attention requires that the encoder and decoder hidden states have the same dimensionality."}, {"id": "3336", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks consist of three components: 1. An encoder that accepts an input sequence, x1n, and generates a corresponding sequence of contextualized representations, hn1. LSTMs, convolutional networks, and Transformers can all be employed as encoders."}, {"id": "2793", "contributed_by": "group 10", "title": "", "section": "", "text": "In a typical encoder-decoder model, the encoder processes the source sentence and outputs a set of hidden states, which are used by the decoder to generate the target sentence. Both the encoder and decoder are often implemented as recurrent neural networks (RNNs), with the encoder processing the source sentence in a left-to-right or bidirectional manner, and the decoder generating the target sentence one word at a time, conditioned on the source sentence and the previously generated words. Encoder-decoder architectures are trained in a supervised manner with paired source and target strings, using maximum likelihood estimation (MLE) to minimize the cross-entropy loss between the predicted and true target sequences. During training, the decoder is typically forced to generate the true target sequence at each time step, a technique known as teacher forcing."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "2779", "contributed_by": "group 10", "title": "", "section": "", "text": "The Encoder-Decoder architecture is a popular approach for solving sequence-to-sequence tasks in natural language processing with RNNs. It involves an encoder network that takes an input sequence and generates a contextualized representation of it, which is then passed to a decoder that generates a task-specific output sequence. However, while this architecture has been successful for tasks such as machine translation and speech recognition, it has its limitations. The Encoder-Decoder architecture is designed specifically for sequence-to-sequence tasks and cannot be used for other NLP tasks, such as sentiment analysis or named entity recognition. Additionally, it may struggle with longer input sequences or those that contain complex dependencies, as the encoder's output representation must encapsulate all information needed for the decoder to generate the correct output sequence"}, {"id": "3290", "contributed_by": "group 2", "title": "", "section": "", "text": "Common language-based applications for RNNs and transformers include: Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. Auto-regressive generation using a trained language model. Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification."}], "metadata": {}, "id": "61"}
{"query": "question: What are some examples of encoder-decoder applications answer: <extra_id_0>", "answers": ["Machine translation, summarization, dialogue, and semantic parsing"], "generation": "Encoder-decoder networks have been applied to a very wide range of applications including machine translation, summarization, question answering, dialogue, and translation Machine translation", "passages": [{"id": "3459", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence."}, {"id": "3333", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks, or sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences. Encoder-decoder networks have been applied to a very wide range of applications including machine translation, summarization, question answering, and dialogue."}, {"id": "3335", "contributed_by": "group 2", "title": "", "section": "", "text": "The encoder-decoder architecture. The context is a function of the hidden representations of the input, and may be used by the decoder in a variety of ways."}, {"id": "3336", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks consist of three components: 1. An encoder that accepts an input sequence, x1n, and generates a corresponding sequence of contextualized representations, hn1. LSTMs, convolutional networks, and Transformers can all be employed as encoders."}, {"id": "3460", "contributed_by": "group 2", "title": "", "section": "", "text": "The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder."}, {"id": "3382", "contributed_by": "group 2", "title": "", "section": "", "text": "The weights Ws, which are then trained during normal end-to-end training, give the network the ability to learn which aspects of similarity between the decoder and encoder states are important to the current application. This bilinear model also allows the encoder and decoder to use different dimensional vectors, whereas the simple dot-product attention requires that the encoder and decoder hidden states have the same dimensionality."}, {"id": "3374", "contributed_by": "group 2", "title": "", "section": "", "text": "The simplest such score, called dot-product attention, implements relevance as similarity: measuring how similar the decoder hidden state is to an encoder hidden state, by computing the dot product between them:"}, {"id": "2778", "contributed_by": "group 10", "title": "", "section": "", "text": "The basic idea behind sequence-to-sequence models is to use two recurrent neural networks (RNNs) together, called an encoder and a decoder, to map an input sequence to an output sequence. The encoder network takes an input sequence and creates a contextualized representation of it, often called the context. This representation is then passed to a decoder which generates a task-specific output sequence. The encoder and decoder are trained jointly to optimize a suitable objective function, such as maximizing the likelihood of the correct output given the input, or minimizing the distance between the correct output and the predicted output."}, {"id": "3350", "contributed_by": "group 2", "title": "", "section": "", "text": "A more formal version of translating a sentence at inference time in the basic RNN-based encoder-decoder architecture. The final hidden state of the encoder RNN, hen, serves as the context for the decoder in its role as hd0 in the decoder RNN."}, {"id": "2780", "contributed_by": "group 10", "title": "", "section": "", "text": "In the encoder-decoder architecture, the final hidden state of the encoder RNN is used to initialize the hidden state of the decoder RNN. The encoder RNN processes the input sequence, and the decoder RNN generates the output sequence. The final hidden state of the system is acting as a bottleneck: it represents absolutely everything about the meaning of the source text, since the only thing the decoder knows about the source text is what is in this context vector. Thus, the decoder must generate the output sequence conditioned only on this context vector."}], "metadata": {}, "id": "62"}
{"query": "question: What is linguistic typology answer: <extra_id_0>", "answers": ["The study of the systematic similarities and differences across languages"], "generation": "Linguistic typology is the study of these systematic cross-linguistic similarities and differences is called linguistic typology. The study of these systematic cross-linguistic similarities and differences Linguistic typology Linguistic typology Linguistic typology", "passages": [{"id": "3458", "contributed_by": "group 2", "title": "", "section": "", "text": "The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects."}, {"id": "3313", "contributed_by": "group 2", "title": "", "section": "", "text": "Yet languages also differ in many ways, and an understanding of what causes such translation divergences will help us build better MT models. We often distinguish the idiosyncratic and lexical differences that must be dealt with one by one (the word for 'dog' differs wildly from language to language), from systematic differences that we can model in a general way (many languages put the verb before the direct object; others put the verb after the direct object). The study of these systematic cross-linguistic similarities and differences is called linguistic typology. This section sketches some typological facts that impact machine translation; the interested reader should also look into WALS, the World Atlas of Language Structures, which gives many typological facts about languages (Dryer and Haspelmath, 2013)."}, {"id": "3479", "contributed_by": "group 2", "title": "", "section": "", "text": "The early history of MT is surveyed in Hutchins 1986 and 1997; Nirenburg et al. (2002) collects early readings. See Croft (1990) or Comrie (1989) for introductions to linguistic typology."}, {"id": "2820", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "Another way word senses can be related is taxonomically. A word (or sense) is a hyponym of another word or sense if the first is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, hypernym and mango is a hyponym of fruit. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. It is unfortunate that the two words (hypernym and hyponym) are very similar and hence easily confused; for this reason, the word superordinate is often used instead of hypernym. We can define hypernymy more formally by saying that the class denoted by the superordinate extensionally includes the class denoted by the hyponym. Thus, the class of animals includes as members all dogs, and the class of moving actions includes all walking actions. Hypernymy can also be defined in terms of entailment. Under this definition, a sense A is a hyponym of a sense B if everything that is A is also B, and hence being an A entails being a B, or x A(x)  B(x). Hyponymy/hypernymy is usually a transitive relation; if A is a hyponym of B and B is a hyponym of C, then A is a hyponym of C. Another name for the hypernym/hyponym structure is the IS-A hierarchy, in which we say A IS-A B, or B subsumes A."}, {"id": "3328", "contributed_by": "group 2", "title": "", "section": "", "text": "Morphologically, languages are often characterized along two dimensions of variation. The first is the number of morphemes per word, ranging from isolating languages like Vietnamese and Cantonese, in which each word generally has one morpheme, to polysynthetic languages like Siberian Yupik ('Eskimo'), in which a single word may have very many morphemes, corresponding to a whole sentence in English. The second dimension is the degree to which morphemes are segmentable, ranging from agglutinative languages like Turkish, in which morphemes have relatively clean boundaries, to fusion languages like Russian, in which a single affix may conflate multiple morphemes, like -om in the word stolom (table-SG-INSTR- DECL1), which fuses the distinct morphological categories instrumental, singular, and first declension."}, {"id": "3465", "contributed_by": "group 2", "title": "", "section": "", "text": "Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts."}, {"id": "2879", "contributed_by": "group 4", "title": "Coreference Resolution", "section": "", "text": "A number of corpora mark richer discourse phenomena. The ISNotes corpus annotates a portion of OntoNotes for information status, include bridging examples (Hou et al., 2018). The LitBank coreference corpus (Bamman et al., 2020) contains coreference annotations for 210,532 tokens from 100 different literary novels, including singletons and quantified and negated noun phrases. The AnCora-CO coreference corpus (Recasens and Mart, 2010) contains 400,000 words each of Spanish (AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for complex phenomena like discourse deixis in both languages. The ARRAU corpus (Uryupina et al., 2020) contains 350,000 words of English marking all NPs, which means singleton clusters are available. ARRAU includes diverse genres like dialog (the TRAINS data) and fiction (the Pear Stories), and has labels for bridging references, discourse deixis, generics, and ambiguous anaphoric relations."}, {"id": "3124", "contributed_by": "group 2", "title": "", "section": "", "text": "Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags."}, {"id": "2112", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "A common solution here, rather than using all the words as individual features, is to predene likely sets of words or phrases as features, combined with features that are not purely linguistic.."}, {"id": "86", "contributed_by": "group 11", "title": "Chapter 12 - Constituency Grammars", "section": "", "text": "The non-terminal associated with each word in the lexicon is its lexical category, or part of speech."}], "metadata": {}, "id": "63"}
{"query": "question: What are SVO and SOV languages answer: <extra_id_0>", "answers": ["SVO stands for Subject-Verb-Object languages such as English, and SOV stands for Subject-Object-Verb languages such as Japanese"], "generation": "SVO languages are languages that have a basic word order type that tends to come between the subject and object. SOV languages", "passages": [{"id": "3315", "contributed_by": "group 2", "title": "", "section": "", "text": "differ in the basic word order of verbs, subjects, and objects in simple declarative clauses. German, French, English, and Mandarin, for example, are all SVO (Subject-Verb-Object) languages, meaning that the verb tends to come between the subject and object. Hindi and Japanese, by contrast, are SOV languages, meaning that the verb tends to come at the end of basic clauses, and Irish and Arabic are VSO languages. Two languages that share their basic word order type often have other similarities. For example, VO languages generally have prepositions, whereas OV languages generally have postpositions."}, {"id": "3318", "contributed_by": "group 2", "title": "", "section": "", "text": "Other kinds of ordering preferences vary idiosyncratically from language to language. In some SVO languages (like English and Mandarin) adjectives tend to appear before verbs, while in others languages like Spanish and Modern Hebrew, adjectives appear after the noun:"}, {"id": "3458", "contributed_by": "group 2", "title": "", "section": "", "text": "The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects."}, {"id": "353", "contributed_by": "group 11", "title": "Chapter 12 - Constituency Grammars", "section": "", "text": "Simple subcategories are transitive and intransitive; most grammars include many more categories than these."}, {"id": "3327", "contributed_by": "group 2", "title": "", "section": "", "text": "Verb-framed languages mark the direction of motion on the verb (leaving the satellites to mark the manner of motion), like Spanish acercarse 'approach', alcanzar 'reach', entrar 'enter', salir 'exit'. Satellite-framed languages mark the direction of motion on the satellite (leaving the verb to mark the manner of motion), like English crawl out, float off, jump down, run after. Languages like Japanese, Tamil, and the many languages in the Romance, Semitic, and Mayan languages families, are verb-framed; Chinese as well as non-Romance Indo-European languages like English, Swedish, Russian, Hindi, and Farsi are (Talmy 1991, Slobin 1996)."}, {"id": "3124", "contributed_by": "group 2", "title": "", "section": "", "text": "Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags."}, {"id": "3328", "contributed_by": "group 2", "title": "", "section": "", "text": "Morphologically, languages are often characterized along two dimensions of variation. The first is the number of morphemes per word, ranging from isolating languages like Vietnamese and Cantonese, in which each word generally has one morpheme, to polysynthetic languages like Siberian Yupik ('Eskimo'), in which a single word may have very many morphemes, corresponding to a whole sentence in English. The second dimension is the degree to which morphemes are segmentable, ranging from agglutinative languages like Turkish, in which morphemes have relatively clean boundaries, to fusion languages like Russian, in which a single affix may conflate multiple morphemes, like -om in the word stolom (table-SG-INSTR- DECL1), which fuses the distinct morphological categories instrumental, singular, and first declension."}, {"id": "3290", "contributed_by": "group 2", "title": "", "section": "", "text": "Common language-based applications for RNNs and transformers include: Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. Auto-regressive generation using a trained language model. Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification."}, {"id": "3457", "contributed_by": "group 2", "title": "", "section": "", "text": "Languages have divergences, both structural and lexical, that make translation difficult."}, {"id": "3316", "contributed_by": "group 2", "title": "", "section": "", "text": "Let's look in more detail at the example we saw above. In this SVO English"}], "metadata": {}, "id": "64"}
{"query": "question: What are the components of an encoder-decoder network answer: <extra_id_0>", "answers": ["The Encoder, the context vector, and the decoder"], "generation": "Encoder-decoder networks consist of three components: 1. An encoder that accepts an input sequence, x1n, and generates 2. : :", "passages": [{"id": "3336", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks consist of three components: 1. An encoder that accepts an input sequence, x1n, and generates a corresponding sequence of contextualized representations, hn1. LSTMs, convolutional networks, and Transformers can all be employed as encoders."}, {"id": "3459", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence."}, {"id": "3333", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks, or sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences. Encoder-decoder networks have been applied to a very wide range of applications including machine translation, summarization, question answering, and dialogue."}, {"id": "3382", "contributed_by": "group 2", "title": "", "section": "", "text": "The weights Ws, which are then trained during normal end-to-end training, give the network the ability to learn which aspects of similarity between the decoder and encoder states are important to the current application. This bilinear model also allows the encoder and decoder to use different dimensional vectors, whereas the simple dot-product attention requires that the encoder and decoder hidden states have the same dimensionality."}, {"id": "2778", "contributed_by": "group 10", "title": "", "section": "", "text": "The basic idea behind sequence-to-sequence models is to use two recurrent neural networks (RNNs) together, called an encoder and a decoder, to map an input sequence to an output sequence. The encoder network takes an input sequence and creates a contextualized representation of it, often called the context. This representation is then passed to a decoder which generates a task-specific output sequence. The encoder and decoder are trained jointly to optimize a suitable objective function, such as maximizing the likelihood of the correct output given the input, or minimizing the distance between the correct output and the predicted output."}, {"id": "3460", "contributed_by": "group 2", "title": "", "section": "", "text": "The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder."}, {"id": "3334", "contributed_by": "group 2", "title": "", "section": "", "text": "The key idea underlying these networks is the use of an encoder network that takes an input sequence and creates a contextualized representation of it, often called the context. This representation is then passed to a decoder which generates a taskspecific output sequence. Fig. 10.3 illustrates the architecture"}, {"id": "2780", "contributed_by": "group 10", "title": "", "section": "", "text": "In the encoder-decoder architecture, the final hidden state of the encoder RNN is used to initialize the hidden state of the decoder RNN. The encoder RNN processes the input sequence, and the decoder RNN generates the output sequence. The final hidden state of the system is acting as a bottleneck: it represents absolutely everything about the meaning of the source text, since the only thing the decoder knows about the source text is what is in this context vector. Thus, the decoder must generate the output sequence conditioned only on this context vector."}, {"id": "3335", "contributed_by": "group 2", "title": "", "section": "", "text": "The encoder-decoder architecture. The context is a function of the hidden representations of the input, and may be used by the decoder in a variety of ways."}, {"id": "3379", "contributed_by": "group 2", "title": "", "section": "", "text": "A sketch of the encoder-decoder network with attention, focusing on the computation of ci. The context value ci is one of the inputs to the computation of hdi . It is computed by taking the weighted sum of all the encoder hidden states, each weighted by their dot product with the prior decoder hidden state hdi-1."}], "metadata": {}, "id": "65"}
{"query": "question: What is context in an Encoder-Decoder architecture answer: <extra_id_0>", "answers": ["The entire purpose of the encoder is to generate a contextualized representation of the input. Such representation is called the context, and is embodied in the final hidden state of the encoder. The context is then passed to the decoder"], "generation": "The context is a function of the hidden representations of the input, and may be used by the decoder in a variety of ways. The context C C c :", "passages": [{"id": "3335", "contributed_by": "group 2", "title": "", "section": "", "text": "The encoder-decoder architecture. The context is a function of the hidden representations of the input, and may be used by the decoder in a variety of ways."}, {"id": "3459", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence."}, {"id": "2780", "contributed_by": "group 10", "title": "", "section": "", "text": "In the encoder-decoder architecture, the final hidden state of the encoder RNN is used to initialize the hidden state of the decoder RNN. The encoder RNN processes the input sequence, and the decoder RNN generates the output sequence. The final hidden state of the system is acting as a bottleneck: it represents absolutely everything about the meaning of the source text, since the only thing the decoder knows about the source text is what is in this context vector. Thus, the decoder must generate the output sequence conditioned only on this context vector."}, {"id": "3350", "contributed_by": "group 2", "title": "", "section": "", "text": "A more formal version of translating a sentence at inference time in the basic RNN-based encoder-decoder architecture. The final hidden state of the encoder RNN, hen, serves as the context for the decoder in its role as hd0 in the decoder RNN."}, {"id": "3351", "contributed_by": "group 2", "title": "", "section": "", "text": "The entire purpose of the encoder is to generate a contextualized representation of the input. This representation is embodied in the final hidden state of the encoder, hen. This representation, also called c for context, is then passed to the decoder."}, {"id": "3336", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks consist of three components: 1. An encoder that accepts an input sequence, x1n, and generates a corresponding sequence of contextualized representations, hn1. LSTMs, convolutional networks, and Transformers can all be employed as encoders."}, {"id": "3354", "contributed_by": "group 2", "title": "", "section": "", "text": "Allowing every hidden state of the decoder (not just the first decoder state) to be influenced by the context c produced by the encoder."}, {"id": "3379", "contributed_by": "group 2", "title": "", "section": "", "text": "A sketch of the encoder-decoder network with attention, focusing on the computation of ci. The context value ci is one of the inputs to the computation of hdi . It is computed by taking the weighted sum of all the encoder hidden states, each weighted by their dot product with the prior decoder hidden state hdi-1."}, {"id": "3334", "contributed_by": "group 2", "title": "", "section": "", "text": "The key idea underlying these networks is the use of an encoder network that takes an input sequence and creates a contextualized representation of it, often called the context. This representation is then passed to a decoder which generates a taskspecific output sequence. Fig. 10.3 illustrates the architecture"}, {"id": "2790", "contributed_by": "group 10", "title": "", "section": "", "text": "The first attention mechanism we describe is dot-product attention. In this form of attention, the relevance of each encoder state to the current step of the decoder is measured using dot-product attention, which computes the dot product between the decoder hidden state and each encoder hidden state. The dot product is then normalized across the encoder hidden states, and these normalized scores are used as attention weights to compute a weighted sum of the encoder hidden states. The resulting context vector is a weighted sum of the encoder hidden states, where the weights are determined by the similarity between the decoder hidden state and each encoder hidden state."}], "metadata": {}, "id": "66"}
{"query": "question: What is the attention mechanism answer: <extra_id_0>", "answers": ["The context is a fixed-length vector that is a weighted sum of all the encoder hidden states. The weights attend to a particular part of the source text that is relevant for the token the decoder is currently producing. Attention thus replaces the static context vector with one that is dynamically derived from the encoder hidden states, different for each token in decoding"], "generation": "The attention mechanism allows each hidden state of the decoder to see information from all the hidden states of the encoder, not just the last hidden state. The attention mechanism Attention", "passages": [{"id": "3366", "contributed_by": "group 2", "title": "", "section": "", "text": "The attention mechanism is a solution to the bottleneck problem, a way of allowing the decoder to get information from all the hidden states of the encoder, not just the last hidden state."}, {"id": "3372", "contributed_by": "group 2", "title": "", "section": "", "text": "The attention mechanism allows each hidden state of the decoder to see a different, dynamic, context, which is a function of all the encoder hidden states."}, {"id": "3370", "contributed_by": "group 2", "title": "", "section": "", "text": "The idea of attention is instead to create the single fixed-length vector c by taking a weighted sum of all the encoder hidden states. The weights focus on ('attend to') a particular part of the source text that is relevant for the token the decoder is currently producing. Attention thus replaces the static context vector with one that is dynamically derived from the encoder hidden states, different for each token in decoding."}, {"id": "3493", "contributed_by": "group 2", "title": "", "section": "", "text": "Information flow in a bidirectional self-attention model. In processing each element of the sequence, the model attends to all inputs, both before and after the current one."}, {"id": "3460", "contributed_by": "group 2", "title": "", "section": "", "text": "The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder."}, {"id": "3243", "contributed_by": "group 2", "title": "", "section": "", "text": "And finally, as a value used to compute the output for the current focus of attention."}, {"id": "2790", "contributed_by": "group 10", "title": "", "section": "", "text": "The first attention mechanism we describe is dot-product attention. In this form of attention, the relevance of each encoder state to the current step of the decoder is measured using dot-product attention, which computes the dot product between the decoder hidden state and each encoder hidden state. The dot product is then normalized across the encoder hidden states, and these normalized scores are used as attention weights to compute a weighted sum of the encoder hidden states. The resulting context vector is a weighted sum of the encoder hidden states, where the weights are determined by the similarity between the decoder hidden state and each encoder hidden state."}, {"id": "3233", "contributed_by": "group 2", "title": "", "section": "", "text": "Information flow in a causal (or masked) self-attention model. In processing each element of the sequence, the model attends to all the inputs up to, and including, the current one. Unlike RNNs, the computations at each time step are independent of all the other steps and therefore can be performed in parallel."}, {"id": "3367", "contributed_by": "group 2", "title": "", "section": "", "text": "In the attention mechanism, as in the vanilla encoder-decoder model, the context vector c is a single vector that is a function of the hidden states of the encoder, that is, c = f (he1 . . . hen ). Because the number of hidden states varies with the size of the"}, {"id": "3374", "contributed_by": "group 2", "title": "", "section": "", "text": "The simplest such score, called dot-product attention, implements relevance as similarity: measuring how similar the decoder hidden state is to an encoder hidden state, by computing the dot product between them:"}], "metadata": {}, "id": "67"}
{"query": "question: What is the problem with greedy decoding answer: <extra_id_0>", "answers": ["Greedy decoding always makes a choice that is locally optimal, but the token that looks good to the decoder now might turn out later to have been the wrong choice"], "generation": "Certainly, greedy search is not optimal, and may not find the highest probability translation. The problem is that the token that looks good to the decoder Certainly,", "passages": [{"id": "3461", "contributed_by": "group 2", "title": "", "section": "", "text": "For the decoder, choosing the single most probable token to generate at each step is called greedy decoding."}, {"id": "3384", "contributed_by": "group 2", "title": "", "section": "", "text": "Choosing the single most probable token to generate at each step is called greedy decoding; a greedy algorithm is one that make a choice that is locally optimal, whether or not it will turn out to have been the best choice with hindsight."}, {"id": "3385", "contributed_by": "group 2", "title": "", "section": "", "text": "Indeed, greedy search is not optimal, and may not find the highest probability translation. The problem is that the token that looks good to the decoder now might turn out later to have been the wrong choice!"}, {"id": "2780", "contributed_by": "group 10", "title": "", "section": "", "text": "In the encoder-decoder architecture, the final hidden state of the encoder RNN is used to initialize the hidden state of the decoder RNN. The encoder RNN processes the input sequence, and the decoder RNN generates the output sequence. The final hidden state of the system is acting as a bottleneck: it represents absolutely everything about the meaning of the source text, since the only thing the decoder knows about the source text is what is in this context vector. Thus, the decoder must generate the output sequence conditioned only on this context vector."}, {"id": "3366", "contributed_by": "group 2", "title": "", "section": "", "text": "The attention mechanism is a solution to the bottleneck problem, a way of allowing the decoder to get information from all the hidden states of the encoder, not just the last hidden state."}, {"id": "2784", "contributed_by": "group 10", "title": "", "section": "", "text": "A greedy algorithm is one that makes a locally optimal choice at each step of the algorithm, without regard for how that choice might affect future decisions. In other words, the algorithm makes the best decision it can at each step based only on the information available to it at that time, without any consideration of how that decision might affect future steps or the overall outcome of the algorithm. Greedy algorithms are often used in optimization problems where finding the globally optimal solution is too computationally expensive, but finding a good solution quickly is still desirable."}, {"id": "3370", "contributed_by": "group 2", "title": "", "section": "", "text": "The idea of attention is instead to create the single fixed-length vector c by taking a weighted sum of all the encoder hidden states. The weights focus on ('attend to') a particular part of the source text that is relevant for the token the decoder is currently producing. Attention thus replaces the static context vector with one that is dynamically derived from the encoder hidden states, different for each token in decoding."}, {"id": "2793", "contributed_by": "group 10", "title": "", "section": "", "text": "In a typical encoder-decoder model, the encoder processes the source sentence and outputs a set of hidden states, which are used by the decoder to generate the target sentence. Both the encoder and decoder are often implemented as recurrent neural networks (RNNs), with the encoder processing the source sentence in a left-to-right or bidirectional manner, and the decoder generating the target sentence one word at a time, conditioned on the source sentence and the previously generated words. Encoder-decoder architectures are trained in a supervised manner with paired source and target strings, using maximum likelihood estimation (MLE) to minimize the cross-entropy loss between the predicted and true target sequences. During training, the decoder is typically forced to generate the true target sequence at each time step, a technique known as teacher forcing."}, {"id": "3460", "contributed_by": "group 2", "title": "", "section": "", "text": "The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder."}, {"id": "3387", "contributed_by": "group 2", "title": "", "section": "", "text": "A search tree for generating the target string T = t1 , t2 , ... from the vocabulary V = {yes, ok, <s>}, given the source string, showing the probability of generating each token from that state. Greedy search would choose yes at the first time step followed by yes, instead of the globally most probable sequence ok ok."}], "metadata": {}, "id": "68"}
{"query": "question: How does beam search work answer: <extra_id_0>", "answers": ["Instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step, with k being the fixed-size memory footprint also called the beam width"], "generation": "Beam search uses a breadth-first search strategy with a heuristic filter that prunes the search frontier to stay within Beam search", "passages": [{"id": "3462", "contributed_by": "group 2", "title": "", "section": "", "text": "In beam search,instead of choosing the best token to generate at each time step,"}, {"id": "3393", "contributed_by": "group 2", "title": "", "section": "", "text": "This process continues until a </s> is generated indicating that a complete candidate output has been found. At this point, the completed hypothesis is removed from the frontier and the size of the beam is reduced by one. The search continues until the beam has been reduced to 0. The result will be k hypotheses."}, {"id": "3397", "contributed_by": "group 2", "title": "", "section": "", "text": "One problem arises from the fact that the completed hypotheses may have different lengths. Because models generally assign lower probabilities to longer strings, a naive algorithm would also choose shorter strings for y. This was not an issue during the earlier steps of decoding; due to the breadth-first nature of beam search all the hypotheses being compared had the same length. The usual solution to this is"}, {"id": "3476", "contributed_by": "group 2", "title": "", "section": "", "text": "Beam-search has an interesting relationship with human language processing; (Meister et al., 2020) show that beam search enforces the cognitive property of uniform information density in text. Uniform information density is the hypothesis that human language processors tend to prefer to distribute information equally across the sentence (Jaeger and Levy, 2007)."}, {"id": "3396", "contributed_by": "group 2", "title": "", "section": "", "text": "Scoring for beam search decoding with a beam width of k = 2. We maintain the log probability of each hypothesis in the beam by incrementally adding the logprob of generating each next token. Only the top k paths are extended to the next step."}, {"id": "3310", "contributed_by": "group 2", "title": "", "section": "", "text": "We'll introduce the algorithm in sections Section 10.2, and in following sections give important components of the model like beam search decoding, and we'll discuss how MT is evaluated, introducing the simple chrF metric."}, {"id": "3389", "contributed_by": "group 2", "title": "", "section": "", "text": "Instead, decoding in MT and other sequence generation problems generally uses a method called beam search. In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width, on the metaphor of a flashlight beam that can be parameterized to be wider or narrower."}, {"id": "2671", "contributed_by": "group 8", "title": "", "section": "", "text": "The computational efficiency of the transition-based approach discussed earlier de-rives from the fact that it makes a single pass through the sentence, greedily making decisions without considering alternatives. Of course, this is also a weakness  once a decision has been made it can not be undone, even in the face of overwhelming evidence arriving later in a sentence. We can use beam search to explore alternative decision sequences. Recall from Chapter 10 that beam search uses a breadth-first search strategy with a heuristic filter that prunes the search frontier to stay within a fixed-size beam width."}, {"id": "3398", "contributed_by": "group 2", "title": "", "section": "", "text": "Beam search is common in large production MT systems, generally with beam widths k between 5 and 10. What do we do with the resulting k hypotheses? In some cases, all we need from our MT algorithm is the single best hypothesis, so we can return that. In other cases our downstream application might want to look at all k hypotheses, so we can pass them all (or a subset) to the downstream application with their respective scores."}, {"id": "2835", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "This chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights:  A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.  Many words are polysemous, having many senses.  Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.  WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.  Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.  The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.  Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.  An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.  Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target words neighborhood.  Word sense induction is the task of learning word senses unsupervised."}], "metadata": {}, "id": "69"}
{"query": "question: Why do source and target languages share the same vocabulary answer: <extra_id_0>", "answers": ["It makes it easy to copy tokens (like names) from source to target"], "generation": "Generally a shared vocabulary makes it easy to copy tokens (like names) from source to target, which makes it easy to copy tokens", "passages": [{"id": "3406", "contributed_by": "group 2", "title": "", "section": "", "text": "Machine translation systems generally use a fixed vocabulary, A common way to generate this vocabulary is with the BPE or wordpiece algorithms sketched in Chapter 2. Generally a shared vocabulary is used for the source and target languages, which makes it easy to copy tokens (like names) from source to target, so we build the wordpiece/BPE lexicon on a corpus that contains both source and target language data. Wordpieces use a special symbol at the beginning of each token; here's a resulting tokenization from the Google MT system (Wu et al., 2016):"}, {"id": "2835", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "This chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights:  A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.  Many words are polysemous, having many senses.  Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.  WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.  Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.  The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.  Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.  An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.  Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target words neighborhood.  Word sense induction is the task of learning word senses unsupervised."}, {"id": "3466", "contributed_by": "group 2", "title": "", "section": "", "text": "MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used."}, {"id": "53", "contributed_by": "group 9", "title": "", "section": "", "text": "The role of context is also important in the similarity of a less biological kind of organism: the word. Words that occur in similar contexts tend to have similar meanings. This link between similarity in how words are distributed and similarity in what they mean is called the distributional hypothesis. The hypothesis was first formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth (1957), who noticed that words which are synonyms (like oculist and eye-doctor) tended to occur in the same environment (e.g., near words like eye or examined) with the amount of meaning difference between two words corresponding roughly to the amount of difference in their environments (Harris, 1954, 157)."}, {"id": "3315", "contributed_by": "group 2", "title": "", "section": "", "text": "differ in the basic word order of verbs, subjects, and objects in simple declarative clauses. German, French, English, and Mandarin, for example, are all SVO (Subject-Verb-Object) languages, meaning that the verb tends to come between the subject and object. Hindi and Japanese, by contrast, are SOV languages, meaning that the verb tends to come at the end of basic clauses, and Irish and Arabic are VSO languages. Two languages that share their basic word order type often have other similarities. For example, VO languages generally have prepositions, whereas OV languages generally have postpositions."}, {"id": "2795", "contributed_by": "group 10", "title": "", "section": "", "text": "Translation is a complex task that involves more than just swapping words from one language to another. Languages have divergences, both structural and lexical, that make translation difficult. Structural divergences refer to differences in the ways languages organize information, such as word order, syntax, and grammatical structures. Lexical divergences, on the other hand, are differences in vocabulary and expressions used in different languages. Some words and phrases have no direct equivalents in other languages, making it challenging to convey the same meaning in the target language. These divergences are further complicated by the fact that language is constantly evolving, with new words and expressions being added all the time. As a result, machine translation systems face a significant challenge in capturing these nuances and producing accurate translations"}, {"id": "3415", "contributed_by": "group 2", "title": "", "section": "", "text": "a cost function that takes a span of source sentences and a span of target sentences and returns a score measuring how likely these spans are to be translations."}, {"id": "2781", "contributed_by": "group 10", "title": "", "section": "", "text": "In order to produce sentence alignments between two documents that are translations of each other, we need a cost function and an alignment algorithm. One intuitive cost function measures the length of the jumps between source and target sentence pairs in a parallel corpus, and tries to minimize the total jump distance by pairing up the most similar source and target sentences. An example of this cost function is the IBM Model 2 alignment model, which is an extension of Model 1 that allows for non-one-to-one sentence alignments."}, {"id": "3417", "contributed_by": "group 2", "title": "", "section": "", "text": "Since it is possible to induce multilingual sentence embeddings (Artetxe and Schwenk, 2019), cosine similarity of such embeddings provides a natural scoring function (Schwenk, 2018). Thompson and Koehn (2019) give the following cost function between two sentences or spans x,y from the source and target documents respectively:"}, {"id": "3465", "contributed_by": "group 2", "title": "", "section": "", "text": "Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts."}], "metadata": {}, "id": "70"}
{"query": "question: What is a bitext answer: <extra_id_0>", "answers": ["It is a parallel corpus, a piece of text that appears in two or more languages, which is used for machine translation training"], "generation": "A bitext is a text that appears in two (or more) languages. It is a parallel corpus that is trained on A A bitext A text that appears in two languages. a", "passages": [{"id": "3465", "contributed_by": "group 2", "title": "", "section": "", "text": "Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts."}, {"id": "3464", "contributed_by": "group 2", "title": "", "section": "", "text": "Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages."}, {"id": "3424", "contributed_by": "group 2", "title": "", "section": "", "text": "In general backtranslation works surprisingly well; one estimate suggests that a system trained on backtranslated text gets about 2/3 of the gain as would training on the same amount of natural bitext (Edunov et al., 2018)."}, {"id": "3128", "contributed_by": "group 2", "title": "", "section": "", "text": "The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence"}, {"id": "2820", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "Another way word senses can be related is taxonomically. A word (or sense) is a hyponym of another word or sense if the first is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, hypernym and mango is a hyponym of fruit. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. It is unfortunate that the two words (hypernym and hyponym) are very similar and hence easily confused; for this reason, the word superordinate is often used instead of hypernym. We can define hypernymy more formally by saying that the class denoted by the superordinate extensionally includes the class denoted by the hyponym. Thus, the class of animals includes as members all dogs, and the class of moving actions includes all walking actions. Hypernymy can also be defined in terms of entailment. Under this definition, a sense A is a hyponym of a sense B if everything that is A is also B, and hence being an A entails being a B, or x A(x)  B(x). Hyponymy/hypernymy is usually a transitive relation; if A is a hyponym of B and B is a hyponym of C, then A is a hyponym of C. Another name for the hypernym/hyponym structure is the IS-A hierarchy, in which we say A IS-A B, or B subsumes A."}, {"id": "3059", "contributed_by": "group 2", "title": "", "section": "", "text": "For part-of-speech tagging, the goal of HMM decoding is to choose the tag sequence t1 . . . tn that is most probable given the observation sequence of n words"}, {"id": "2752", "contributed_by": "group 10", "title": "", "section": "", "text": "hidden Markov model taggers make two further simplifying assumptions. The first is that the probability of a word appearing depends only on its own tag and is independent of neighboring words and tags.The second assumption, the bigram assumption, is that the probability of a tag is dependent only on the previous tag, rather than the entire tag sequence."}, {"id": "3178", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech tagging as sequence labeling with a simple RNN. Pre-trained word embeddings serve as inputs and a softmax layer provides a probability distribution over the part-of-speech tags as output at each time step."}, {"id": "2751", "contributed_by": "group 10", "title": "", "section": "", "text": "HMM taggers make two further simplifying assumptions. The first is that the probability of a word appearing depends only on its own tag and is independent of neighboring words and tags.The second assumption, the bigram assumption, is that the probability of a tag is dependent only on the previous tag, rather than the entire tag sequence."}, {"id": "2769", "contributed_by": "group 10", "title": "", "section": "", "text": "Sequence labeling is a common task in natural language processing that involves assigning labels to each element of a sequence. In an RNN approach to sequence labeling, inputs are typically word embeddings that represent the input sequence, and the outputs are tag probabilities generated by a softmax layer over the given tagset. The RNN processes the input sequence one token at a time, and the hidden state at each time step is updated based on the current input and the previous hidden state. The output at each time step is a probability distribution over the possible tags, and the tag with the highest probability is selected as the prediction. This approach has been used in a variety of sequence labeling tasks, such as named entity recognition and part-of-speech tagging, and has been shown to be effective in capturing the contextual dependencies between the input tokens."}], "metadata": {}, "id": "71"}
{"query": "question: When is sentence alignment necessary answer: <extra_id_0>", "answers": ["Standard training corpora for machine translation come as aligned pairs of sentences. When creating new corpora, for example for under-resourced languages or new domains, these sentence alignments must be created"], "generation": "Generally, sentence alignment is necessary when creating new corpora for machine translation.", "passages": [{"id": "3413", "contributed_by": "group 2", "title": "", "section": "", "text": "A sample alignment between sentences in English and French, with sentences extracted from Antoine de Saint-Exupery's Le Petit Prince and a hypothetical translation. Sentence alignment takes sentences e1,...,en, and f1,..., fn and finds minimal sets of sentences that are translations of each other, including single sentence mappings like (e1 ,f1 ), (e4 ,f3 ), (e5 ,f4 ), (e6 ,f6 ) as well as 2-1 alignments (e2 /e3 ,f2 ), (e7 /e8 ,f7 ), and null alignments (f5)."}, {"id": "1580", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other."}, {"id": "3412", "contributed_by": "group 2", "title": "", "section": "", "text": "Standard training corpora for MT come as aligned pairs of sentences. When creating new corpora, for example for underresourced languages or new domains, these sentence alignments must be created. Fig. 10.17 gives a sample hypothetical sentence alignment."}, {"id": "1538", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "Given two sequences, an alignment is a correspondence between substrings of the two sequences."}, {"id": "2781", "contributed_by": "group 10", "title": "", "section": "", "text": "In order to produce sentence alignments between two documents that are translations of each other, we need a cost function and an alignment algorithm. One intuitive cost function measures the length of the jumps between source and target sentence pairs in a parallel corpus, and tries to minimize the total jump distance by pairing up the most similar source and target sentences. An example of this cost function is the IBM Model 2 alignment model, which is an extension of Model 1 that allows for non-one-to-one sentence alignments."}, {"id": "3418", "contributed_by": "group 2", "title": "", "section": "", "text": "where nSents() gives the number of sentences (this biases the metric toward many alignments of single sentences instead of aligning very large spans). The denominator helps to normalize the similarities, and so x1,...,xS,y1,...,yS, are randomly selected sentences sampled from the respective documents."}, {"id": "3414", "contributed_by": "group 2", "title": "", "section": "", "text": "Given two documents that are translations of each other, we generally need two steps to produce sentence alignments:"}, {"id": "1524", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "The period character . is ambiguous between a sentence boundary marker and a marker of abbreviations like Mr. or Inc."}, {"id": "1576", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "Aligning two strings is useful throughout speech and language processing."}, {"id": "1604", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "word segmentation and normalization, sentence segmentation, and stemming."}], "metadata": {}, "id": "72"}
{"query": "question: When is back-translation necessary answer: <extra_id_0>", "answers": ["When we are short of data for training MT models, since parallel corpora may be limited for particular languages or domains"], "generation": "Back-translation works well when there is a large parallel corpus available for the target language. In such cases, the model can be trained to translate in both directions When Usually", "passages": [{"id": "2791", "contributed_by": "group 10", "title": "", "section": "", "text": "Back-translation works well when there is a large parallel corpus available for the target language. In such cases, the model can be trained to translate in both directions, which leads to better generalization. However, back-translation is not always feasible, as it requires a large parallel corpus for the target language."}, {"id": "3465", "contributed_by": "group 2", "title": "", "section": "", "text": "Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts."}, {"id": "3415", "contributed_by": "group 2", "title": "", "section": "", "text": "a cost function that takes a span of source sentences and a span of target sentences and returns a score measuring how likely these spans are to be translations."}, {"id": "1891", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "The probability for a backoff n-gram PBO is thus computed as follows: if C(wnN+1:n) > 0 Katz backoff is often combined with a smoothing method called Good-Turing.."}, {"id": "3301", "contributed_by": "group 2", "title": "", "section": "", "text": "Finally, a more recent application of MT is to in-the-moment human communication needs. This includes incremental translation, translating speech on-the-fly before the entire sentence is complete, as is commonly used in simultaneous interpretation. Image-centric translation can be used for example to use OCR of the text on a phone camera image as input to an MT system to translate menus or street signs."}, {"id": "3300", "contributed_by": "group 2", "title": "", "section": "", "text": "Another common use of machine translation is to aid human translators. MT systems are routinely used to produce a draft translation that is fixed up in a post-editing phase by a human translator. This task is often called computer-aided translation CAT or CAT. CAT is commonly used as part of localization: the task of adapting content localization or a product to a particular language community."}, {"id": "3350", "contributed_by": "group 2", "title": "", "section": "", "text": "A more formal version of translating a sentence at inference time in the basic RNN-based encoder-decoder architecture. The final hidden state of the encoder RNN, hen, serves as the context for the decoder in its role as hd0 in the decoder RNN."}, {"id": "3466", "contributed_by": "group 2", "title": "", "section": "", "text": "MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used."}, {"id": "3348", "contributed_by": "group 2", "title": "", "section": "", "text": "Translating a single sentence (inference time) in the basic RNN version of encoder-decoder approach to machine translation. Source and target sentences are concatenated with a separator token in between, and the decoder uses context information from the encoder's last hidden state."}, {"id": "77", "contributed_by": "group 11", "title": "Chapter 12 - Constituency Grammars", "section": "", "text": "Context-free grammars are also called Phrase-Structure Grammars, and the formalism is equivalent to Backus-Naur Form, or BNF ."}], "metadata": {}, "id": "73"}
{"query": "question: How does backtranslation work answer: <extra_id_0>", "answers": ["We first create synthetic bitexts, and train an intermediate target-to-source MT system on the small bitext to translate the monolingual target data to the source language. Now we can add this synthetic bitext to our training data, and retrain our source-to-target MT model"], "generation": "Backtranslation works surprisingly well; one estimate suggests that a system trained on backtranslated data gets about 2/3 the gain as would training on", "passages": [{"id": "3465", "contributed_by": "group 2", "title": "", "section": "", "text": "Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts."}, {"id": "3424", "contributed_by": "group 2", "title": "", "section": "", "text": "In general backtranslation works surprisingly well; one estimate suggests that a system trained on backtranslated text gets about 2/3 of the gain as would training on the same amount of natural bitext (Edunov et al., 2018)."}, {"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "2765", "contributed_by": "group 10", "title": "", "section": "", "text": "Backpropagation Through Time (BPTT) is a general approach for training the weights in RNNs. It involves processing the input sequence in the forward direction to generate the output sequence, and then using the output sequence to compute the loss function. The loss function is then backpropagated through the network in the reverse direction using the chain rule of calculus, allowing the gradients to be computed for all the weights in the network. However, because the RNN is unrolled over time, the gradients can become unstable and either vanish or explode. To address this problem, various modifications have been proposed, such as gradient clipping and using different activation functions. Despite its challenges, BPTT remains a popular method for training RNNs and has been used in many applications, including speech recognition and machine translation."}, {"id": "2764", "contributed_by": "group 10", "title": "", "section": "", "text": "The backpropagation algorithm for training weights in RNNs is a two-pass process. In the first pass, forward inference is performed to generate the output sequence. The output sequence is then compared to the target sequence, and the loss function is computed. In the second pass, the gradients of the loss function with respect to the weights are computed by backpropagating the errors through the network in reverse order. This is known as the backpropagation through time (BPTT) algorithm. However, because the RNN is unrolled over time, the gradients can become unstable and either vanish or explode. To address this problem, various modifications have been proposed, such as gradient clipping and using different activation functions. Despite its challenges, BPTT remains a popular method for training RNNs and has been used in many applications, including speech recognition and machine translation."}, {"id": "2791", "contributed_by": "group 10", "title": "", "section": "", "text": "Back-translation works well when there is a large parallel corpus available for the target language. In such cases, the model can be trained to translate in both directions, which leads to better generalization. However, back-translation is not always feasible, as it requires a large parallel corpus for the target language."}, {"id": "2991", "contributed_by": "group 2", "title": "", "section": "", "text": "Neural networks are trained by optimization algorithms like gradient descent.Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling."}, {"id": "3210", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN for sequence classification. The final hidden units from the forward and backward passes are combined to represent the entire sequence. This combined representation serves as input to the subsequent classifier."}, {"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}, {"id": "3423", "contributed_by": "group 2", "title": "", "section": "", "text": "Backtranslation has various parameters. One is how we generate the backtranslated data; we can run the decoder in greedy inference, or use beam search. Or we can do sampling, or Monte Carlo search. In Monte Carlo decoding, at each timestep, instead of always generating the word with the highest softmax probability, we roll a weighted die, and use it to choose the next word according to its softmax probability. This works just like the sampling algorithm we saw in Chapter 3 for generating random sentences from n-gram language models. Imagine there are only 4 words and the softmax probability distribution at time t is (the: 0.6, green: 0.2, a: 0.1, witch: 0.1). We roll a weighted die, with the 4 sides weighted 0.6, 0.2, 0.1, and 0.1, and chose the word based on which side comes up. Another parameter is the ratio of backtranslated data to natural bitext data; we can choose to upsample the bitext data (include multiple copies of each sentence)."}], "metadata": {}, "id": "74"}
{"query": "question: How are machine translations evaluated answer: <extra_id_0>", "answers": ["Adequacy (how well the translation captures the exact meaning of the source sentence), and fluency (how fluent the translation is in the target language)"], "generation": "MT is evaluated by measuring a translation's adequacy and fluency. The most accurate evaluations use human raters. MT is evaluated MT", "passages": [{"id": "2789", "contributed_by": "group 10", "title": "", "section": "", "text": "When evaluating machine translation output, we need to have some notion of what we are trying to achieve. One common way to think about the task is in terms of two dimensions: fluency and adequacy. Fluency refers to the grammaticality and naturalness of the output. Adequacy refers to the extent to which the output conveys the same meaning as the input. There are several methods for evaluating machine translation systems, each with its own strengths and weaknesses. Human raters, such as online crowdworkers, are considered the most accurate evaluation method for machine translation along the dimensions of fluency and adequacy. However, this method can be time-consuming and expensive, so it is often complemented by automatic evaluation metrics."}, {"id": "3466", "contributed_by": "group 2", "title": "", "section": "", "text": "MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used."}, {"id": "3425", "contributed_by": "group 2", "title": "", "section": "", "text": "Translations are evaluated along two dimensions: 1. adequacy: how well the translation captures the exact meaning of the source sentence. Sometimes called faithfulness or fidelity.  2. fluency: how fluent the translation is in the target language (is it grammatical, clear, readable, natural)."}, {"id": "3477", "contributed_by": "group 2", "title": "", "section": "", "text": "Research on evaluation of machine translation began quite early. Miller and Beebe-Center (1956) proposed a number of methods drawing on work in psycholinguistics. These included the use of cloze and Shannon tasks to measure intelligibility as well as a metric of edit distance from a human translation, the intuition that underlies all modern overlap-based automatic evaluation metrics. The ALPAC report included an early evaluation study conducted by John Carroll that was extremely influential (Pierce et al., 1966, Appendix 10). Carroll proposed distinct measures for fidelity and intelligibility, and had raters score them subjectively on 9-point scales. Much early evaluation work focuses on automatic word-overlap metrics like BLEU"}, {"id": "3427", "contributed_by": "group 2", "title": "", "section": "", "text": "The most accurate evaluations use human raters, such as online crowdworkers, to evaluate each translation along the two dimensions. For example, along the dimension of fluency, we can ask how intelligible, how clear, how readable, or how natural the MT output (the target text) is. We can give the raters a scale, for example, from 1 (totally unintelligible) to 5 (totally intelligible, or 1 to 100, and ask them to rate each sentence or paragraph of the MT output."}, {"id": "3300", "contributed_by": "group 2", "title": "", "section": "", "text": "Another common use of machine translation is to aid human translators. MT systems are routinely used to produce a draft translation that is fixed up in a post-editing phase by a human translator. This task is often called computer-aided translation CAT or CAT. CAT is commonly used as part of localization: the task of adapting content localization or a product to a particular language community."}, {"id": "3430", "contributed_by": "group 2", "title": "", "section": "", "text": "While humans produce the best evaluations of machine translation output, running a human evaluation can be time consuming and expensive. For this reason automatic metrics are often used. Automatic metrics are less accurate than human evaluation, but can help test potential system improvements, and even be used as an automatic loss function for training. In this section we introduce two families of such metrics, those based on characteror word-overlap and those based on embedding similarity."}, {"id": "3415", "contributed_by": "group 2", "title": "", "section": "", "text": "a cost function that takes a span of source sentences and a span of target sentences and returns a score measuring how likely these spans are to be translations."}, {"id": "3348", "contributed_by": "group 2", "title": "", "section": "", "text": "Translating a single sentence (inference time) in the basic RNN version of encoder-decoder approach to machine translation. Source and target sentences are concatenated with a separator token in between, and the decoder uses context information from the encoder's last hidden state."}, {"id": "3464", "contributed_by": "group 2", "title": "", "section": "", "text": "Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages."}], "metadata": {}, "id": "75"}
{"query": "question: What is Character F-Score answer: <extra_id_0>", "answers": ["It is also known as ChrF, a metric that ranks each MT target sentence by a function of the number of character n-gram overlaps with the human translation"], "generation": "Character F-Score is a metric for character classification. It is a measure of recall, precision, and recall. Character F-Score Character", "passages": [{"id": "3435", "contributed_by": "group 2", "title": "", "section": "", "text": "The metric then computes an F-score by combining chrP and chrR using a weighting parameter beta . It is common to set beta = 2, thus weighing recall twice as much as precision:"}, {"id": "2156", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "When  = 1, precision and recall are equally balanced; this is the most frequently used metric, and is called F=1 or just F1: F1 = 2PR F-measure comes from a weighted harmonic mean of precision and recall.."}, {"id": "3439", "contributed_by": "group 2", "title": "", "section": "", "text": "Finally we average to get chrP and chrR, and compute the F-score:"}, {"id": "3381", "contributed_by": "group 2", "title": "", "section": "", "text": "score(hd ,he) = hd W he i-1 j t-1 s j"}, {"id": "2686", "contributed_by": "group 8", "title": "", "section": "", "text": "Constants in FOL refer to specific objects in the world being described. Such constants are conventionally depicted as either single capitalized letters such as A and B or single capitalized words that are often reminiscent of proper nouns such as Maharani and Harry. Like programming language constants, FOL constants refer to exactly one object. Objects can, however, have multiple constants that refer to them."}, {"id": "2157", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "The harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recipa1 + 1 a2 + 1 a3 +...+ 1 and hence F-measure is P +(1) 1 with  2 = 1 F = ( 2 +1)PR TEST SETS AND CROSS-VALIDATION Harmonic mean is used because it is a conservative metric; the harmonic mean of two values is closer to the minimum of the two values than the arithmetic mean is.."}, {"id": "2739", "contributed_by": "group 10", "title": "", "section": "", "text": "Part-of-speech taggers are evaluated by the standard metric of accuracy. Named entity recognizers are evaluated by recall, precision, and F1 measure. Recall that recall is the ratio of the number of correctly labeled responses to the total that should have been labeled; precision is the ratio of the number of correctly labeled responses to the total labeled; and F-measure is the harmonic mean of the two."}, {"id": "3110", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech taggers are evaluated by the standard metric of accuracy. Named entity recognizers are evaluated by recall, precision, and F1 measure. Recall that recall is the ratio of the number of correctly labeled responses to the total that should have been labeled; precision is the ratio of the number of correctly labeled responses to the total labeled; and F-measure is the harmonic mean of the two."}, {"id": "2040", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "The dialogue is great and the adventure scenes are fun...."}, {"id": "3374", "contributed_by": "group 2", "title": "", "section": "", "text": "The simplest such score, called dot-product attention, implements relevance as similarity: measuring how similar the decoder hidden state is to an encoder hidden state, by computing the dot product between them:"}], "metadata": {}, "id": "76"}
{"query": "question: Why is ChrF preferred over BLEU score answer: <extra_id_0>", "answers": ["BLEU is a word-based metric, it is very sensitive to word tokenization, making it difficult to compare across situations, and doesn’t work as well in languages with complex morphology"], "generation": "chrF is simple, robust, and correlates very well with human judgments in many languages.", "passages": [{"id": "3440", "contributed_by": "group 2", "title": "", "section": "", "text": "chrF is simple, robust, and correlates very well with human judgments in many languages (Kocmi et al., 2021). There are various alternative overlap metrics. For example, before the development of chrF, it was common to use a word-based overlap metric called BLEU (for BiLingual Evaluation Understudy), that is purely precision-based rather than combining precision and recall (Papineni et al., 2002). The BLEU score for a corpus of candidate translation sentences is a function of the n-gram word precision over all the sentences combined with a brevity penalty computed over the corpus as a whole. Because BLEU is a word-based metric, it is very sensitive to word tokenization, making it difficult to compare across situations, and doesn't work as well in languages with complex morphology."}, {"id": "3441", "contributed_by": "group 2", "title": "", "section": "", "text": "Character or word overlap-based metrics like chrF (or BLEU, or etc.) are mainly used to compare two systems, with the goal of answering questions like: did the new algorithm we just invented improve our MT system? To know if the difference between the chrF scores of two MT systems is a significant difference, we use the paired bootstrap test, or the similar randomization test."}, {"id": "3443", "contributed_by": "group 2", "title": "", "section": "", "text": "To compare two MT systems A and B, we draw the same set of pseudo-testsets, and compute the chrF scores for each of them. We then compute the percentage of pseudo-test-sets in which A has a higher chrF score than B."}, {"id": "3435", "contributed_by": "group 2", "title": "", "section": "", "text": "The metric then computes an F-score by combining chrP and chrR using a weighting parameter beta . It is common to set beta = 2, thus weighing recall twice as much as precision:"}, {"id": "3432", "contributed_by": "group 2", "title": "", "section": "", "text": "Given the hypothesis and the reference, chrF is given a parameter k indicating the length of character n-grams to be considered, and computes the average of the k precisions (unigram precision, bigram, and so on) and the average of the k recalls (unigram recall, bigram recall, etc.):"}, {"id": "3444", "contributed_by": "group 2", "title": "", "section": "", "text": "While automatic character and word-overlap metrics like chrF or BLEU are useful, they have important limitations. chrF is very local: a large phrase that is moved around might barely change the chrF score at all, and chrF can't evaluate crosssentence properties of a document like its discourse coherence (Chapter 22). chrF and similar automatic metrics also do poorly at comparing very different kinds of systems, such as comparing human-aided translation against machine translation, or different machine translation architectures against each other (Callison-Burch et al., 2006). Instead, automatic overlap metrics like chrF are most appropriate when evaluating changes to a single system."}, {"id": "3431", "contributed_by": "group 2", "title": "", "section": "", "text": "The simplest and most robust metric for MT evaluation is called chrF, which stands for character F-score (Popovic, 2015). chrF (along with many other earlier related metrics like BLEU, METEOR, TER, and others) is based on a simple intuition derived from the pioneering work of Miller and Beebe-Center (1956): a good machine translation will tend to contain characters and words that occur in a human translation of the same sentence. Consider a test set from a parallel corpus, in which each source sentence has both a gold human target translation and a candidate MT translation we'd like to evaluate. The chrF metric ranks each MT target sentence by a function of the number of character n-gram overlaps with the human translation."}, {"id": "3466", "contributed_by": "group 2", "title": "", "section": "", "text": "MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used."}, {"id": "3439", "contributed_by": "group 2", "title": "", "section": "", "text": "Finally we average to get chrP and chrR, and compute the F-score:"}, {"id": "3310", "contributed_by": "group 2", "title": "", "section": "", "text": "We'll introduce the algorithm in sections Section 10.2, and in following sections give important components of the model like beam search decoding, and we'll discuss how MT is evaluated, introducing the simple chrF metric."}], "metadata": {}, "id": "77"}
{"query": "question: What is the limitation of ChrF answer: <extra_id_0>", "answers": ["It is very local, cannot evaluate cross-sentence properties, and perform poorly at comparing very different kinds of systems"], "generation": "ChrF is very local: a large phrase that is moved around might barely change the chrF score at all.", "passages": [{"id": "3432", "contributed_by": "group 2", "title": "", "section": "", "text": "Given the hypothesis and the reference, chrF is given a parameter k indicating the length of character n-grams to be considered, and computes the average of the k precisions (unigram precision, bigram, and so on) and the average of the k recalls (unigram recall, bigram recall, etc.):"}, {"id": "3435", "contributed_by": "group 2", "title": "", "section": "", "text": "The metric then computes an F-score by combining chrP and chrR using a weighting parameter beta . It is common to set beta = 2, thus weighing recall twice as much as precision:"}, {"id": "3444", "contributed_by": "group 2", "title": "", "section": "", "text": "While automatic character and word-overlap metrics like chrF or BLEU are useful, they have important limitations. chrF is very local: a large phrase that is moved around might barely change the chrF score at all, and chrF can't evaluate crosssentence properties of a document like its discourse coherence (Chapter 22). chrF and similar automatic metrics also do poorly at comparing very different kinds of systems, such as comparing human-aided translation against machine translation, or different machine translation architectures against each other (Callison-Burch et al., 2006). Instead, automatic overlap metrics like chrF are most appropriate when evaluating changes to a single system."}, {"id": "3443", "contributed_by": "group 2", "title": "", "section": "", "text": "To compare two MT systems A and B, we draw the same set of pseudo-testsets, and compute the chrF scores for each of them. We then compute the percentage of pseudo-test-sets in which A has a higher chrF score than B."}, {"id": "3441", "contributed_by": "group 2", "title": "", "section": "", "text": "Character or word overlap-based metrics like chrF (or BLEU, or etc.) are mainly used to compare two systems, with the goal of answering questions like: did the new algorithm we just invented improve our MT system? To know if the difference between the chrF scores of two MT systems is a significant difference, we use the paired bootstrap test, or the similar randomization test."}, {"id": "3466", "contributed_by": "group 2", "title": "", "section": "", "text": "MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used."}, {"id": "3310", "contributed_by": "group 2", "title": "", "section": "", "text": "We'll introduce the algorithm in sections Section 10.2, and in following sections give important components of the model like beam search decoding, and we'll discuss how MT is evaluated, introducing the simple chrF metric."}, {"id": "3440", "contributed_by": "group 2", "title": "", "section": "", "text": "chrF is simple, robust, and correlates very well with human judgments in many languages (Kocmi et al., 2021). There are various alternative overlap metrics. For example, before the development of chrF, it was common to use a word-based overlap metric called BLEU (for BiLingual Evaluation Understudy), that is purely precision-based rather than combining precision and recall (Papineni et al., 2002). The BLEU score for a corpus of candidate translation sentences is a function of the n-gram word precision over all the sentences combined with a brevity penalty computed over the corpus as a whole. Because BLEU is a word-based metric, it is very sensitive to word tokenization, making it difficult to compare across situations, and doesn't work as well in languages with complex morphology."}, {"id": "3445", "contributed_by": "group 2", "title": "", "section": "", "text": "The chrF metric is based on measuring the exact character n-grams a human reference and candidate machine translation have in common. However, this criterion is overly strict, since a good translation may use alternate words or paraphrases. A solution first pioneered in early metrics like METEOR (Banerjee and Lavie, 2005) was to allow synonyms to match between the reference x and candidate. More recent metrics use BERT or other embeddings to implement this intuition."}, {"id": "521", "contributed_by": "group 11", "title": "Chapter 13 - Constituency Parsing", "section": "", "text": "CCG supertaggers rely on treebanks such as CCGbank to provide the overall set of lexical categories and allowable category assignments for each word in the lexicon."}], "metadata": {}, "id": "78"}
{"query": "question: How are self-attention and cross-attention different answer: <extra_id_0>", "answers": ["Cross-attention has the same form as self-attention, except that while the queries as usual come from the previous layer of the decoder, the keys and values come from the output of the encoder"], "generation": "Self-attention is a mechanism for comparing an item of interest to a collection of other items within a sequence. Cross-attention is They are Self-attention", "passages": [{"id": "3493", "contributed_by": "group 2", "title": "", "section": "", "text": "Information flow in a bidirectional self-attention model. In processing each element of the sequence, the model attends to all inputs, both before and after the current one."}, {"id": "3288", "contributed_by": "group 2", "title": "", "section": "", "text": "Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, using attention heads that model how the surrounding words are relevant for the processing of the current word."}, {"id": "2790", "contributed_by": "group 10", "title": "", "section": "", "text": "The first attention mechanism we describe is dot-product attention. In this form of attention, the relevance of each encoder state to the current step of the decoder is measured using dot-product attention, which computes the dot product between the decoder hidden state and each encoder hidden state. The dot product is then normalized across the encoder hidden states, and these normalized scores are used as attention weights to compute a weighted sum of the encoder hidden states. The resulting context vector is a weighted sum of the encoder hidden states, where the weights are determined by the similarity between the decoder hidden state and each encoder hidden state."}, {"id": "3460", "contributed_by": "group 2", "title": "", "section": "", "text": "The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder."}, {"id": "3233", "contributed_by": "group 2", "title": "", "section": "", "text": "Information flow in a causal (or masked) self-attention model. In processing each element of the sequence, the model attends to all the inputs up to, and including, the current one. Unlike RNNs, the computations at each time step are independent of all the other steps and therefore can be performed in parallel."}, {"id": "3374", "contributed_by": "group 2", "title": "", "section": "", "text": "The simplest such score, called dot-product attention, implements relevance as similarity: measuring how similar the decoder hidden state is to an encoder hidden state, by computing the dot product between them:"}, {"id": "3234", "contributed_by": "group 2", "title": "", "section": "", "text": "At the core of an attention-based approach is the ability to compare an item of interest to a collection of other items in a way that reveals their relevance in the current context. In the case of self-attention, the set of comparisons are to other elements within a given sequence. The result of these comparisons is then used to compute an output for the current input. For example, returning to Fig. 9.15, the computation of y3 is based on a set of comparisons between the input x3 and its preceding elements x1 and x2, and to x3 itself. The simplest form of comparison between elements in a self-attention layer is a dot product. Let's refer to the result of this comparison as a score (we'll be updating this equation to add attention to the computation of this score):"}, {"id": "3370", "contributed_by": "group 2", "title": "", "section": "", "text": "The idea of attention is instead to create the single fixed-length vector c by taking a weighted sum of all the encoder hidden states. The weights focus on ('attend to') a particular part of the source text that is relevant for the token the decoder is currently producing. Attention thus replaces the static context vector with one that is dynamically derived from the encoder hidden states, different for each token in decoding."}, {"id": "3372", "contributed_by": "group 2", "title": "", "section": "", "text": "The attention mechanism allows each hidden state of the decoder to see a different, dynamic, context, which is a function of all the encoder hidden states."}, {"id": "3382", "contributed_by": "group 2", "title": "", "section": "", "text": "The weights Ws, which are then trained during normal end-to-end training, give the network the ability to learn which aspects of similarity between the decoder and encoder states are important to the current application. This bilinear model also allows the encoder and decoder to use different dimensional vectors, whereas the simple dot-product attention requires that the encoder and decoder hidden states have the same dimensionality."}], "metadata": {}, "id": "79"}
{"query": "question: What are some of the biases and ethical issues in machine translation answer: <extra_id_0>", "answers": ["For example, MT systems often assign gender according to culture stereotypes. Moreover, these stereotypes cannot completely be accounted for by gender bias in US labor statistics, but instead are amplified by MT systems"], "generation": "Machine translation raises many of the same biases and ethical issues that we've discussed in earlier chapters. For example, consider MT systems translating from Hungarian", "passages": [{"id": "2789", "contributed_by": "group 10", "title": "", "section": "", "text": "When evaluating machine translation output, we need to have some notion of what we are trying to achieve. One common way to think about the task is in terms of two dimensions: fluency and adequacy. Fluency refers to the grammaticality and naturalness of the output. Adequacy refers to the extent to which the output conveys the same meaning as the input. There are several methods for evaluating machine translation systems, each with its own strengths and weaknesses. Human raters, such as online crowdworkers, are considered the most accurate evaluation method for machine translation along the dimensions of fluency and adequacy. However, this method can be time-consuming and expensive, so it is often complemented by automatic evaluation metrics."}, {"id": "3425", "contributed_by": "group 2", "title": "", "section": "", "text": "Translations are evaluated along two dimensions: 1. adequacy: how well the translation captures the exact meaning of the source sentence. Sometimes called faithfulness or fidelity.  2. fluency: how fluent the translation is in the target language (is it grammatical, clear, readable, natural)."}, {"id": "3300", "contributed_by": "group 2", "title": "", "section": "", "text": "Another common use of machine translation is to aid human translators. MT systems are routinely used to produce a draft translation that is fixed up in a post-editing phase by a human translator. This task is often called computer-aided translation CAT or CAT. CAT is commonly used as part of localization: the task of adapting content localization or a product to a particular language community."}, {"id": "3477", "contributed_by": "group 2", "title": "", "section": "", "text": "Research on evaluation of machine translation began quite early. Miller and Beebe-Center (1956) proposed a number of methods drawing on work in psycholinguistics. These included the use of cloze and Shannon tasks to measure intelligibility as well as a metric of edit distance from a human translation, the intuition that underlies all modern overlap-based automatic evaluation metrics. The ALPAC report included an early evaluation study conducted by John Carroll that was extremely influential (Pierce et al., 1966, Appendix 10). Carroll proposed distinct measures for fidelity and intelligibility, and had raters score them subjectively on 9-point scales. Much early evaluation work focuses on automatic word-overlap metrics like BLEU"}, {"id": "3466", "contributed_by": "group 2", "title": "", "section": "", "text": "MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used."}, {"id": "3348", "contributed_by": "group 2", "title": "", "section": "", "text": "Translating a single sentence (inference time) in the basic RNN version of encoder-decoder approach to machine translation. Source and target sentences are concatenated with a separator token in between, and the decoder uses context information from the encoder's last hidden state."}, {"id": "2835", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "This chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights:  A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.  Many words are polysemous, having many senses.  Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.  WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.  Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.  The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.  Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.  An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.  Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target words neighborhood.  Word sense induction is the task of learning word senses unsupervised."}, {"id": "2792", "contributed_by": "group 10", "title": "", "section": "", "text": "During the training phase of a neural machine translation model, the decoder is fed with the ground truth target token as input at each timestep, a technique known as teacher forcing. However, during inference when the model is generating translations for unseen data, the decoder has to rely on its own predictions as input for the next step. This tendency of the decoder to deviate from the actual target sentence with increasing generation steps is known as the exposure bias problem. As a result, the generated translations may drift away from the desired output, leading to decreased quality. This issue can be addressed through various methods, such as using scheduled sampling or reinforcement learning. Nevertheless, teacher forcing remains the most commonly used technique during training to achieve high quality translations"}, {"id": "3451", "contributed_by": "group 2", "title": "", "section": "", "text": "Machine translation raises many of the same ethical issues that we've discussed in earlier chapters. For example, consider MT systems translating from Hungarian (which has the gender neutral pronoun o) or Spanish (which often drops pronouns) into English (in which pronouns are obligatory, and they have grammatical gender). When translating a reference to a person described without specified gender, MT systems often default to male gender (Schiebinger 2014, Prates et al. 2019). And MT systems often assign gender according to culture stereotypes of the sort we saw in Section 6.11. Fig. 10.19 shows examples from Prates et al. (2019), in which Hungarian gender-neutral o is a nurse is translated with she, but gender-neutral o is a CEO is translated with he. Prates et al. (2019) find that these stereotypes can't completely be accounted for by gender bias in US labor statistics, because the biases are amplified by MT systems, with pronouns being mapped to male or female gender with a probability higher than if the mapping was based on actual labor employment statistics."}, {"id": "3464", "contributed_by": "group 2", "title": "", "section": "", "text": "Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages."}], "metadata": {}, "id": "80"}
{"query": "question: What is the distributional hypothesis answer: <extra_id_0>", "answers": ["The idea that something about what we’re loosely calling word meanings can be learned even without any grounding in the real world, solely based on the content of the texts we’ve encountered over our lives"], "generation": "The distributional hypothesis is the idea that something about what we're loosely calling word meanings can be learned even without any grounding in the real world The distributional hypothesis The hypothesis", "passages": [{"id": "53", "contributed_by": "group 9", "title": "", "section": "", "text": "The role of context is also important in the similarity of a less biological kind of organism: the word. Words that occur in similar contexts tend to have similar meanings. This link between similarity in how words are distributed and similarity in what they mean is called the distributional hypothesis. The hypothesis was first formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth (1957), who noticed that words which are synonyms (like oculist and eye-doctor) tended to occur in the same environment (e.g., near words like eye or examined) with the amount of meaning difference between two words corresponding roughly to the amount of difference in their environments (Harris, 1954, 157)."}, {"id": "363", "contributed_by": "group 11", "title": "Chapter 12 - Constituency Grammars", "section": "", "text": "The most well known of the specic denitions is Harris idea of distributional similarity to individual units, with the substitutability test."}, {"id": "3168", "contributed_by": "group 2", "title": "", "section": "", "text": "The vector resulting from Vh can be thought of as a set of scores over the vocabulary given the evidence provided in h. Passing these scores through the softmax normalizes the scores into a probability distribution. The probability that a particular word i in the vocabulary is the next word is represented by yt [i], the ith component of yt :"}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "3062", "contributed_by": "group 2", "title": "", "section": "", "text": "The second assumption, the bigram assumption, is that the probability of a tag is"}, {"id": "1663", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "This model would dene an innite set of probability distributions, with one distribution per sentence length.."}, {"id": "3484", "contributed_by": "group 2", "title": "", "section": "", "text": "Many of these facts have motivated approaches to word learning based on the distributional hypothesis, introduced in Chapter 6. This is the idea that something about what we're loosely calling word meanings can be learned even without any grounding in the real world, solely based on the content of the texts we've encountered over our lives. This knowledge is based on the complex association of words with the words they co-occur with (and with the words that those words occur with)."}, {"id": "3147", "contributed_by": "group 2", "title": "", "section": "", "text": "Language models give us the ability to assign such a conditional probability to every possible next word, giving us a distribution over the entire vocabulary. We can also assign probabilities to entire sequences by using these conditional probabilities in combination with the chain rule:"}, {"id": "2759", "contributed_by": "group 10", "title": "", "section": "", "text": "Language models give us the ability to assign such a conditional probability to every possible next word, giving us a distribution over the entire vocabulary. We can also assign probabilities to entire sequences by using these conditional probabilities in combination with the chain rule"}, {"id": "3065", "contributed_by": "group 2", "title": "", "section": "", "text": "Viterbi algorithm for finding the optimal sequence of tags. Given an observation sequence and an HMM lambda = (A, B), the algorithm returns the state path through the HMM that assigns maximum likelihood to the observation sequence."}], "metadata": {}, "id": "81"}
{"query": "question: What is the concept of contextual embedding answer: <extra_id_0>", "answers": ["That each word will be represented by a different vector each time it appears in a different context"], "generation": "Contextual embeddings are representations of word tokens in context for any task that might require a model of word meaning. Contextual embeddings A A s A", "passages": [{"id": "2566", "contributed_by": "group 1", "title": "Vector Semantics and Embeddings", "section": "", "text": "Closely related to semantic fields is the idea of a semantic frame. A semantic frame is a set of words that denote perspectives or participants in a particular type of event."}, {"id": "3591", "contributed_by": "group 2", "title": "", "section": "", "text": "Bidirectional encoders can be used to generate contextualized representations of input embeddings using the entire input context."}, {"id": "3541", "contributed_by": "group 2", "title": "", "section": "", "text": "The result of this pretraining process consists of both learned word embeddings, as well as all the parameters of the bidirectional encoder that are used to produce contextual embeddings for novel inputs."}, {"id": "2549", "contributed_by": "group 1", "title": "Vector Semantics and Embeddings", "section": "", "text": "The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived from the distributions of word neighbors. Vectors for representing words are called embeddings."}, {"id": "2567", "contributed_by": "group 1", "title": "Vector Semantics and Embeddings", "section": "", "text": "One of the fundamental tenets of semantics, called the principle of contrast states that a difference in linguistic form is always associated with some difference in meaning. For example, the word H2O is used in scientific contexts and would be inappropriate in a hiking guide."}, {"id": "3544", "contributed_by": "group 2", "title": "", "section": "", "text": "Just as we used static embeddings like word2vec to represent the meaning of words, we can use contextual embeddings as representations of word meanings in context for any task that might require a model of word meaning. Where static embeddings represent the meaning of word types (vocabulary entries), contextual embeddings represent the meaning of word tokens: instances of a particular word type in a particular context. Contextual embeddings can thus by used for tasks like measuring the semantic similarity of two words in context, and are useful in linguistic tasks that require models of word meaning."}, {"id": "2562", "contributed_by": "group 1", "title": "Vector Semantics and Embeddings", "section": "", "text": "This could result in what Crawford (2017) and Blodgett et al. (2020) call an allocational harm, when a system allocates resources (jobs or credit) unfairly to different groups. For example, algorithms that use embeddings as part of a search for hiring potential programmers or doctors might thus incorrectly downweight documents with women 's names."}, {"id": "3542", "contributed_by": "group 2", "title": "", "section": "", "text": "Given a pretrained language model and a novel input sentence, we can think of the output of the model as constituting contextual embeddings for each token in the input. These contextual embeddings can be used as a contextual representation of the meaning of the input token for any task requiring the meaning of word."}, {"id": "2731", "contributed_by": "group 10", "title": "", "section": "", "text": "Neural language models represent words in this prior context by their embeddings, rather than just by their word identity as used in n-gram language models. Using embeddings allows neural language models to generalize better to unseen data."}, {"id": "2557", "contributed_by": "group 1", "title": "Vector Semantics and Embeddings", "section": "", "text": "Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'."}], "metadata": {}, "id": "82"}
{"query": "question: What is pre-training answer: <extra_id_0>", "answers": ["The process of learning some sort of representation of meaning for words or sentences by processing very large amounts of text"], "generation": "Pre-training is the process of learning a model by pre-tuning it with the training data. Pre-training is the process of learning a model by Pre-training Pre-training Pre-training is the process of learning Using", "passages": [{"id": "3128", "contributed_by": "group 2", "title": "", "section": "", "text": "The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence"}, {"id": "2074", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "This can be done by sorting the vocabulary by frequency in the training set, and dening the top 10100 vocabulary entries as stop words, or alternatively by using one of the many predened stop word lists available online.."}, {"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "3593", "contributed_by": "group 2", "title": "", "section": "", "text": "Pretrained language models can be fine-tuned for specific applications by adding lightweight classifier layers on top of the outputs of the pretrained model."}, {"id": "3178", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech tagging as sequence labeling with a simple RNN. Pre-trained word embeddings serve as inputs and a softmax layer provides a probability distribution over the part-of-speech tags as output at each time step."}, {"id": "1474", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "The token learner takes a raw training corpus (sometimes roughly preseparated into words, for example by whitespace) and induces a vocabulary, a set of tokens."}, {"id": "3592", "contributed_by": "group 2", "title": "", "section": "", "text": "Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input."}, {"id": "3555", "contributed_by": "group 2", "title": "", "section": "", "text": "Fine-tuning an application for one of these tasks proceeds just as with pretraining using the NSP objective. During fine-tuning, pairs of labeled sentences from the supervised training data are presented to the model. As with sequence classification, the output vector associated with the prepended [CLS] token represents the model's view of the input pair. And as with NSP training, the two inputs are separated by the a [SEP] token. To perform classification, the [CLS] vector is multiplied by a set of learning classification weights and passed through a softmax to generate label predictions, which are then used to update the weights."}, {"id": "3097", "contributed_by": "group 2", "title": "", "section": "", "text": "The known-word templates are computed for every word seen in the training set; the unknown word features can also be computed for all words in training, or only on training words whose frequency is below some threshold. The result of the known-word templates and word-signature features is a very large set of features. Generally a feature cutoff is used in which features are thrown out if they have count < 5 in the training set."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}], "metadata": {}, "id": "83"}
{"query": "question: What is fine-tuning answer: <extra_id_0>", "answers": ["The process of taking the representations from the pretrained models, and further training the model, often via an added neural net classifier, to perform some down-stream task like named entity tagging or question answering or coreference"], "generation": "Fine-tuning is the process of taking the representations from these pretrained models, and further training the model to perform some downstream task like named entity tagging or question answering. Using", "passages": [{"id": "3547", "contributed_by": "group 2", "title": "", "section": "", "text": "The following sections introduce fine-tuning methods for the most common applications including sequence classification, sequence labeling, sentence-pair inference, and span-based operations."}, {"id": "3579", "contributed_by": "group 2", "title": "", "section": "", "text": "With this approach, fine-tuning entails using supervised training data to learn the parameters of the final classifier, as well as the weights used to generate the boundary representations, and the weights in the self-attention layer that generates the span content representation. During training, the model's predictions for all spans are compared to their gold-standard labels and cross-entropy loss is used to drive the training."}, {"id": "2808", "contributed_by": "group 10", "title": "", "section": "", "text": "We call fine-tuning the process of taking the representations from these pretrained models, and further training the model, often via an added neural net classifier, to perform some down- stream task like named entity tagging or question answering or coreference. The intuition is that the pretraining phase learns a language model that instantiates a rich representations of word meaning, that thus enables the model to more easily learn ('be fine-tuned to') the requirements of a downstream language understanding task."}, {"id": "3487", "contributed_by": "group 2", "title": "", "section": "", "text": "Second, we'll introduce in this chapter the idea of pretraining and fine-tuning. We call pretraining the process of learning some sort of representation of meaning for words or sentences by processing very large amounts of text. We'll call these pretrained models pretrained language models, since they can take the form of the transformer language models we introduced in Chapter 9. We call fine-tuning the process of taking the representations from these pretrained models, and further training the model, often via an added neural net classifier, to perform some downstream task like named entity tagging or question answering or coreference. The intuition is that the pretraining phase learns a language model that instantiates a rich representations of word meaning, that thus enables the model to more easily learn ('be fine-tuned to') the requirements of a downstream language understanding task."}, {"id": "2101", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "Four popular lexicons are the General Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon of Hu and Liu (2004a) and the MPQA Subjectivity Lexicon (Wilson et al., 2005).."}, {"id": "2010", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "Various sets of subject categories exist, such as the MeSH (Medical Subject Headings) thesaurus.."}, {"id": "3555", "contributed_by": "group 2", "title": "", "section": "", "text": "Fine-tuning an application for one of these tasks proceeds just as with pretraining using the NSP objective. During fine-tuning, pairs of labeled sentences from the supervised training data are presented to the model. As with sequence classification, the output vector associated with the prepended [CLS] token represents the model's view of the input pair. And as with NSP training, the two inputs are separated by the a [SEP] token. To perform classification, the [CLS] vector is multiplied by a set of learning classification weights and passed through a softmax to generate label predictions, which are then used to update the weights."}, {"id": "2156", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "When  = 1, precision and recall are equally balanced; this is the most frequently used metric, and is called F=1 or just F1: F1 = 2PR F-measure comes from a weighted harmonic mean of precision and recall.."}, {"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}, {"id": "3460", "contributed_by": "group 2", "title": "", "section": "", "text": "The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder."}], "metadata": {}, "id": "84"}
{"query": "question: What is transfer learning answer: <extra_id_0>", "answers": ["The method of acquiring knowledge from one task or domain, and then applying it to solve a new task"], "generation": "Transfer learning is the task of learning word senses from pretrained language models. Transfer learning is the task of learning word senses unsupervised.", "passages": [{"id": "3590", "contributed_by": "group 2", "title": "", "section": "", "text": "This chapter has introduced the topic of transfer learning from pretrained language models. Here's a summary of the main points that we covered:"}, {"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "1474", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "The token learner takes a raw training corpus (sometimes roughly preseparated into words, for example by whitespace) and induces a vocabulary, a set of tokens."}, {"id": "3519", "contributed_by": "group 2", "title": "", "section": "", "text": "More specifically, the original input sequence is first tokenized using a subword model. The sampled items which drive the learning process are chosen from among the set of tokenized inputs. Word embeddings for all of the tokens in the input are retrieved from the word embedding matrix and then combined with positional embeddings to form the input to the transformer."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "3517", "contributed_by": "group 2", "title": "", "section": "", "text": "In BERT, 15% of the input tokens in a training sequence are sampled for learning. Of these, 80% are replaced with [MASK], 10% are replaced with randomly selected tokens, and the remaining 10% are left unchanged."}, {"id": "2835", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "This chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights:  A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.  Many words are polysemous, having many senses.  Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.  WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.  Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.  The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.  Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.  An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.  Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target words neighborhood.  Word sense induction is the task of learning word senses unsupervised."}, {"id": "2462", "contributed_by": "group 1", "title": "Logistic Regression", "section": "", "text": "The learning rate  is a hyperparameter that must be adjusted."}, {"id": "3592", "contributed_by": "group 2", "title": "", "section": "", "text": "Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input."}, {"id": "3524", "contributed_by": "group 2", "title": "", "section": "", "text": "learning items from a single training sequence (or batch of sequences)."}], "metadata": {}, "id": "85"}
{"query": "question: What problem do bidirectional encoders try to solve answer: <extra_id_0>", "answers": ["The issue with encoders that process input from left to right is that hidden state computation at each point in time is based solely on the current and earlier elements of the input, ignoring potentially useful information located to the right of each tagging decision"], "generation": "Bidirectional encoders try to generate contextualized representations of input embeddings using the entire input context. The problem is that generating contextualized representations generating contextualized representations generating", "passages": [{"id": "3591", "contributed_by": "group 2", "title": "", "section": "", "text": "Bidirectional encoders can be used to generate contextualized representations of input embeddings using the entire input context."}, {"id": "3553", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification with a bidirectional transformer encoder. The output vector for the [CLS] token serves as input to a simple classifier."}, {"id": "3459", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence."}, {"id": "3561", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence labeling for part-of-speech tagging with a bidirectional transformer encoder. The output vector for each input token is passed to a simple k-way classifier."}, {"id": "3541", "contributed_by": "group 2", "title": "", "section": "", "text": "The result of this pretraining process consists of both learned word embeddings, as well as all the parameters of the bidirectional encoder that are used to produce contextual embeddings for novel inputs."}, {"id": "3592", "contributed_by": "group 2", "title": "", "section": "", "text": "Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input."}, {"id": "3505", "contributed_by": "group 2", "title": "", "section": "", "text": "To make this more concrete, the original bidirectional transformer encoder model, BERT (Devlin et al., 2019), consisted of the following:"}, {"id": "2793", "contributed_by": "group 10", "title": "", "section": "", "text": "In a typical encoder-decoder model, the encoder processes the source sentence and outputs a set of hidden states, which are used by the decoder to generate the target sentence. Both the encoder and decoder are often implemented as recurrent neural networks (RNNs), with the encoder processing the source sentence in a left-to-right or bidirectional manner, and the decoder generating the target sentence one word at a time, conditioned on the source sentence and the previously generated words. Encoder-decoder architectures are trained in a supervised manner with paired source and target strings, using maximum likelihood estimation (MLE) to minimize the cross-entropy loss between the predicted and true target sequences. During training, the decoder is typically forced to generate the true target sequence at each time step, a technique known as teacher forcing."}, {"id": "3336", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks consist of three components: 1. An encoder that accepts an input sequence, x1n, and generates a corresponding sequence of contextualized representations, hn1. LSTMs, convolutional networks, and Transformers can all be employed as encoders."}, {"id": "3374", "contributed_by": "group 2", "title": "", "section": "", "text": "The simplest such score, called dot-product attention, implements relevance as similarity: measuring how similar the decoder hidden state is to an encoder hidden state, by computing the dot product between them:"}], "metadata": {}, "id": "86"}
{"query": "question: What is the advantage of contextual embedding instead of static embedding answer: <extra_id_0>", "answers": ["Given a sequence of input tokens, we can use the output vector from the final layer of the model as a representation of the meaning of tokens in the context of the whole sentence. This improves performance of tasks that might require a model of word meaning"], "generation": "Contextual embeddings represent the meaning of word tokens in the context of the target word for any task that might require a model of word meaning.", "passages": [{"id": "3541", "contributed_by": "group 2", "title": "", "section": "", "text": "The result of this pretraining process consists of both learned word embeddings, as well as all the parameters of the bidirectional encoder that are used to produce contextual embeddings for novel inputs."}, {"id": "2832", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "Thesauruses have also been used to improve both static and contextual word embeddings. For example, static word embeddings have a problem with antonyms. A word like expensive is often very similar in embedding cosine to its antonym like cheap. Antonymy information from thesauruses can help solve this problem; There are two families of solutions. The first requires retraining: we modify the embedding training to incorporate thesaurus relations like synonymy, antonym, or supersenses. This can be done by modifying the static embedding loss function for word2vec (Yu and Dredze 2014, Nguyen et al. 2016) or by modifying contextual embedding training (Levine et al. 2020, Lauscher et al. 2019). The second, for static embeddings, is more light-weight; after the embeddings have been trained we learn a second mapping based on a thesaurus that shifts the embeddings of words in such a way that synonyms (according to the thesaurus) are pushed closer and antonyms further apart. Such methods are called retrofitting (Faruqui et al. 2015, Lengerich et al. 2018) or counterfitting (Mrksi c et al.  , 2016)."}, {"id": "2562", "contributed_by": "group 1", "title": "Vector Semantics and Embeddings", "section": "", "text": "This could result in what Crawford (2017) and Blodgett et al. (2020) call an allocational harm, when a system allocates resources (jobs or credit) unfairly to different groups. For example, algorithms that use embeddings as part of a search for hiring potential programmers or doctors might thus incorrectly downweight documents with women 's names."}, {"id": "3574", "contributed_by": "group 2", "title": "", "section": "", "text": "A weakness of this approach is that it doesn't distinguish the use of a word's embedding as the beginning of a span from its use as the end of one. Therefore, more elaborate schemes for representing the span boundaries involve learned representations for start and end points through the use of two distinct feedforward networks:"}, {"id": "3544", "contributed_by": "group 2", "title": "", "section": "", "text": "Just as we used static embeddings like word2vec to represent the meaning of words, we can use contextual embeddings as representations of word meanings in context for any task that might require a model of word meaning. Where static embeddings represent the meaning of word types (vocabulary entries), contextual embeddings represent the meaning of word tokens: instances of a particular word type in a particular context. Contextual embeddings can thus by used for tasks like measuring the semantic similarity of two words in context, and are useful in linguistic tasks that require models of word meaning."}, {"id": "2731", "contributed_by": "group 10", "title": "", "section": "", "text": "Neural language models represent words in this prior context by their embeddings, rather than just by their word identity as used in n-gram language models. Using embeddings allows neural language models to generalize better to unseen data."}, {"id": "3591", "contributed_by": "group 2", "title": "", "section": "", "text": "Bidirectional encoders can be used to generate contextualized representations of input embeddings using the entire input context."}, {"id": "2835", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "This chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights:  A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.  Many words are polysemous, having many senses.  Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.  WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.  Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.  The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.  Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.  An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.  Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target words neighborhood.  Word sense induction is the task of learning word senses unsupervised."}, {"id": "2566", "contributed_by": "group 1", "title": "Vector Semantics and Embeddings", "section": "", "text": "Closely related to semantic fields is the idea of a semantic frame. A semantic frame is a set of words that denote perspectives or participants in a particular type of event."}, {"id": "2557", "contributed_by": "group 1", "title": "Vector Semantics and Embeddings", "section": "", "text": "Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'."}], "metadata": {}, "id": "87"}
{"query": "question: What is the difference between word types and word tokens answer: <extra_id_0>", "answers": ["Instead of just capturing the vocabulary entries with word types, word tokens also capture the context of the words in the sentence"], "generation": "Word types are the locus of word meaning; definitions and meaning relations are defined at the level of the word type rather than word tokens. Word types", "passages": [{"id": "1469", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "To deal with this unknown word problem, modern tokenizers often automatically induce sets of tokens that include tokens smaller than words, called subwords."}, {"id": "2835", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "This chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights:  A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.  Many words are polysemous, having many senses.  Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.  WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.  Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.  The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.  Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.  An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.  Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target words neighborhood.  Word sense induction is the task of learning word senses unsupervised."}, {"id": "1471", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "(A morpheme is the smallest meaning-bearing unit of a language; for example the word unlikeliest has the morphemes un-, likely, and -est.) In modern tokenization schemes, most tokens are words, but some tokens are frequently occurring morphemes or other subwords like -er."}, {"id": "1454", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "Word tokenization is more complex in languages like written Chinese, Japanese, and Thai, which do not use spaces to mark potential word-boundaries."}, {"id": "1440", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "A tokenizer can also be used to expand clitic contractions that are marked by apostrophes, for example, converting what 're to the two tokens what are, and we 're to we are."}, {"id": "1386", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "The larger the corpora we look at, the more word types we nd, and in fact this relationship between the number of types |V| and number of tokens N is called Herdan 's Law (Herdan, 1960) or Heaps ' Law (Heaps, 1978) after its discoverers (in linguistics and information retrieval respectively)."}, {"id": "1382", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "Tokens are the total number N of running words."}, {"id": "1392", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "Another measure of the number of words in the language is the number of lemmas instead of wordform types."}, {"id": "1608", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": " Word tokenization and normalization are generally done by cascades of simple regular expression substitutions or nite automata."}, {"id": "1388", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "|V| = kN Tokens = N Types = |V| 884 thousand 31 thousand 1 million 38 thousand Switchboard telephone conversations 2.4 million 20 thousand Rough numbers of types and tokens for some English language corpora."}], "metadata": {}, "id": "88"}
{"query": "question: Where does the power of pre-trained language models come from answer: <extra_id_0>", "answers": ["Their ability to extract generalizations from large amounts of text"], "generation": "The power of pretrained language models lies in their ability to extract generalizations from large amounts of text-generalizations that are useful for myriad downstream applications.", "passages": [{"id": "3592", "contributed_by": "group 2", "title": "", "section": "", "text": "Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input."}, {"id": "3593", "contributed_by": "group 2", "title": "", "section": "", "text": "Pretrained language models can be fine-tuned for specific applications by adding lightweight classifier layers on top of the outputs of the pretrained model."}, {"id": "2808", "contributed_by": "group 10", "title": "", "section": "", "text": "We call fine-tuning the process of taking the representations from these pretrained models, and further training the model, often via an added neural net classifier, to perform some down- stream task like named entity tagging or question answering or coreference. The intuition is that the pretraining phase learns a language model that instantiates a rich representations of word meaning, that thus enables the model to more easily learn ('be fine-tuned to') the requirements of a downstream language understanding task."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "3490", "contributed_by": "group 2", "title": "", "section": "", "text": "Finally, we'll show how the contextual embeddings from these pretrained language models can be used to transfer the knowledge embodied in these models to novel applications via fine-tuning. Indeed, in later chapters we'll see pretrained language models fine-tuned to tasks from parsing to question answering, from information extraction to semantic parsing."}, {"id": "2815", "contributed_by": "group 10", "title": "", "section": "", "text": "The power of pretrained language models lies in their ability to extract generalizations from large amounts of textgeneralizations that are useful for myriad downstream applications. To make practical use of these generalizations, we need to create interfaces from these models to downstream applications through a process called fine-tuning. Fine-tuning facilitates the creation of applications on top of pre-trained models through the addition of a small set of application-specific parameters. The fine-tuning process consists of using labeled data from the application to train these additional application-specific parameters. Typically, this training will either freeze or make only minimal adjustments to the pretrained language model parameters."}, {"id": "3290", "contributed_by": "group 2", "title": "", "section": "", "text": "Common language-based applications for RNNs and transformers include: Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. Auto-regressive generation using a trained language model. Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification."}, {"id": "3546", "contributed_by": "group 2", "title": "", "section": "", "text": "The power of pretrained language models lies in their ability to extract generalizations from large amounts of text-generalizations that are useful for myriad downstream applications. To make practical use of these generalizations, we need to create interfaces from these models to downstream applications through a process called fine-tuning. Fine-tuning facilitates the creation of applications on top of pretrained models through the addition of a small set of application-specific parameters. The fine-tuning process consists of using labeled data from the application to train these additional application-specific parameters. Typically, this training will either freeze or make only minimal adjustments to the pretrained language model parameters."}, {"id": "2991", "contributed_by": "group 2", "title": "", "section": "", "text": "Neural networks are trained by optimization algorithms like gradient descent.Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling."}, {"id": "3487", "contributed_by": "group 2", "title": "", "section": "", "text": "Second, we'll introduce in this chapter the idea of pretraining and fine-tuning. We call pretraining the process of learning some sort of representation of meaning for words or sentences by processing very large amounts of text. We'll call these pretrained models pretrained language models, since they can take the form of the transformer language models we introduced in Chapter 9. We call fine-tuning the process of taking the representations from these pretrained models, and further training the model, often via an added neural net classifier, to perform some downstream task like named entity tagging or question answering or coreference. The intuition is that the pretraining phase learns a language model that instantiates a rich representations of word meaning, that thus enables the model to more easily learn ('be fine-tuned to') the requirements of a downstream language understanding task."}], "metadata": {}, "id": "89"}
{"query": "question: What is the CLS token answer: <extra_id_0>", "answers": ["An additional vector that is added to the model to stand for the entire sequence. The output vector in the final layer of the model for the CLS input represents the entire input sequence and serves as the input to a classifier head"], "generation": "The CLS token serves as input to a simple classifier. The output vector for the [CLS] token serves as input to - --", "passages": [{"id": "1440", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "A tokenizer can also be used to expand clitic contractions that are marked by apostrophes, for example, converting what 're to the two tokens what are, and we 're to we are."}, {"id": "1471", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "(A morpheme is the smallest meaning-bearing unit of a language; for example the word unlikeliest has the morphemes un-, likely, and -est.) In modern tokenization schemes, most tokens are words, but some tokens are frequently occurring morphemes or other subwords like -er."}, {"id": "3569", "contributed_by": "group 2", "title": "", "section": "", "text": "span is a contiguous sequence of tokens with start i and end j such that 1 <= i <="}, {"id": "3515", "contributed_by": "group 2", "title": "", "section": "", "text": "(1) It is replaced with the unique vocabulary token [MASK]."}, {"id": "1474", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "The token learner takes a raw training corpus (sometimes roughly preseparated into words, for example by whitespace) and induces a vocabulary, a set of tokens."}, {"id": "3553", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification with a bidirectional transformer encoder. The output vector for the [CLS] token serves as input to a simple classifier."}, {"id": "1473", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "Most tokenization schemes have two parts: a token learner, and a token segmenter."}, {"id": "1445", "contributed_by": "group 1", "title": "Regular Expressions, Text Normalization, Edit Distance", "section": "", "text": "One commonly used tokenization standard is known as the Penn Treebank tokenization standard, used for the parsed corpora (treebanks) released by the Linguistic Data Consortium (LDC), the source of many useful datasets."}, {"id": "3384", "contributed_by": "group 2", "title": "", "section": "", "text": "Choosing the single most probable token to generate at each step is called greedy decoding; a greedy algorithm is one that make a choice that is locally optimal, whether or not it will turn out to have been the best choice with hindsight."}, {"id": "3449", "contributed_by": "group 2", "title": "", "section": "", "text": "matching in red, and include the optional idf importance weighting. which tokens are also weighted by their idf values."}], "metadata": {}, "id": "90"}
{"query": "question: How to classify with transfer learning through fine-tuning answer: <extra_id_0>", "answers": ["Append the CLS token to the beginning of the sentence, and the output vector of the CLS token in the final layer of the model serves as the input to a classifier head that makes the relevant decision."], "generation": "Transfer learning is a method for learning the parameters of the final classifier. During fine-tuning, pairs of labeled sentences from the supervised training data are presented to the model Transfer learning Transfer", "passages": [{"id": "3579", "contributed_by": "group 2", "title": "", "section": "", "text": "With this approach, fine-tuning entails using supervised training data to learn the parameters of the final classifier, as well as the weights used to generate the boundary representations, and the weights in the self-attention layer that generates the span content representation. During training, the model's predictions for all spans are compared to their gold-standard labels and cross-entropy loss is used to drive the training."}, {"id": "12", "contributed_by": "group 9", "title": "", "section": "", "text": "Classification lies at the heart of both human and machine intelligence. Deciding what letter, word, or image has been presented to our senses, recognizing faces or voices, sorting mail, assigning grades to homeworks; these are all examples of assigning a category to an input. The potential challenges of this task are highlighted by the fabulist Jorge Luis Borges (1964), who imagined classifying animals into"}, {"id": "3555", "contributed_by": "group 2", "title": "", "section": "", "text": "Fine-tuning an application for one of these tasks proceeds just as with pretraining using the NSP objective. During fine-tuning, pairs of labeled sentences from the supervised training data are presented to the model. As with sequence classification, the output vector associated with the prepended [CLS] token represents the model's view of the input pair. And as with NSP training, the two inputs are separated by the a [SEP] token. To perform classification, the [CLS] vector is multiplied by a set of learning classification weights and passed through a softmax to generate label predictions, which are then used to update the weights."}, {"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}, {"id": "3290", "contributed_by": "group 2", "title": "", "section": "", "text": "Common language-based applications for RNNs and transformers include: Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. Auto-regressive generation using a trained language model. Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification."}, {"id": "3590", "contributed_by": "group 2", "title": "", "section": "", "text": "This chapter has introduced the topic of transfer learning from pretrained language models. Here's a summary of the main points that we covered:"}, {"id": "2253", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "Text categorization, in which an entire text is assigned a class from a nite set, includes such tasks as sentiment analysis, spam detection, language identication, and authorship attribution.."}, {"id": "2247", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "But these problems can also be caused by the labels (for example due to biases in the human labelers), by the resources used (like lexicons, or model components like pretrained embeddings), or even by model architecture (like what the model is trained to optimize).."}, {"id": "2258", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "Classiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.."}, {"id": "2010", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "Various sets of subject categories exist, such as the MeSH (Medical Subject Headings) thesaurus.."}], "metadata": {}, "id": "91"}
{"query": "question: What is a main difference between neural classifiers and classifiers based on pre-trained language model answer: <extra_id_0>", "answers": ["Reasonable classification performance with pre-trained language models can be typically achieved with only minimal changes to the language model parameters, often limited to updates over the final few layers of the transformer"], "generation": "Neural classifiers are trained by optimization algorithms like gradient descent.Error backpropagation is used to compute the gradients of the loss function for a network. Using", "passages": [{"id": "3549", "contributed_by": "group 2", "title": "", "section": "", "text": "is added to the vocabulary and is prepended to the start of all input sequences, both during pretraining and encoding. The output vector in the final layer of the model for the [CLS] input represents the entire input sequence and serves as the input to a classifier head, a logistic regression or neural network classifier that makes the relevant decision."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "3593", "contributed_by": "group 2", "title": "", "section": "", "text": "Pretrained language models can be fine-tuned for specific applications by adding lightweight classifier layers on top of the outputs of the pretrained model."}, {"id": "3290", "contributed_by": "group 2", "title": "", "section": "", "text": "Common language-based applications for RNNs and transformers include: Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. Auto-regressive generation using a trained language model. Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification."}, {"id": "2991", "contributed_by": "group 2", "title": "", "section": "", "text": "Neural networks are trained by optimization algorithms like gradient descent.Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling."}, {"id": "2808", "contributed_by": "group 10", "title": "", "section": "", "text": "We call fine-tuning the process of taking the representations from these pretrained models, and further training the model, often via an added neural net classifier, to perform some down- stream task like named entity tagging or question answering or coreference. The intuition is that the pretraining phase learns a language model that instantiates a rich representations of word meaning, that thus enables the model to more easily learn ('be fine-tuned to') the requirements of a downstream language understanding task."}, {"id": "3552", "contributed_by": "group 2", "title": "", "section": "", "text": "A key difference from what we've seen earlier with neural classifiers is that this loss can be used to not only learn the weights of the classifier, but also to update the weights for the pretrained language model itself. In practice, reasonable classification performance is typically achieved with only minimal changes to the language model parameters, often limited to updates over the final few layers of the transformer. Fig. 11.8 illustrates this overall approach to sequence classification."}, {"id": "3592", "contributed_by": "group 2", "title": "", "section": "", "text": "Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input."}, {"id": "2800", "contributed_by": "group 10", "title": "", "section": "", "text": "This vector is sometimes called the sentence embedding since it refers to the entire sequence, although the term 'sentence embedding' is also used in other ways. In BERT, the [CLS] token plays the role of this embedding. This unique token is added to the vocabulary and is prepended to the start of all input sequences, both during pretraining and encoding. The output vector in the final layer of the model for the [CLS] input represents the entire input sequence and serves as the input to a classifier head, a logistic regression or neural network classifier that makes the relevant decision."}, {"id": "2731", "contributed_by": "group 10", "title": "", "section": "", "text": "Neural language models represent words in this prior context by their embeddings, rather than just by their word identity as used in n-gram language models. Using embeddings allows neural language models to generalize better to unseen data."}], "metadata": {}, "id": "92"}
{"query": "question: What does NSP stand for answer: <extra_id_0>", "answers": ["Next Sentence Prediction"], "generation": "NSP stands for Next Sentence Prediction. NSP is a training objective for NSP models. Next sentence prediction.", "passages": [{"id": "3555", "contributed_by": "group 2", "title": "", "section": "", "text": "Fine-tuning an application for one of these tasks proceeds just as with pretraining using the NSP objective. During fine-tuning, pairs of labeled sentences from the supervised training data are presented to the model. As with sequence classification, the output vector associated with the prepended [CLS] token represents the model's view of the input pair. And as with NSP training, the two inputs are separated by the a [SEP] token. To perform classification, the [CLS] vector is multiplied by a set of learning classification weights and passed through a softmax to generate label predictions, which are then used to update the weights."}, {"id": "3538", "contributed_by": "group 2", "title": "", "section": "", "text": "Cross entropy is used to compute the NSP loss for each sentence pair presented to the model. Fig. 11.7 illustrates the overall NSP training setup. In BERT, the NSP loss was used in conjunction with the MLM training objective to form final loss."}, {"id": "3535", "contributed_by": "group 2", "title": "", "section": "", "text": "To facilitate NSP training, BERT introduces two new tokens to the input representation (tokens that will prove useful for fine-tuning as well). After tokenizing the"}, {"id": "3210", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN for sequence classification. The final hidden units from the forward and backward passes are combined to represent the entire sequence. This combined representation serves as input to the subsequent classifier."}, {"id": "3013", "contributed_by": "group 2", "title": "", "section": "", "text": "Below we show some examples with each word tagged according to both the UD and Penn tagsets. Notice that the Penn tagset distinguishes tense and participles on verbs, and has a special tag for the existential there construction in English. Note that since New England Journal of Medicine is a proper noun, both tagsets mark its component nouns as NNP, including journal and medicine, which might otherwise be labeled as common nouns (NOUN/NN)."}, {"id": "2814", "contributed_by": "group 10", "title": "", "section": "", "text": "To train the original BERT models, pairs of sentences were selected from the training corpus according to the next sentence prediction 50/50 scheme. Pairs were sampled so that their combined length was less than the 512 token input. Tokens within these sentence pairs were then masked using the MLM approach with the combined loss from the MLM and NSP objectives used for a final loss. Approximately 40 passes (epochs) over the training data was required for the model to converge."}, {"id": "3540", "contributed_by": "group 2", "title": "", "section": "", "text": "To train the original BERT models, pairs of sentences were selected from the training corpus according to the next sentence prediction 50/50 scheme. Pairs were sampled so that their combined length was less than the 512 token input. Tokens within these sentence pairs were then masked using the MLM approach with the combined loss from the MLM and NSP objectives used for a final loss. Approximately 40 passes (epochs) over the training data was required for the model to converge."}, {"id": "2210", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "There are two common non-parametric tests used in NLP: approximate randomization (Noreen, 1989) and the bootstrap test.."}, {"id": "3286", "contributed_by": "group 2", "title": "", "section": "", "text": "RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT)."}, {"id": "3125", "contributed_by": "group 2", "title": "", "section": "", "text": "Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words."}], "metadata": {}, "id": "93"}
{"query": "question: How does pairwise sequence classification work answer: <extra_id_0>", "answers": ["Append the CLS token to the beginning of the sentence, and add a SEP token between the two sentences, and use a classifier to classify the final layer of the CLS token"], "generation": "Pairwise sequence classification is the task of assigning an entire text to a category, as in spam detection, sentiment analysis or topic classification. Pairwise classification", "passages": [{"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}, {"id": "3210", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN for sequence classification. The final hidden units from the forward and backward passes are combined to represent the entire sequence. This combined representation serves as input to the subsequent classifier."}, {"id": "3554", "contributed_by": "group 2", "title": "", "section": "", "text": "As mentioned in Section 11.2.3, an important type of problem involves the classification of pairs of input sequences. Practical applications that fall into this class include logical entailment, paraphrase detection and discourse analysis."}, {"id": "3290", "contributed_by": "group 2", "title": "", "section": "", "text": "Common language-based applications for RNNs and transformers include: Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. Auto-regressive generation using a trained language model. Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification."}, {"id": "3553", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification with a bidirectional transformer encoder. The output vector for the [CLS] token serves as input to a simple classifier."}, {"id": "3558", "contributed_by": "group 2", "title": "", "section": "", "text": "To fine-tune a classifier for the MultiNLI task, we pass the premise/hypothesis pairs through a bidirectional encoder as described above and use the output vector for the [CLS] token as the input to the classification head. As with ordinary sequence classification, this head provides the input to a three-way classifier that can be trained on the MultiNLI training corpus."}, {"id": "2253", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "Text categorization, in which an entire text is assigned a class from a nite set, includes such tasks as sentiment analysis, spam detection, language identication, and authorship attribution.."}, {"id": "2835", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "This chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights:  A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.  Many words are polysemous, having many senses.  Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.  WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.  Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.  The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.  Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.  An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.  Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target words neighborhood.  Word sense induction is the task of learning word senses unsupervised."}, {"id": "3547", "contributed_by": "group 2", "title": "", "section": "", "text": "The following sections introduce fine-tuning methods for the most common applications including sequence classification, sequence labeling, sentence-pair inference, and span-based operations."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}], "metadata": {}, "id": "94"}
{"query": "question: What are examples of pairwise sequence classification answer: <extra_id_0>", "answers": ["Recognizing textual entailment, or whether one sentence leads logically to the next sentence"], "generation": "Biological sequence classification, spam detection, sentiment analysis, topic classification, and topic classification using a simple RNN.", "passages": [{"id": "3183", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification."}, {"id": "3210", "contributed_by": "group 2", "title": "", "section": "", "text": "A bidirectional RNN for sequence classification. The final hidden units from the forward and backward passes are combined to represent the entire sequence. This combined representation serves as input to the subsequent classifier."}, {"id": "3554", "contributed_by": "group 2", "title": "", "section": "", "text": "As mentioned in Section 11.2.3, an important type of problem involves the classification of pairs of input sequences. Practical applications that fall into this class include logical entailment, paraphrase detection and discourse analysis."}, {"id": "3290", "contributed_by": "group 2", "title": "", "section": "", "text": "Common language-based applications for RNNs and transformers include: Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. Auto-regressive generation using a trained language model. Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification."}, {"id": "2820", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "Another way word senses can be related is taxonomically. A word (or sense) is a hyponym of another word or sense if the first is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, hypernym and mango is a hyponym of fruit. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. It is unfortunate that the two words (hypernym and hyponym) are very similar and hence easily confused; for this reason, the word superordinate is often used instead of hypernym. We can define hypernymy more formally by saying that the class denoted by the superordinate extensionally includes the class denoted by the hyponym. Thus, the class of animals includes as members all dogs, and the class of moving actions includes all walking actions. Hypernymy can also be defined in terms of entailment. Under this definition, a sense A is a hyponym of a sense B if everything that is A is also B, and hence being an A entails being a B, or x A(x)  B(x). Hyponymy/hypernymy is usually a transitive relation; if A is a hyponym of B and B is a hyponym of C, then A is a hyponym of C. Another name for the hypernym/hyponym structure is the IS-A hierarchy, in which we say A IS-A B, or B subsumes A."}, {"id": "3547", "contributed_by": "group 2", "title": "", "section": "", "text": "The following sections introduce fine-tuning methods for the most common applications including sequence classification, sequence labeling, sentence-pair inference, and span-based operations."}, {"id": "2835", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "This chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights:  A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.  Many words are polysemous, having many senses.  Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.  WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.  Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.  The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.  Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.  An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.  Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target words neighborhood.  Word sense induction is the task of learning word senses unsupervised."}, {"id": "3553", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification with a bidirectional transformer encoder. The output vector for the [CLS] token serves as input to a simple classifier."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "2778", "contributed_by": "group 10", "title": "", "section": "", "text": "The basic idea behind sequence-to-sequence models is to use two recurrent neural networks (RNNs) together, called an encoder and a decoder, to map an input sequence to an output sequence. The encoder network takes an input sequence and creates a contextualized representation of it, often called the context. This representation is then passed to a decoder which generates a task-specific output sequence. The encoder and decoder are trained jointly to optimize a suitable objective function, such as maximizing the likelihood of the correct output given the input, or minimizing the distance between the correct output and the predicted output."}], "metadata": {}, "id": "95"}
{"query": "question: What is an example of span-based application answer: <extra_id_0>", "answers": ["Identification of spans of interest, entity recognition, question answering, coreference resolution"], "generation": "Span-based language model training. In this example, a span of length 3 is selected for training and all of the words in the span are selected for training. Named entity recognition Span Span Span Named entity", "passages": [{"id": "3567", "contributed_by": "group 2", "title": "", "section": "", "text": "Span-oriented applications operate in a middle ground between sequence level and token level tasks. That is, in span-oriented applications the focus is on generating and operating with representations of contiguous sequences of tokens. Typical operations include identifying spans of interest, classifying spans according to some labeling scheme, and determining relations among discovered spans. Applications include named entity recognition, question answering, syntactic parsing, semantic role labeling and coreference resolution."}, {"id": "3576", "contributed_by": "group 2", "title": "", "section": "", "text": "Now, given span representations g for each span in S(x), classifiers can be finetuned to generate application-specific scores for various span-oriented tasks: binary span identification (is this a legitimate span of interest or not?), span classification (what kind of span is this?), and span relation classification (how are these two spans related?)."}, {"id": "3528", "contributed_by": "group 2", "title": "", "section": "", "text": "Downstream span-based applications rely on span representations derived from the tokens within the span, as well as the start and end points, or the boundaries, of a span. Representations for these boundaries are typically derived from the first and last words of a span, the words immediately preceding and following the span, or some combination of them. The SpanBERT learning objective augments the MLM objective with a boundary oriented component called the Span Boundary Objective (SBO). The SBO relies on a model's ability to predict the words within a masked span from the words immediately preceding and following it. This prediction is made using the output vectors associated with the words that immediately precede and follow the span being masked, along with positional embedding that signals which word in the span is being predicted:"}, {"id": "3532", "contributed_by": "group 2", "title": "", "section": "", "text": "Span-based language model training. In this example, a span of length 3 is selected for training and all of the words in the span are masked. The figure illustrates the loss computed for word thanks; the loss for the entire span is based on the loss for all three of the words in the span."}, {"id": "3582", "contributed_by": "group 2", "title": "", "section": "", "text": "The second advantage to span-based approaches is that they naturally accommodate embedded named entities. For example, in this example both United Airlines and United Airlines Holding are legitimate named entities. The BIO approach has no way of encoding this embedded structure. But the span-based approach can naturally label both since the spans are labeled separately."}, {"id": "3572", "contributed_by": "group 2", "title": "", "section": "", "text": "The first step in fine-tuning a pretrained language model for a span-based application using the contextualized input embeddings from the model to generate representations for all the spans in the input. Most schemes for representing spans make use of two primary components: representations of the span boundaries and summary representations of the contents of each span. To compute a unified span representation, we concatenate the boundary representations with the summary representation."}, {"id": "3571", "contributed_by": "group 2", "title": "", "section": "", "text": "For practical purposes, span-based models often impose an application-specific length limit L, so the legal spans are limited to those where j - i < L. In the following, we'll refer to the enumerated set of legal spans in x as S(x)."}, {"id": "3526", "contributed_by": "group 2", "title": "", "section": "", "text": "A span is a contiguous sequence of one or more words selected from a training text, prior to subword tokenization. In span-based masking, a set of randomly selected spans from a training sequence are chosen. In the SpanBERT work that originated this technique (Joshi et al., 2020), a span length is first chosen by sampling from a geometric distribution that is biased towards shorter spans an with upper bound of 10. Given this span length, a starting location consistent with the desired span length and the length of the input is sampled uniformly."}, {"id": "3569", "contributed_by": "group 2", "title": "", "section": "", "text": "span is a contiguous sequence of tokens with start i and end j such that 1 <= i <="}, {"id": "3547", "contributed_by": "group 2", "title": "", "section": "", "text": "The following sections introduce fine-tuning methods for the most common applications including sequence classification, sequence labeling, sentence-pair inference, and span-based operations."}], "metadata": {}, "id": "96"}
{"query": "question: What are some of the potential harms from language models answer: <extra_id_0>", "answers": ["Language models can generate toxic language, show biased, and be easily manipulated for disinformation, and may violate privacy"], "generation": "Language models can generate toxic language. Gehman et al. (2020) show that large pretrained neural language models exhibit many of the potential harms discussed in Chapter 4. Language models Several Several", "passages": [{"id": "3583", "contributed_by": "group 2", "title": "", "section": "", "text": "Large pretrained neural language models exhibit many of the potential harms discussed in Chapter 4 and Chapter 6. Many of these harms become realized when pretrained language models are fine-tuned to downstream tasks, particularly those involving text generation, such as in assistive technologies like web search query completion, or predictive typing for email (Olteanu et al., 2020)."}, {"id": "1626", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "Models that assign probabilities to sequences of words are called language models or LMs.."}, {"id": "3589", "contributed_by": "group 2", "title": "", "section": "", "text": "Mitigating all these harms is an important but unsolved research question in NLP. Extra pretraining (Gururangan et al., 2020) on non-toxic subcorpora seems to reduce a language model's tendency to generate toxic language somewhat (Gehman et al., 2020). And analyzing the data used to pretrain large language models is important to understand toxicity and bias in generation, as well as privacy, making it extremely important that language models include datasheets (page 14) or model cards (page 75) giving full replicable information on the corpora used to train them."}, {"id": "3584", "contributed_by": "group 2", "title": "", "section": "", "text": "For example, language models can generate toxic language. Gehman et al. (2020) show that many kinds of completely non-toxic prompts can nonetheless lead large language models to output hate speech and abuse. Brown et al. (2020) and Sheng et al. (2019) showed that large language models generate sentences displaying negative attitudes toward minority identities such as being Black or gay."}, {"id": "3586", "contributed_by": "group 2", "title": "", "section": "", "text": "Language models can also be a tool for generating text for misinformation, phishing, radicalization, and other socially harmful activities (Brown et al., 2020). McGuffie and Newhouse (2020) show how large language models generate text that emulates online extremists, with the risk of amplifying extremist movements and their attempt to radicalize and recruit."}, {"id": "1989", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity.."}, {"id": "1808", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "Our models may still be subject to the problem of sparsity.."}, {"id": "2767", "contributed_by": "group 10", "title": "", "section": "", "text": "In language modeling, the goal is to learn a model that can generate the correct next word in a sequence of words. One way to train such a model is to use the cross-entropy loss as the objective function. The cross-entropy loss is determined by the probability that the RNN language model assigns to the correct next word. Specifically, at each time step, the model generates a probability distribution over the possible next words in the sequence, and the cross-entropy loss is calculated by taking the negative log probability of the correct next word. By minimizing the cross-entropy loss, the model is trained to assign higher probabilities to the correct next words, and thus generate more accurate predictions. This approach has been used in a variety of language modeling tasks, and is a key component of many state-of-the-art language models."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "1720", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "The perplexity (sometimes called PP for short) of a language model on a test set is the inverse probability of the test set, normalized by the number of words.."}], "metadata": {}, "id": "97"}
{"query": "question: What cause biases in language models answer: <extra_id_0>", "answers": ["For example, the training data may contain a biased distribution, such as a disproportionate amount of data generated by authors from developed countries. Such biased population samples likely skew the resulting generation away from the perspectives or topics of underrepresented populations"], "generation": "Several factors can cause biases in language models..", "passages": [{"id": "1993", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "Both backoff and interpolation require discounting to create a probability dis Kneser-Ney smoothing makes use of the probability of a word being a novel continuation.."}, {"id": "2247", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "But these problems can also be caused by the labels (for example due to biases in the human labelers), by the resources used (like lexicons, or model components like pretrained embeddings), or even by model architecture (like what the model is trained to optimize).."}, {"id": "1720", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "The perplexity (sometimes called PP for short) of a language model on a test set is the inverse probability of the test set, normalized by the number of words.."}, {"id": "1833", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "This modication is called smoothing or discounting.."}, {"id": "1626", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "Models that assign probabilities to sequences of words are called language models or LMs.."}, {"id": "2007", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "Starting in the late 1990s, Chen and Goodman performed a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters (Chen and Goodman.."}, {"id": "2767", "contributed_by": "group 10", "title": "", "section": "", "text": "In language modeling, the goal is to learn a model that can generate the correct next word in a sequence of words. One way to train such a model is to use the cross-entropy loss as the objective function. The cross-entropy loss is determined by the probability that the RNN language model assigns to the correct next word. Specifically, at each time step, the model generates a probability distribution over the possible next words in the sequence, and the cross-entropy loss is calculated by taking the negative log probability of the correct next word. By minimizing the cross-entropy loss, the model is trained to assign higher probabilities to the correct next words, and thus generate more accurate predictions. This approach has been used in a variety of language modeling tasks, and is a key component of many state-of-the-art language models."}, {"id": "3589", "contributed_by": "group 2", "title": "", "section": "", "text": "Mitigating all these harms is an important but unsolved research question in NLP. Extra pretraining (Gururangan et al., 2020) on non-toxic subcorpora seems to reduce a language model's tendency to generate toxic language somewhat (Gehman et al., 2020). And analyzing the data used to pretrain large language models is important to understand toxicity and bias in generation, as well as privacy, making it extremely important that language models include datasheets (page 14) or model cards (page 75) giving full replicable information on the corpora used to train them."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}, {"id": "2000", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "These arguments led many linguists and computational linguists to ignore work in statistical modeling for decades.."}], "metadata": {}, "id": "98"}
{"query": "question: What are ways to mitigate the biases in language models answer: <extra_id_0>", "answers": ["To carefully analyze the data used to pre-train large language models, and  include datasheets or model cards giving full replicable information on the corpora used to train them"], "generation": "Several ways to mitigate the biases in language models are described in this chapter..", "passages": [{"id": "1833", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "This modication is called smoothing or discounting.."}, {"id": "2247", "contributed_by": "group 1", "title": "Naive Bayes and Sentiment Classification", "section": "", "text": "But these problems can also be caused by the labels (for example due to biases in the human labelers), by the resources used (like lexicons, or model components like pretrained embeddings), or even by model architecture (like what the model is trained to optimize).."}, {"id": "2767", "contributed_by": "group 10", "title": "", "section": "", "text": "In language modeling, the goal is to learn a model that can generate the correct next word in a sequence of words. One way to train such a model is to use the cross-entropy loss as the objective function. The cross-entropy loss is determined by the probability that the RNN language model assigns to the correct next word. Specifically, at each time step, the model generates a probability distribution over the possible next words in the sequence, and the cross-entropy loss is calculated by taking the negative log probability of the correct next word. By minimizing the cross-entropy loss, the model is trained to assign higher probabilities to the correct next words, and thus generate more accurate predictions. This approach has been used in a variety of language modeling tasks, and is a key component of many state-of-the-art language models."}, {"id": "1993", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "Both backoff and interpolation require discounting to create a probability dis Kneser-Ney smoothing makes use of the probability of a word being a novel continuation.."}, {"id": "1720", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "The perplexity (sometimes called PP for short) of a language model on a test set is the inverse probability of the test set, normalized by the number of words.."}, {"id": "1722", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "Thus, minimizing perplexity is equivalent to maximizing the test set probability according to the language model.."}, {"id": "2835", "contributed_by": "group 4", "title": "Word Senses and WordNet", "section": "", "text": "This chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights:  A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.  Many words are polysemous, having many senses.  Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.  WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.  Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.  The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.  Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.  An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.  Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target words neighborhood.  Word sense induction is the task of learning word senses unsupervised."}, {"id": "1626", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "Models that assign probabilities to sequences of words are called language models or LMs.."}, {"id": "2007", "contributed_by": "group 1", "title": "N-gram Language Models", "section": "", "text": "Starting in the late 1990s, Chen and Goodman performed a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters (Chen and Goodman.."}, {"id": "2855", "contributed_by": "group 4", "title": "Semantic Role Labeling", "section": "", "text": " Semantic roles are abstract models of the role an argument plays in the event described by the predicate.  Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.  Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.  Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class."}], "metadata": {}, "id": "99"}
{"query": "question: What are some examples of bidirectional transformer encoders answer: <extra_id_0>", "answers": ["BERT, RoBERTa, and SpanBERT"], "generation": "Several examples of bidirectional transformer encoders are shown in Fig. 11.4. Several examples of bidirectional transformer encoders are shown in Fig. 11.4.", "passages": [{"id": "3505", "contributed_by": "group 2", "title": "", "section": "", "text": "To make this more concrete, the original bidirectional transformer encoder model, BERT (Devlin et al., 2019), consisted of the following:"}, {"id": "3459", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence."}, {"id": "3504", "contributed_by": "group 2", "title": "", "section": "", "text": "Beyond this simple change, all of the other elements of the transformer architecture remain the same for bidirectional encoder models. Inputs to the model are segmented using subword tokenization and are combined with positional embeddings before being passed through a series of standard transformer blocks consisting of self-attention and feedforward layers augmented with residual connections and layer normalization, as shown in Fig. 11.4."}, {"id": "3553", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence classification with a bidirectional transformer encoder. The output vector for the [CLS] token serves as input to a simple classifier."}, {"id": "3591", "contributed_by": "group 2", "title": "", "section": "", "text": "Bidirectional encoders can be used to generate contextualized representations of input embeddings using the entire input context."}, {"id": "3336", "contributed_by": "group 2", "title": "", "section": "", "text": "Encoder-decoder networks consist of three components: 1. An encoder that accepts an input sequence, x1n, and generates a corresponding sequence of contextualized representations, hn1. LSTMs, convolutional networks, and Transformers can all be employed as encoders."}, {"id": "3561", "contributed_by": "group 2", "title": "", "section": "", "text": "Sequence labeling for part-of-speech tagging with a bidirectional transformer encoder. The output vector for each input token is passed to a simple k-way classifier."}, {"id": "3460", "contributed_by": "group 2", "title": "", "section": "", "text": "The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder."}, {"id": "2804", "contributed_by": "group 10", "title": "", "section": "", "text": "With bidirectional encoders we simply skip the mask, allowing the model to contextualize each token using information from the entire input. Beyond this simple change, all of the other elements of the transformer architecture remain the same for bidirectional encoder models. Inputs to the model are segmented using subword tokenization and are combined with positional embed- dings before being passed through a series of standard transformer blocks consisting of self-attention and feedforward layers augmented with residual connections and layer normalization."}, {"id": "3592", "contributed_by": "group 2", "title": "", "section": "", "text": "Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input."}], "metadata": {}, "id": "100"}
