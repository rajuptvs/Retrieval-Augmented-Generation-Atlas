{"id": "1", "contributed_by": "group 2", "question": "What is a feed forward network", "answers": ["A neural network architecture where the computation proceeds iteratively from one layer of units to the next"]}
{"id": "2", "contributed_by": "group 2", "question": "What is the main advantage of neural network over logistic regression", "answers": ["Neural networks are typically more powerful classifiers than logistic regression, and they can learn features automatically and are more suitable for large scale problems"]}
{"id": "3", "contributed_by": "group 2", "question": "What is the property of a Sigmoid function", "answers": ["The Sigmoid function maps the input into the range between 0 and 1, and it is differentiable"]}
{"id": "4", "contributed_by": "group 2", "question": "Describe the vanishing gradient problem", "answers": ["Since we train networks by propagating an error signal backwards by multiplying gradients from each layer of the network, gradients that are almost zero cause the error signal to get smaller and smaller until it is too small to be used for training"]}
{"id": "5", "contributed_by": "group 2", "question": "What is a fully connected layer in neural network", "answers": ["It is a layer such that each unit in the layer takes as input the outputs from all the units in the previous layer"]}
{"id": "6", "contributed_by": "group 2", "question": "What does softmax do", "answers": ["Softmax is a function that converts (or normalizes) a vector of real values to a vector that encodes a probability distribution. Each element of the resulting vector ranges between 0 and 1, and they sum to 1."]}
{"id": "7", "contributed_by": "group 2", "question": "What is an activation function", "answers": ["Instead of using z, a linear function of x, as the output, neural units apply a nonlinear function f to z. The output of this function f is the activation value. Typical activation functions include sigmoid, hyperbolic tangent (tanh), and ReLU."]}
{"id": "8", "contributed_by": "group 2", "question": "Why do we need activation functions", "answers": ["Without activation functions, any multi-layer neural network can be reduced to a single-layer network, and thus losing its representational power."]}
{"id": "9", "contributed_by": "group 2", "question": "What is pre-training", "answers": ["Pre-training is the idea of relying on another algorithm, such as Word2Vec or GloVe, to have already learned an embedding representation for our input words"]}
{"id": "10", "contributed_by": "group 2", "question": "Why does neural network use embeddings", "answers": ["Embeddings allows neural language models to generalize better to unseen data"]}
{"id": "11", "contributed_by": "group 2", "question": "How does backprop (back propagation) works", "answers": ["Backprop computes each of the partial derivatives along each edge of the computation graph from right to left using the chain rule in Calculus, until we have annotated the graph all the way to all the input variables."]}
{"id": "12", "contributed_by": "group 2", "question": "What is dropout and what is its purpose", "answers": ["It is to randomly drop some neural units and their connections from the network during training, a form of regularization to reduce overfitting"]}
{"id": "13", "contributed_by": "group 2", "question": "What is the difference between parameters and hyperparameters", "answers": ["The parameters of a neural network are the weights and biases learned by gradient descent, whereas hyperparameters are things that are chosen by the algorithm designer"]}
{"id": "14", "contributed_by": "group 2", "question": "What are some examples of hyperparameters", "answers": ["The learning rate, the mini-batch size, the number of layers, the number of hidden nodes per layer, the choice of activation functions, and the method of regularization"]}
{"id": "15", "contributed_by": "group 2", "question": "Why is the advantage of training the word embeddings as part of the optimization", "answers": ["Doing so is useful when the task the network is designed for, such as sentiment classification, translation, or parsing, places strong constraints on what makes a good representation for words."]}
{"id": "16", "contributed_by": "group 2", "question": "What is neural language model", "answers": ["A probabilistic classifier implemented as a neural network that computes the probability of the next word given the previous n words."]}
{"id": "17", "contributed_by": "group 2", "question": "What is the perceptron", "answers": ["The perceptron is a very simple neural unit that has a binary output and does not have a non-linear activation function"]}
{"id": "18", "contributed_by": "group 2", "question": "Why do we need deep neural network", "answers": ["A neural unit cannot compute some very simple functions, such as XOR, but a multi-layer neural network with non-linear activation can"]}
{"id": "19", "contributed_by": "group 2", "question": "What is the cross-entropy loss", "answers": ["The cross-entropy loss is the negative log of the output probability corresponding to the correct class. It is also call the negative log likelihood loss"]}
{"id": "20", "contributed_by": "group 2", "question": "What is a one-hot vector", "answers": ["A one-hot vector is a vector that has one element equal to one, while all the other elements are set to zero. The index that is hot corresponds to the word\u2019s index in the vocabulary"]}
{"id": "21", "contributed_by": "group 2", "question": "What are the parts of speech", "answers": ["Parts of speech (POS) are the basis for descriptions of European languages, and most commonly include noun, verb, pronoun, preposition, adverb, conjunction, participle, and article."]}
{"id": "22", "contributed_by": "group 2", "question": "What is a named entity", "answers": ["A named entity is a word or a multi-word phrase that is anything that can be referred to with a proper name: a person, a location, an organization."]}
{"id": "23", "contributed_by": "group 2", "question": "Why do we need POS", "answers": ["They provide useful clues to sentence structure and meaning, and thus tagging parts of speech is a key part of parsing"]}
{"id": "24", "contributed_by": "group 2", "question": "What is named entity recognition (NER)", "answers": ["To recognize named entities in a sentence, and assign them tags like person, location, or organization."]}
{"id": "25", "contributed_by": "group 2", "question": "Why is tagging a disambiguation task", "answers": ["Words are ambiguous, as they can be assigned to several possible part-of-speech tags. The goal of tagging is to resolve these ambiguities, choosing the proper tag for the context"]}
{"id": "26", "contributed_by": "group 2", "question": "What are some generic named entity types", "answers": ["People, location, organization, geo-political entity, money, time, and so on"]}
{"id": "27", "contributed_by": "group 2", "question": "What is Hidden Markov Model (HMM)", "answers": ["An HMM is a probabilistic sequence model. Given a sequence of words, it computes a probability distribution over possible sequences of labels and chooses the best label sequence."]}
{"id": "28", "contributed_by": "group 2", "question": "What is the Markov assumption", "answers": ["The assumption on the probabilities of this sequence that, when predicting the future, the past doesn\u2019t matter, only the present."]}
{"id": "29", "contributed_by": "group 2", "question": "What is the transition probability matrix of a Markov Model", "answers": ["A matrix representation such that the element at the ith row and the jth column is the probability of moving from state i to state j"]}
{"id": "30", "contributed_by": "group 2", "question": "What assumptions are made in the HMM", "answers": ["First, the probability of a particular state depends only on the previous state. Second, the probability of an output observation depends only on the state that produced the observation. They are also known as the Markov assumption and the Bigram assumption"]}
{"id": "31", "contributed_by": "group 2", "question": "What does the Viterbi algorithm do", "answers": ["The Viterbi algorithm is a dynamic programming algorithm to solve the HMM part-of-speech problem. Mathematically, given as an input the HMM which is made of the probability transition matrix and the emission probability matrix, and a sequence of observations, it finds the most probable sequence of states"]}
{"id": "32", "contributed_by": "group 2", "question": "What is Conditional Random Field (CRF)", "answers": ["A CRF is a log-linear model that assigns a probability to an entire output (tag) sequence Y, out of all possible sequences Y, given the entire input (word) sequence X"]}
{"id": "33", "contributed_by": "group 2", "question": "What is the advantage of CRF over HMM", "answers": ["It is hard for generative models like HMMs to add arbitrary features directly into the model in a clean way, but it is easier to incorporate a lot of features into discriminative models like CRF"]}
{"id": "34", "contributed_by": "group 2", "question": "What is linear-chain CRF", "answers": ["Linear chain CRF is a CRF such that the features only depend on the current output and previous output. This restriction allows the Viterbi algorithm to be used"]}
{"id": "35", "contributed_by": "group 2", "question": "What are word shape features?", "answers": ["Word shape features represent the abstract letter pattern of the word by mapping lower-case letters to \u2018x\u2019, upper-case to \u2018X\u2019, numbers to \u2019d\u2019, and retaining punctuation"]}
{"id": "36", "contributed_by": "group 2", "question": "What is a gazetteer", "answers": ["A gazetteer is a list of place names, often providing millions of entries for locations with detailed geographical and political information. It is an important feature in CRF-based named entity recognition"]}
{"id": "37", "contributed_by": "group 2", "question": "What is the algorithm used to find the most likely tag sequence in CRF", "answers": ["The Viterbi algorithm can be used to solve CRF"]}
{"id": "38", "contributed_by": "group 2", "question": "Can features in the CRF depend on earlier output tags (such as the beginning tag)?", "answers": ["If the features depend on other features than the current and prior tags, then the CRF is not a linear chain CRF, and hence the Viterbi algorithm cannot be used"]}
{"id": "39", "contributed_by": "group 2", "question": "What are feature templates in CRF", "answers": ["The system designer would specify templates to be filled in, and they are automatically populated as the words are processed."]}
{"id": "40", "contributed_by": "group 2", "question": "What is BIO tagging", "answers": ["BIO tagging is a standard approach to sequence labeling for a span-recognition problem. Tokens that begin a span of interest are tagged with a B, tokens that occur inside a span are tagged with an I, and any tokens outside of any span of interest are labeled O"]}
{"id": "41", "contributed_by": "group 2", "question": "What is a recurrent neural network (RNN)", "answers": ["A network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input"]}
{"id": "42", "contributed_by": "group 2", "question": "What is the main difference between a feed-forward network and a recurrent network", "answers": ["In a feed-forward network, all computation goes forward, whereas in a recurrent network, there is a link that augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time."]}
{"id": "43", "contributed_by": "group 2", "question": "Why is RNN computation done sequentially", "answers": ["The fact that the computation at time t requires the value of the hidden layer from time t \u2212 1 mandates an incremental inference algorithm that proceeds from the start of the sequence to the end"]}
{"id": "44", "contributed_by": "group 2", "question": "What is backpropagation through time", "answers": ["RNN weights are trained in two passes. In the first pass, we perform forward inference, accumulating the loss at each step in time. In the second phase, we process the sequence in reverse, computing the required gradients as we go"]}
{"id": "45", "contributed_by": "group 2", "question": "What is an RNN language model", "answers": ["RNN language models process the input sequence one word at a time, attempting to predict the next word from the current word and the previous hidden state"]}
{"id": "46", "contributed_by": "group 2", "question": "What is teacher forcing", "answers": ["Rather than feeding the model its best case from the previous time step, we always give the model the correct history sequence to predict the next word"]}
{"id": "47", "contributed_by": "group 2", "question": "What is weight tying", "answers": ["Instead of having one embedding after the input layer, and one embedding at output prior to softmax, we simply use a single set of embeddings at both layers"]}
{"id": "48", "contributed_by": "group 2", "question": "What is the advantage of weight tying", "answers": ["Weight tying improves model perplexity and significantly reduces the number of parameters required for the model"]}
{"id": "49", "contributed_by": "group 2", "question": "What kind of tasks can RNN solve", "answers": ["Examples include sequence classification, sequence labeling, and text generation tasks"]}
{"id": "50", "contributed_by": "group 2", "question": "What is sequence labeling", "answers": ["To assign a label chosen from a small fixed set of labels to each element of a sequence, like the part-of-speech tagging and named entity recognition"]}
{"id": "51", "contributed_by": "group 2", "question": "How does RNN solve sequence labeling", "answers": ["The outputs of the network at each time step represent the distribution over the POS tagset generated by a softmax layer"]}
{"id": "52", "contributed_by": "group 2", "question": "How does RNN solve sequence classification", "answers": ["We can take the hidden layer for the last token of the text as a compressed representation of the text. Then, such representation is fed through a feed-forward classification network"]}
{"id": "53", "contributed_by": "group 2", "question": "What is end-to-end training", "answers": ["The training regimen that uses the loss from a downstream application to adjust the weights all the way through the network"]}
{"id": "54", "contributed_by": "group 2", "question": "What is autoregression generation", "answers": ["It is the use of a language model to incrementally generate words by repeatedly sampling the next word conditioned on the previous choices"]}
{"id": "55", "contributed_by": "group 2", "question": "What is bidirectional RNN", "answers": ["It combines two independent RNNs, one processing the input from the start to the end, and the other from the end to the start. The representations computed by the networks are then combined into a single vector"]}
{"id": "56", "contributed_by": "group 2", "question": "What problem does LSTM solve", "answers": ["The inability of RNNs to carry forward critical information and the vanishing gradient problem"]}
{"id": "57", "contributed_by": "group 2", "question": "What is the key advantages of Transformers", "answers": ["Transformers were designed to eliminate recurrent connections and return to architectures reminiscent of the fully connected networks"]}
{"id": "58", "contributed_by": "group 2", "question": "What is self-attention", "answers": ["The use of self-attention layers allows a network to directly extract and use information from arbitrarily large contexts without the need to pass it through intermediate recurrent connections as in RNNs."]}
{"id": "59", "contributed_by": "group 2", "question": "What layer normalization is important", "answers": ["It can improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training"]}
{"id": "60", "contributed_by": "group 2", "question": "What is pre-training", "answers": ["It is to first train a transformer language model on a large corpus of text, and only afterwards add a linear or feedforward layer on top that we finetune on a smaller dataset"]}
{"id": "61", "contributed_by": "group 2", "question": "What are encoder-decoder or sequence-to-sequence models", "answers": ["Neural networks of which the output sequence is a complex function of the entire input sequencer; The output sequence is not merely a direct mapping from individual input words"]}
{"id": "62", "contributed_by": "group 2", "question": "What are some examples of encoder-decoder applications", "answers": ["Machine translation, summarization, dialogue, and semantic parsing"]}
{"id": "63", "contributed_by": "group 2", "question": "What is linguistic typology", "answers": ["The study of the systematic similarities and differences across languages"]}
{"id": "64", "contributed_by": "group 2", "question": "What are SVO and SOV languages", "answers": ["SVO stands for Subject-Verb-Object languages such as English, and SOV stands for Subject-Object-Verb languages such as Japanese"]}
{"id": "65", "contributed_by": "group 2", "question": "What are the components of an encoder-decoder network", "answers": ["The Encoder, the context vector, and the decoder"]}
{"id": "66", "contributed_by": "group 2", "question": "What is context in an Encoder-Decoder architecture", "answers": ["The entire purpose of the encoder is to generate a contextualized representation of the input. Such representation is called the context, and is embodied in the final hidden state of the encoder. The context is then passed to the decoder"]}
{"id": "67", "contributed_by": "group 2", "question": "What is the attention mechanism", "answers": ["The context is a fixed-length vector that is a weighted sum of all the encoder hidden states. The weights attend to a particular part of the source text that is relevant for the token the decoder is currently producing. Attention thus replaces the static context vector with one that is dynamically derived from the encoder hidden states, different for each token in decoding"]}
{"id": "68", "contributed_by": "group 2", "question": "What is the problem with greedy decoding", "answers": ["Greedy decoding always makes a choice that is locally optimal, but the token that looks good to the decoder now might turn out later to have been the wrong choice"]}
{"id": "69", "contributed_by": "group 2", "question": "How does beam search work", "answers": ["Instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step, with k being the fixed-size memory footprint also called the beam width"]}
{"id": "70", "contributed_by": "group 2", "question": "Why do source and target languages share the same vocabulary", "answers": ["It makes it easy to copy tokens (like names) from source to target"]}
{"id": "71", "contributed_by": "group 2", "question": "What is a bitext", "answers": ["It is a parallel corpus, a piece of text that appears in two or more languages, which is used for machine translation training"]}
{"id": "72", "contributed_by": "group 2", "question": "When is sentence alignment necessary", "answers": ["Standard training corpora for machine translation come as aligned pairs of sentences. When creating new corpora, for example for under-resourced languages or new domains, these sentence alignments must be created"]}
{"id": "73", "contributed_by": "group 2", "question": "When is back-translation necessary", "answers": ["When we are short of data for training MT models, since parallel corpora may be limited for particular languages or domains"]}
{"id": "74", "contributed_by": "group 2", "question": "How does backtranslation work", "answers": ["We first create synthetic bitexts, and train an intermediate target-to-source MT system on the small bitext to translate the monolingual target data to the source language. Now we can add this synthetic bitext to our training data, and retrain our source-to-target MT model"]}
{"id": "75", "contributed_by": "group 2", "question": "How are machine translations evaluated", "answers": ["Adequacy (how well the translation captures the exact meaning of the source sentence), and fluency (how fluent the translation is in the target language)"]}
{"id": "76", "contributed_by": "group 2", "question": "What is Character F-Score", "answers": ["It is also known as ChrF, a metric that ranks each MT target sentence by a function of the number of character n-gram overlaps with the human translation"]}
{"id": "77", "contributed_by": "group 2", "question": "Why is ChrF preferred over BLEU score", "answers": ["BLEU is a word-based metric, it is very sensitive to word tokenization, making it difficult to compare across situations, and doesn\u2019t work as well in languages with complex morphology"]}
{"id": "78", "contributed_by": "group 2", "question": "What is the limitation of ChrF", "answers": ["It is very local, cannot evaluate cross-sentence properties, and perform poorly at comparing very different kinds of systems"]}
{"id": "79", "contributed_by": "group 2", "question": "How are self-attention and cross-attention different", "answers": ["Cross-attention has the same form as self-attention, except that while the queries as usual come from the previous layer of the decoder, the keys and values come from the output of the encoder"]}
{"id": "80", "contributed_by": "group 2", "question": "What are some of the biases and ethical issues in machine translation", "answers": ["For example, MT systems often assign gender according to culture stereotypes. Moreover, these stereotypes cannot completely be accounted for by gender bias in US labor statistics, but instead are amplified by MT systems"]}
{"id": "81", "contributed_by": "group 2", "question": "What is the distributional hypothesis", "answers": ["The idea that something about what we\u2019re loosely calling word meanings can be learned even without any grounding in the real world, solely based on the content of the texts we\u2019ve encountered over our lives"]}
{"id": "82", "contributed_by": "group 2", "question": "What is the concept of contextual embedding", "answers": ["That each word will be represented by a different vector each time it appears in a different context"]}
{"id": "83", "contributed_by": "group 2", "question": "What is pre-training", "answers": ["The process of learning some sort of representation of meaning for words or sentences by processing very large amounts of text"]}
{"id": "84", "contributed_by": "group 2", "question": "What is fine-tuning", "answers": ["The process of taking the representations from the pretrained models, and further training the model, often via an added neural net classifier, to perform some down-stream task like named entity tagging or question answering or coreference"]}
{"id": "85", "contributed_by": "group 2", "question": "What is transfer learning", "answers": ["The method of acquiring knowledge from one task or domain, and then applying it to solve a new task"]}
{"id": "86", "contributed_by": "group 2", "question": "What problem do bidirectional encoders try to solve", "answers": ["The issue with encoders that process input from left to right is that hidden state computation at each point in time is based solely on the current and earlier elements of the input, ignoring potentially useful information located to the right of each tagging decision"]}
{"id": "87", "contributed_by": "group 2", "question": "What is the advantage of contextual embedding instead of static embedding", "answers": ["Given a sequence of input tokens, we can use the output vector from the final layer of the model as a representation of the meaning of tokens in the context of the whole sentence. This improves performance of tasks that might require a model of word meaning"]}
{"id": "88", "contributed_by": "group 2", "question": "What is the difference between word types and word tokens", "answers": ["Instead of just capturing the vocabulary entries with word types, word tokens also capture the context of the words in the sentence"]}
{"id": "89", "contributed_by": "group 2", "question": "Where does the power of pre-trained language models come from", "answers": ["Their ability to extract generalizations from large amounts of text"]}
{"id": "90", "contributed_by": "group 2", "question": "What is the CLS token", "answers": ["An additional vector that is added to the model to stand for the entire sequence. The output vector in the final layer of the model for the CLS input represents the entire input sequence and serves as the input to a classifier head"]}
{"id": "91", "contributed_by": "group 2", "question": "How to classify with transfer learning through fine-tuning", "answers": ["Append the CLS token to the beginning of the sentence, and the output vector of the CLS token in the final layer of the model serves as the input to a classifier head that makes the relevant decision."]}
{"id": "92", "contributed_by": "group 2", "question": "What is a main difference between neural classifiers and classifiers based on pre-trained language model", "answers": ["Reasonable classification performance with pre-trained language models can be typically achieved with only minimal changes to the language model parameters, often limited to updates over the final few layers of the transformer"]}
{"id": "93", "contributed_by": "group 2", "question": "What does NSP stand for", "answers": ["Next Sentence Prediction"]}
{"id": "94", "contributed_by": "group 2", "question": "How does pairwise sequence classification work", "answers": ["Append the CLS token to the beginning of the sentence, and add a SEP token between the two sentences, and use a classifier to classify the final layer of the CLS token"]}
{"id": "95", "contributed_by": "group 2", "question": "What are examples of pairwise sequence classification", "answers": ["Recognizing textual entailment, or whether one sentence leads logically to the next sentence"]}
{"id": "96", "contributed_by": "group 2", "question": "What is an example of span-based application", "answers": ["Identification of spans of interest, entity recognition, question answering, coreference resolution"]}
{"id": "97", "contributed_by": "group 2", "question": "What are some of the potential harms from language models", "answers": ["Language models can generate toxic language, show biased, and be easily manipulated for disinformation, and may violate privacy"]}
{"id": "98", "contributed_by": "group 2", "question": "What cause biases in language models", "answers": ["For example, the training data may contain a biased distribution, such as a disproportionate amount of data generated by authors from developed countries. Such biased population samples likely skew the resulting generation away from the perspectives or topics of underrepresented populations"]}
{"id": "99", "contributed_by": "group 2", "question": "What are ways to mitigate the biases in language models", "answers": ["To carefully analyze the data used to pre-train large language models, and  include datasheets or model cards giving full replicable information on the corpora used to train them"]}
{"id": "100", "contributed_by": "group 2", "question": "What are some examples of bidirectional transformer encoders", "answers": ["BERT, RoBERTa, and SpanBERT"]}